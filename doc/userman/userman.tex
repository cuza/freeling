\documentclass[a4paper]{book}
\usepackage{a4wide}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\setcounter{tocdepth}{1}
\usepackage{listings}
\lstset{language=C++}

\usepackage{html}   %  load this for LaTeX2HTML
\begin{latexonly}
\lstnewenvironment{LSTverbatim}{}{}
\end{latexonly}

\begin{document}

\begin{titlepage}
\vspace*{7cm}
\begin{center}
{\Large FreeLing User Manual\\[1ex]\large 3.1}\\
\vspace*{1cm}
{\small October 2013}\\
\end{center}
\end{titlepage}

{\newpage{\pagestyle{empty}\cleardoublepage}}
\pagenumbering{roman}

\tableofcontents

{\newpage{\pagestyle{empty}\cleardoublepage}}
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

The FreeLing package consists of a library providing language analysis
services (such as morphological analysis, date recognition, PoS
tagging, etc.)

The current version provides language identification, tokenizing,
sentence splitting, morphological analysis, NE detection and
classification, recognition of dates/numbers/physical
magnitudes/currency/ratios, phonetic encoding, PoS tagging, shallow
parsing, dependency parsing, WN-based sense annotation, Word Sense
Disambiguation, and coreference resolution.  Future versions are
expected to improve performance in existing functionalities, as well
as incorporate new features.

FreeLing is designed to be used as an external library from any
application requiring this kind of services. Nevertheless, a sample
main program is also provided as a basic interface to the library,
which enables the user to analyze text files from the command line.

%..................................................
\section{What is FreeLing}

FreeLing is a developer-oriented library providing language analysis services.
If you want to develop, say, a machine translation system, and you need some
kind of linguistic processing of the source text, your MT application can
call FreeLing modules to do the required analysis.

In the directory \verb#src/main/simple_examples# in FreeLing tarball, some
sample programs are provided to illustrate how an application program 
can call the library.

In the directory \verb#src/main/sample_analyzer# a couple of more complex programs
are provided, which can be used either as a command line interface to the library to 
process texts, or as examples of how to build customized applications using FreeLing.

%..................................................
\section{What is NOT FreeLing}

FreeLing is not a user-oriented text analysis tool. That is, it is not
designed to be user friendly, or to output results with a cute image,
or in a certain format.

FreeLing results are linguistic analysis in a data structure. Each
end-user application (e.g. anything from a simple syntactic-tree drawing
plugin to a complete machine translation system) can access those data and
process them as needed.

Nevertheless, FreeLing package provides a quite complete application
program ({\tt analyzer}) that enables an end user with no programming skills
to obtain the analysis of a text. See chapter \ref{cap-analyzer} for details.

This program offers a set of options that cover most of FreeLing
capabilities. Nevertheless, much more advantadge can be taken of
FreeLing, and more information can be accessed if you call FreeLing
from your own application program.

%..................................................
\section{Supported Languages}

The current version supports (to different extents, see
Table~\ref{t-langs}) Asturian (as), Catalan (ca), English (en), French
(fr), Galician (gl), Italian (it), Portuguese (pt), Russian (ru),
Slovene (sl), Spanish (es), and Welsh (cy).

\begin{table}[htb] \center
\begin{tabular}{l|ccccccccccc|}
                            & as & ca & cy & en & es & fr & gl & it & pt & ru & sl \\ \hline
Tokenization                & X  & X  & X  & X  & X  & X  & X  & X  & X  & X  &    \\
Sentence splitting          & X  & X  & X  & X  & X  & X  & X  & X  & X  & X  &    \\
Number detection            &    & X  &    & X  & X  &    & X  & X  & X  & X  &    \\
Date detection              &    & X  &    & X  & X  &    & X  &    & X  & X  &    \\
Morphological dictionary    & X  & X  & X  & X  & X  & X  & X  & X  & X  & X  &    \\
Affix rules                 & X  & X  & X  & X  & X  & X  & X  & X  & X  &    &    \\
Multiword detection         & X  & X  & X  & X  & X  & X  & X  & X  & X  &    &    \\
Basic named entity detection& X  & X  & X  & X  & X  & X  & X  & X  & X  & X  &    \\
B-I-O named entity detection&    & X  &    & X  & X  &    & X  &    & X  &    &    \\
Named Entity Classification &    & X  &    & X  & X  &    &    &    & X  &    &    \\ 
Quantity detection          &    & X  &    & X  & X  &    & X  &    & X  & X  &    \\ 
PoS tagging                 & X  & X  & X  & X  & X  & X  & X  & X  & X  & X  &    \\ 
Phonetic encoding           &    &    &    & X  & X  &    &    &    &    &    &    \\ 
WN sense annotation         &    & X  &    & X  & X  &    & X  &    &    &    & X  \\
UKB sense disambiguation    &    & X  &    & X  & X  &    &    &    &    &    & X  \\
Shallow parsing             & X  & X  &    & X  & X  &    & X  &    & X  &    &    \\ 
Full/dependency parsing     & X  & X  &    & X  & X  &    & X  &    &    &    &    \\
Coreference resolution      &    &    &    &    & X  &    &    &    &    &    &    \\ \hline
\end{tabular}
\label{t-langs}
\caption{Analysis services available for each language.}
\end{table}

 FreeLing also includes WordNet-based sense dictionaries for some of
the covered languages, as well as some knowledge extracted from
WordNet, such as semantic file codes, or hypernymy relationships.
See {\tt http://wordnet.princeton.edu} and {\tt http://www.illc.uva.nl/EuroWordNet}
for details on WordNet and EuroWordNet, respectively.

See the {\sl Linguistic Data} section on FreeLing webpage to find out more
about the size and origin the linguistic resources for these languages.

 See file \texttt{COPYING} in the distribution packages to find out 
the license of each third-party linguistic resource included in 
FreeLing packages.
   

%..................................................
\section{License}

\noindent FreeLing code is licensed under GNU General Public License
(GPL).
  
\noindent The linguistic data collections are licensed under diverse
licenses, depending on their original sources.

\noindent Find the details in the {\tt COPYING} file in the tarball, or in the
  {\tt License} section in FreeLing webpage.

%..................................................
\section{Contributions}
 
  FreeLing is developed and maintained by people in TALP Research 
  Center at Universitat Politecnica de Catalunya (http://www.talp.upc.edu). 

  Many people further contributed to by reporting problems, suggesting 
  various improvements, submitting actual code or extending linguistic 
  databases. 

  A detailed list can be found in {\sl Contributions} section at
  FreeLing webpage \\
  ({\tt http://nlp.lsi.upc.edu/freeling}).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Getting it to work}

%..................................................
\section{Requirements}
\label{sec-requirements}

 To install FreeLing you'll need:

\begin{list}{\labelitemi}{\leftmargin=1em}
 \item A typical Linux box with usual development tools:
   \begin{list}{\labelitemii}{\leftmargin=1em}
    \itemsep 0cm
    \item bash
    \item make
    \item C++ compiler with basic STL support
    \end{list}

 \item Enough hard disk space (1.3 Gb for source and compilation
   files, plus 600Mb for final installation)

 \item Some external libraries are required to compile FreeLing:
    \begin{list}{\labelitemii}{\leftmargin=1em}
    \itemsep 0.25cm

     \item {\tt libboost \& libicu} \\ 
       Boost library. Included in all Linux distributions. You
       probably {\bf do not} have all neeeded components
       installed. Make sure to install {\bf both} runtime and
       development packages for:
      \begin{list}{\labelitemiii}{\leftmargin=1em}
       \itemsep 0cm 
         \item libicu
         \item libboost-regex
         \item libboost-system
         \item libboost-thread
         \item libboost-program-options
         \item libboost-locale (only required for MacOSX or FreeBSD, not required in Linux)
      \end{list}

     \item {\tt libz} \\ 
       Compression library. Included in all Linux distributions. You
       probably {\bf do not} have all neeeded components
       installed. Make sure to install {\bf both} runtime and
       development packages for:
      \begin{list}{\labelitemiii}{\leftmargin=1em}
       \itemsep 0cm 
         \item zlib
      \end{list}
    \end{list}

       {\sl Orientative package names} (check the package manager in 
      your system):
      \begin{list}{\labelitemiii}{\leftmargin=1em}
       \itemsep 0cm
       \item Ubuntu/Debian: {\tt libboost-dev libboost-regex-dev libicu-dev libboost-system-dev libboost-program-options-dev libboost-thread-dev zlib1g-dev}
       \item OpenSuse/Fedora/Mandriva: {\tt boost-devel boost-regex-devel libicu-devel boost-system-devel boost-program-options-devel boost-thread-dev zlib-devel}
       \item Slackware: {\tt boost icu4c zlib}
       \end{list}

      Note that you need to install both the binary libraries and the
      development packages (usually sufixed as \texttt{-dev} or
      \texttt{-devel}).

      Most package managers will install both binary and development
      packages when the \texttt{-dev} package is required.  If this is
      not your case, you'll need to manually select both packages.

\end{list}

   See details on the installation procedure in section \ref{sec-installation}.

%..................................................
\section{Installation}
\label{sec-installation}

This section provides a detailed guide on different options to install
FreeLing (and all its required packages).

\subsection{Install from {\tt .deb} binary packages}
\label{sec-install-deb}

This installation procedure is the fastest and easiest. 
If you do not plan to modify the code, this is the option you should take.

Binary packages are available {\bf only for stable FreeLing versions}. 
If you want to install an alpha or beta version, please see sections
\ref{sec-install-tgz} and \ref{sec-install-svn}.
                        
The provided packages will only work on debian-based distributions.
They have been tested in Ubuntu (10.04LTS Lucid, 11.04 Natty, 12.04
Precise, 12.10 Quantal) and Debian (6.0.2 Squeeze, 7.0 Wheezy).

Most debian-bawsed systems will launch the apropriate installer if you
just double click on the package file. The installer should solve the
dependencies and install all required packages.

If that doesn't work, you can install it by hand (in Ubuntu or Debian) 
with the following procedure (will probably work for other debian-based distros):
 
\begin{enumerate}
\item Install required system libraries.

The following commands should install *both* header packages and
binary libraries. If they don't, use your package manager as described
in section Section~\ref{sec-requirements} to install all required packages.
\begin{verbatim}
 sudo apt-get install libboost-regex-dev libicu-dev zlib1g-dev
 sudo apt-get install libboost-system-dev libboost-program-options-dev
\end{verbatim}

\item Install freeling package:
\begin{verbatim}
 sudo dpkg -i freeling-3.0.deb
\end{verbatim}
\end{enumerate}

\noindent In a Debian system, the above commands must be issued as
root and without {\tt sudo}.

\subsection{Install from {\tt .tar.gz} source packages}
\label{sec-install-tgz}

 Installation from source follows standard GNU autoconfigure
 installation procedures (that is, the usual
 \verb#./configure && make && make install# stuff).

 Installing from source is slower and harder, but it will work in any
 Linux box, even if you have library versions different than those
 required by the {\tt .deb} package.

\begin{list}{{\bf \arabic{enumi}.}}{\usecounter{enumi}\leftmargin=1.5em}

\item {\bf Install development tools}

 You'll need to install the C++ compiler:
\begin{verbatim}
 sudo apt-get install build-essential automake autoconf
\end{verbatim}
  In Debian, use the same command as root, without {\sl sudo}.  In
  other distributions, check the distribution package manager to
  install a working C++ compiler.

\item {\bf Install packaged requirements}

 All required libraries are standard packages in all Linux
 distributions.  Just open your favorite software package manager and
 install them.

  Package names may vary slightly in different distributions. See
  section \ref{sec-requirements} for some hints on possible package
  names.

  As an example, commands to install the packages from command line in
  Ubuntu and Debian are provided, though you can do the same using
  synaptic, or aptitude.
  If you have another distribution, use your package manager to locate
  and install the appropriate library packages (see section
  \ref{sec-requirements}).

  Both in Debian (squeeze or wheezy) and in Ubuntu (Lucid or later)
  you need to do:
\begin{verbatim}
 sudo apt-get install libboost-regex-dev libicu-dev
 sudo apt-get install libboost-system-dev libboost-program-options-dev
\end{verbatim}

\item {\bf Install FreeLing}

\begin{verbatim}
 tar xzvf freeling-3.0.tar.gz
 cd freeling-3.0
 ./configure
 make
 sudo make install
\end{verbatim}

  FreeLing library is entirely contained in the file {\tt libfreeling.so}
 installed in {\tt /usr/local/lib} by default.

 Sample program {\tt analyze} is installed in {\tt /usr/local/bin}. 
 See sections \ref{sec-execute} and \ref{cap-analyzer} for details.
\end{list}


\subsection{Install from {\tt SVN} repositories}
\label{sec-install-svn}

  Installing from the SVN is very similar to installing from source,
  but you'll have the chance to easily update your FreeLing to the
  latest development version.

\begin{list}{{\bf \arabic{enumi}.}}{\usecounter{enumi}\leftmargin=1.5em}
\setcounter{enumi}{0}

\item {\bf Install development tools}

 You'll need to install the C++ compiler, the GNU autotools, and a SVN client.
\begin{verbatim}
 sudo apt-get install build-essential automake autoconf libtool subversion
\end{verbatim}

   If you use a distribution different than Debian or Ubuntu, these packages
  may have different names. Use your package manager to locate and install
  the appropriate ones.
  
\item {\bf Install packaged requirements}

  Follow the same procedure described in section \ref{sec-install-tgz} for
  this step.

\item {\bf Checkout FreeLing sources}

  If you want the latest development version, do:
\begin{verbatim}
 svn checkout http://devel.cpl.upc.edu/freeling/svn/trunk myfreeling
\end{verbatim}
\noindent (you can replace myfreeling with the directory name of your choice).

  Alternatively, you can get any previous release with a command like:
\begin{verbatim}
 svn checkout http://devel.cpl.upc.edu/freeling/svn/versions/freeling-3.0-alfa1 myfreeling
\end{verbatim}
\noindent (just replace \verb#3.0-alfa1# with the right version
number). You can see which versions are available by pointing your web
browser to \verb#http://devel.cpl.upc.edu/freeling/svn/versions#\footnote{Note
  that FreeLing-2.x versions require the appropiate {\tt libomlet} and
  {\tt libfries} versions (which you can checkout from the SVN in the
  same way).  Check the {\sl Download} section in FreeLing webpage to
  find out which {\sl Omlet\&Fries} version is suitable for each 2.x
  FreeLing version.}.

\item {\bf Prepare local repositories for compilation}
\begin{verbatim}
 cd myfreeling
 aclocal; libtoolize; autoconf; automake -a
\end{verbatim}

\item {\bf Build and install FreeLing}
\begin{verbatim}
 ./configure
 make
 sudo make install
\end{verbatim}

\end{list}

\noindent If you keep the svn directories, you will be able to update
to newer versions at any moment:
\begin{verbatim}
 cd myfreeling
 svn update
 ./configure
 make
 sudo make install
\end{verbatim}
 Depending on what changed in the repository, you may need to issue
 \verb#aclocal; autoconf; automake -a# after \verb#svn update#.  You
 may also need to issue \verb#make distclean# and repeat the process
 from \verb#./configure# onwards.

\subsection{Locale-related problems when installing}

  If you get an error about {\tt bad locale} when you enter {\tt make
    install} or when you try to execute the {\tt analyzer} sample
  program, you probably need to generate some locales in your system.
  
  FreeLing uses \verb#en_US.UTF8# locale as default during
  installation. If this locale is not installed in your system, you'll
  get an error during dictionary installation.

  Most languages in FreeLing will work with this locale, but Russian
  will need to have its own locale installed in the system.

  The procedure to install a locale in your system varies depending on
  your distribution. For instance:
 \begin{itemize}
  \item In Ubuntu, you must use the \verb#locale-get# command. E.g.: \\
    ~~ \verb#sudo locale-gen en_US.UTF8#\\
    ~~ \verb#sudo locale-gen pt_BR.UTF8#\\
    ~~ \verb#sudo locale-gen ru_RU.UTF8#\\
    ~~ ...

   \item In Debian, you need to run the command: \\
     ~~ \verb#dpkg-reconfigure locales#\\ 
     ~~ and select the desired locales from the list.
  \end{itemize}

\subsection{Installing on MacOS}

  Installing on MacOS is very similar to installing on Linux. The main
  difference is how to install the dependencies and required
  development tools, which is greatly eased by MacPorts.

\begin{itemize}
 \item Download and install MacPorts following the instructions in
   \texttt{www.macports.org/install.php}.  Note that you will need to
   install Apple XCode too, as described in the same page.
 \item Use MacPorts to install required developer tools:
 \begin{verbatim}
  sudo port install automake
  sudo port install libtoool
  sudo port install subversion
 \end{verbatim}
 \item Use MacPorts to install required dependencies
 \begin{verbatim}
  sudo port install boost
 \end{verbatim}
  This will install also libicu. Note that zlib is already installed
  in MacOS.  If configure complains about it not being there, you can
  install it with \texttt{sudo port install zlib}.

 \item Compile and install FreeLing using the procedures described
   above (either ``install from source'' or ``install from SVN''), but
   skipping the steps about installing development tools and
   dependencies.
  
  Important: libraries in MacOS are installed in \texttt{/opt/local}
  instead of \texttt{/usr/local}. So, when running \texttt{configure},
  you need to specify the right library paths.  Also, locales need
  some specific handling which requires the use of
  \texttt{libboost-locale}

   In summary, you need to run \texttt{./configure} with the command:
 \begin{verbatim}
./configure --enable-boost-locale CPPFLAGS="-I/opt/local/include" 
                                               LDFLAGS="-L/opt/local/lib"
 \end{verbatim}
   You can add to this command any extra options you wish
   (\texttt{--enable-traces}, \texttt{--prefix}, etc).  Use
   \texttt{./configure --help} to find out available options.

\end{itemize}

%..................................................
\section{Executing}
\label{sec-execute}

  FreeLing is a library, which means that it not a final-user oriented
  executable program but a tool to develop new programs that
  require linguistic analysis services.

  Nevertheless, a sample main program is included in the package for
 those who just want a text analyzer. This program may 
 be adapted to fit your needs (e.g. customized input/output formats).
  
  The usage and options of this main program is described in chapter
  \ref{cap-analyzer}.
  
  Please take into account that this program is only a friendly
  interface to demonstrate FreeLing abilities, but that there are
  many other potential usages of FreeLing.  

  Thus, the question is not {\sl why this program doesn't offer
    functionality X?}, {\sl why it doesn't output information Y?}, or
  {\sl why it doesn't present results in format Z?}, but {\sl How should
    I use FreeLing to write a program that does exactly what I need?}.
    
  In the directory {\tt src/main/simple\_examples} in the tarball, you
 can find simpler sample programs that illustrate how to call the library, 
 and that can be used as a starting point to develop your own application.

%..................................................
\section{Porting to other platforms}

The FreeLing library is entirely written in C++, so it should be
possible to compile it on non-unix platforms with a reasonable
effort.

Success have been reported on compiling FreeLing on MacOS, as well as on
MS-Windows using cygwin (http://www.cygwin.com/).

Also, since version 3.0, project files are provided that allow
to compile FreeLing under MSVC. Note that this files are contributor
provided, and not supported by FreeLing developers. You'll find the
files and a thorough README in the {\tt msvc} folder inside FreeLing
tarball.

You can visit the {\sl Forum\&FAQs} sections in
FreeLing webpage for further help and details.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Analysis Modules}
\label{chap-modules}

 This chapter describes each of the modules in FreeLing. For each
 module, its public API is described, as well as its main
 configuration options. Most modules can be customized via a
 configuration file.

 A typical module receives a list of sentences, and enriches them with
 new linguistic information (morphological analysis, disambiguation,
 parsing, etc.)

 Usually, when the module is instantiated, it receives as a parameter
 the name of a file where the information and/or parameters needed by
 the module is stored (e.g. a dictionary file for the dictionary
 search module, or a CFG grammar for a parser module).
 
 Most modules are language-independent, that is, if the provided file
 contains data for another language, the same module will be able to
 process that language.

 All modules are thread-safe, except the sentence splitter when used
 in non-flush mode (see section \ref{file-split} below).

  If an application needs to process more than one language, it can 
 instantiate the needed modules for each language simply calling the 
 constructors with different data files as a parameter.

%..................................................
\section{Language Identifier Module}
\label{lang-ident}

  This module is somehow different of the other modules, since it
  doesn't enrich the given text.  It compares the given text with
  available models for different languages, and returns the most
  likely language the text is written in.
  It can be used as a preprocess to determine which data files are to
  be used to analyze the text.

\noindent  The API of the language identifier is the following:

\begin{verbatim}
class lang_ident {
  public:
    /// Build an empty language identifier.
    lang_ident();
    /// Build a language identifier, read options from given file.
    lang_ident(const std::wstring &);
    /// load given language from given model file, add to existing languages.
    void add_language(const std::wstring&);
    /// train a model for a language, store in modelFile, and add 
    /// it to the known languages list.
    void train_language(const std::wstring &, const std::wstring &, 
                        const std::wstring &);
    /// Classify the input text and return the code of the best language (or "none")
    std::wstring identify_language (
                    const std::wstring&, 
                    const std::set<std::wstring> &ls=std::set<std::wstring>()) const; 
    /// fill a vector with sorted probabilities for each language
    void rank_languages (
               std::vector<std::pair<double,std::wstring> > &, 
               const std::wstring &,
               const std::set<std::wstring> &ls=std::set<std::wstring>()) const;
};
\end{verbatim}

  Once created, the language identifier may be used to get the most
  likely language of a text (\verb#identify_language#) or to return a
  sorted vector of probabilities for each language
  (\verb#rank_languages#).  In both cases, a set of languages to be
  considered may be supplied, telling the identifier to apply to the
  input text only models for those languages in the list. An empty
  list is interpreted as ``use all available language models''.
  The language list parameter is optional in both identification methods, 
  and defaults to the empty list.

  The same \verb#lang_ident# class may be used to train models for new
  languages. The method \verb#train_language# will use a plain text
  file to create a new model, which will enlarge the identifier's
  language repertoire, and will be stored for its use in future
  instances of the class.

  The constructor expects a configuration file name, containing
  information about where are the language models located, and some
  parameters. The contents of that file are described below.

\subsection{Language Identifier Options File}

  The language identifier options file is divided in three sections:
  \verb#<Languages>#, \verb#<Threshold>#, and \verb#<ScaleFactor>#,
  which are closed by \verb#</Languages>#, \verb#</Threshold>#, and
  \verb#</ScaleFactor>#, respectively.

  Section \verb#<Languages># contains a list of filenames, one per
  line. Each filename contains a language model (generated with the
  \verb#train_language# method). The filenames may be absolute or
  relative. If relative, they are considered to be relative to the
  location of the identifier options file.

  Section \verb#<Threshold># and \verb#<ScaleFactor># contain one
  single line each, consisting of a real number in both cases.

  The identifier uses a 4-gram visible Markov model to compute the
  probability of the text in each candidate language. Since the
  probabilitity of a sequence depends on its length, the result is
  divided by the text length to obtain a per-char ``averaged''
  probability. Even in this way, the resulting probability is usually
  low and unintuitive. The parameter \verb#ScaleFactor# multiplies
  this result to enlarge the difference between languages and to give
  probabilities in a more human scale.  The parameter \verb#Threshold#
  states minimun value that a language must achive to be considered a
  possible result. If no language reaches the threshold, the
  \verb#identify_language# method will return {\tt none}.

  Note that this scaling is artificial, and doesn't change the
  results, only makes them more readable. The results with {\tt
    ScaleFactor=1.0} and {\tt Threshold=0.04} would be the same than
  with {\tt ScaleFactor=5.0} and {\tt Threshold=0.2}.

 An example of a language identifier option file is:
\begin{verbatim}
   <Languages>
   ./es.dat
   ./ca.dat
   ./it.dat
   ./pt.dat
   </Languages>
   <Threshold>
   0.2
   </Threshold>
   <ScaleFactor>
   5.0
   </ScaleFactor>
\end{verbatim}


%..................................................
\section{Tokenizer Module}
\label{file-tok}

  The first module in the processing chain is the tokenizer. It
  converts plain text to a vector of {\tt word} objects, according to
  a set of tokenization rules.

  Tokenization rules are regular expressions that are matched against
  the beggining of the text line being processed. The first matching
  rule is used to extract the token, the matching substring is deleted
  from the line, and the process is repeated until the line is empty.

  The API of the tokenizer module is the following:
\begin{verbatim}
class tokenizer {
  public:
    /// Constructor
    tokenizer(const std::wstring &);

    /// tokenize string 
    void tokenize(const std::wstring &, std::list<word> &) const;
    /// tokenize string, return result as list
    std::list<word> tokenize(const std::wstring &) const;
    /// tokenize string, tracking offset
    void tokenize(const std::wstring &, unsigned long &, std::list<word> &) const;
    /// tokenize string, tracking offset, return result as list
    std::list<word> tokenize(const std::wstring &, unsigned long &) const;
};
\end{verbatim}

  That is, once created, the tokenizer module receives plain text in a
  string, tokenizes it, and returns a list of {\tt word} objects
  corresponding to the created tokens

\subsection{Tokenizer Rules File}

  The tokenizer rules file is divided in three sections
  \verb#<Macros>#, \verb#<RegExps># and \verb#<Abbreviations>#.  Each
  section is closed by \verb#</Macros>#, \verb#</RegExps># and
  \verb#</Abbreviations># tags respectively.
  
   The \verb#<Macros># section allows the user to define regexp macros
   that will be used later in the rules. Macros are defined with a name and
   a POSIX regexp. E.g.:\\
   \verb#  MYALPHA  [A-Za-z]#\\
   \verb#  ALPHA    [[:alpha:]]#

    The \verb#<RegExps># section defines the tokenization
   rules. Previously defined macros may be referred to with their name
   in curly brackets. E.g.:\\
   \verb#  *ABREVIATIONS1  0  ((\{ALPHA\}+\.)+)(?!\.\.)#

   Rules are regular expressions, and are applied in the order of definition. 
   The first rule matching the {\em beginning} of the line is applied,
   a token is built, and the rest of the rules are ignored.
   The process is repeated until the line has been completely processed.

   The format of each rule is:
  \begin{itemize}
    \item The first field in the rule is the rule name. If it starts
      with a {\tt *}, the RegExp will only produce a token if the
      match is found in the abbreviation list (\verb#<Abbreviations>#
      section).  Apart from that, the rule name is only for
      informative/readability purposes.

    \item The second field in the rule is the substring to form the
      token/s with.  It may be 0 (the match of the whole expression)
      or any number from 1 to the number of subexpression (up to 9). A
      token will be created for each subexpression from 1 to the
      specified value.

    \item The third field is the regexp to match against the input.
      line. Any POSIX regexp convention may be used.

    \item An optional fourth field may be added, containing the string
      {\tt CI} (standing for {\sl Case Insensitive}). In this case,
      the input text will be matched case-insensitively against the
      regexp.  If the fourth field is not present, or it is different
      than {\tt CI}, the rule is matched case-sensitively.
  \end{itemize}

  The \verb#<Abbreviations># section defines common abbreviations (one
  per line) that must not be separated of their following dot
  (e.g. {\tt etc.}, {\tt mrs.}). They must be lowercased, even if
  they are expected to appear uppercased in the text.

%..................................................
\section{Splitter Module}
\label{file-split}

  The splitter module receives lists of {\tt word} objects (either
  produced by the tokenizer or by any other means in the calling
  application) and buffers them until a sentence boundary is detected.
  Then, a list of {\tt sentence} objects is returned.

  The buffer of the splitter may retain part of the tokens if the
  given list didn't end with a clear sentence boundary.  The caller
  application can submit further token lists to be added, or request
  the splitter to flush the buffer.

  Note that the splitter is not thread-safe when the buffer is not
  flushed at each call.

 The API for the splitter class is:
\begin{verbatim}
class splitter {
  public:
    /// Constructor. Receives a file with the desired options
    splitter(const std::string &);

    /// Add list of words to the buffer, and return complete sentences 
    /// that can be build.
    /// The boolean states if a buffer flush has to be forced (true) or
    /// some words may remain in the buffer (false) if the splitter 
    /// needs to wait to see what is coming next.
    std::list<sentence> split(const std::list<word> &, bool);
    /// split given list, add resulting sentence to output parameter
    void split(const std::list<word> &, bool, std::list<sentence> &);
};
\end{verbatim}

\subsection{Splitter Options File}

The splitter options file contains four sections: \verb#<General>#, \verb#<Markers>#,
\verb#<SentenceEnd>#, and \verb#<SentenceStart>#.

The \verb#<General># section contains general options for the
splitter: Namely, {\tt AllowBetweenMarkers} and {\tt MaxWords}
options. The former may take values 1 or 0 (on/off). The
later may be any integer. An example of the  \verb#<General># section is:
\begin{verbatim}
<General>
AllowBetweenMarkers 0
MaxWords 0
</General>
\end{verbatim}

If {\tt AllowBetweenMarkers} is off ({\tt 0}), a sentence split will never be
introduced inside a pair of parenthesis-like markers, which is useful
to prevent splitting in sentences such as {\em ``I hate'' (Mary
said. Angryly.) ``apple pie''.}
 If this option is on ({\tt 1}), sentence splits will be introduced as if 
they had happened outside the markers.

{\tt MaxWords} states how many words are processed before forcing a
sentence split inside parenthesis-like markers (this option is
intended to avoid memory fillups in case the markers are not properly
closed in the text).  A value of zero means ``Never split, I'll risk
to a memory fillup''.  This option is less aggressive than
unconditionally activating {\tt AllowBetweenMarkers}, since it will
introduce a sentence split between markers only after a sentence of
length {\tt MaxWords} has been accumulated. Setting {\tt MaxWords} to 
a large value will prevent memory fillups, while keeping at a minimum the 
splittings inside markers.

The \verb#<Markers># section lists the pairs of characters (or
character groups) that have to be considered open-close markers. For instance:
\begin{verbatim}
<Markers>
" "
( )
{ }
/* */
</Markers>
\end{verbatim}

The \verb#<SentenceEnd># section lists which characters are considered
as possible sentence endings. Each character is followed by a binary
value stating whether the character is an unambiguous sentence ending
or not. For instance, in the following example, ``?'' is an unambiguous
sentence marker, so a sentence split will be introduced
unconditionally after each ``?''.  The other two characters are not
unambiguous, so a sentence split will only be introduced if they are
followed by a capitalized word or a sentence start character.
\begin{verbatim}
<SentenceEnd>
. 0
? 1
! 0
</SentenceEnd>
\end{verbatim}

The \verb#<SentenceStart># section lists characters known to appear
only at sentence beggining. For instance, open question/exclamation
marks in Spanish:\\
\verb#<SentenceStart>#\\
{\tt ?` }\\
{\tt !` }\\
\verb#</SentenceStart>#

%..................................................
\section{Morphological Analyzer Module}
\label{sec-maco}

  The morphological analyzer is a meta-module which does not perform
  any processing of its own.

  It is just a convenience module to simplify the instantiation and
  call to the submodules described in the next sections (from
  \ref{file-numb} to \ref{file-prob}).

  At instantiation time, it receives a {\tt maco\_options} object,
  containing information about which submodules have to be created and
  which files must be used to create them.

  A calling application may bypass this module and just call directly
  the submodules.

\noindent The Morphological Analyzer API is:
\begin{verbatim}
class maco {
  public:
    /// Constructor. Receives a set of options.
    maco(const maco_options &); 

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

\noindent The {\tt maco\_options} class has the following API:
\begin{verbatim}
class maco_options {
  public:
    /// Language analyzed
    std::string Lang;

    /// Submodules to activate
    bool UserMap, AffixAnalysis, MultiwordsDetection, 
         NumbersDetection, PunctuationDetection, 
         DatesDetection,   QuantitiesDetection, 
         DictionarySearch, ProbabilityAssignment,
         NERecognition;

    /// Names of data files to provide to each submodule.
    std::string UserMapFile, LocutionsFile, QuantitiesFile,
            AffixFile, ProbabilityFile, DictionaryFile, 
            NPdataFile, PunctuationFile;

    /// module-specific parameters for number recognition
    std::wstring Decimal, Thousand;
    /// module-specific parameters for probabilities
    double ProbabilityThreshold;
    /// module-specific parameters for dictionary
    bool InverseDict,RetokContractions;

    /// constructor
    maco_options(const std::string &); 

    /// Option setting methods provided to ease perl interface generation. 
    /// Since option data members are public and can be accessed directly
    /// from C++, the following methods are not necessary, but may become
    /// convenient sometimes.
    /// The order of the parameters is the same than the variables defined above.
    void set_active_modules(bool,bool,bool,bool,bool,bool,bool,bool,bool,bool);
    void set_data_files(const std::wstring &,const std::wstring &,const std::wstring &,
                        const std::wstring &,const std::wstring &,const std::wstring &,
                        const std::wstring &,const std::wstring &);
    void set_nummerical_points(const std::string &,const std::string &);
    void set_threshold(double);
    void set_inverse_dict(bool);
    void set_retok_contractions(bool);
\end{verbatim}

  To instantiate a Morphological Analyzer object, the calling
  application needs to instantiate a {\tt maco\_options} object,
  initialize its fields with the desired values, and use it to call
  the constructor of the {\tt maco} class.

  The created object will create the required submodules, and when
  asked to {\tt analyze} some sentences, it will just pass it down to
  each the submodule, and return the final result.

  The {\tt maco\_options} class has convenience methods to set the
  values of the options, but note that all the members are public, so
  the user application can set those values directly if preferred.

%..................................................
\section{Number Detection Module}
\label{file-numb}

  The number detection module is language dependent: It recognizes
  nummerical expression (e.g.: {\tt 1,220.54} or {\tt two-hundred
    sixty-five}), and assigns them a normalized value as lemma.

  The module is basically a finite-state automata that recognizes
  valid nummerical expressions. Since the structure of the automata
  and the actions to compute the actual nummerical value are different
  for each lemma, the automata is coded in C++ and has to be rewritten 
  for any new language.

   For languages that do not have an implementation of a specific
   automata, a generic module is used to recognize number-like
   expressions that contain nummerical digits.

   There is no configuration file to be provided to the class when it
   is instantiated. The API of the class is:
\begin{verbatim}  
class numbers {
  public:
    /// Constructor: receives the language code, and the decimal 
    /// and thousand point symbols
    numbers(const std::string &, const std::string &, const std::string &); 

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}  

   The parameters that the constructor expects are: 
\begin{itemize}
\item The language code: used to decide whether the generic recognizer
  or a language-specific module is used.
\item The decimal point symbol.
\item The thousand point sympol.
\end{itemize}
  The last two parameters are needed because in some latin languages,
  the comma is used as decimal point separator, and the dot as
  thousand mark, while in languages like English it is the other way
  round.  These parameters make it possible to specify what character
  is to be expected at each of these positions. They will usually be
  comma and dot, but any character could be used.


%..................................................
\section{Punctuation Detection Module}
\label{file-punt}

 The punctuation detection module assigns Part-of-Speech tags to
 punctuation symbols. The API of the class is the following:
\begin{verbatim}  
class punts {
  public:
    /// Constructor: receives data file name
    punts(const std::string &); 

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

 The constructor receives as parameter the name of a file containing
 the list of the PoS tags to be assigned to each punctuation symbol.

 Note that this module will be applied afer the tokenizer, so, it will
 only annotate symbols that have been separated at the tokenization
 step. For instance, if you include the three suspensive dots (\ldots)
 as a single punctuation symbol, it will have no effect unless the
 tokenizer has a rule that causes these substring to be tokenized in
 one piece.

\subsection{Punctuation Tags File}

 The format of the file listing the PoS for punctuation symbols is one
 punctuation symbol per line, each line with the format: {\tt
   punctuation-symbol lemma tag}.\\
 E.g.: 
\begin{verbatim}  
   ! ! Fat
   , , Fc
   : : Fd
   ... ... Fs
\end{verbatim}

  One special line may be included defining the tag that will be assigned to any other 
  punctuation symbol not found in the list. Any token containing no alphanumeric character 
  is considered a punctuation symbol.
  This special line has the format: \verb#<Other> tag#.\\
\noindent E.g.\\
    \verb#<Other> Fz#

%..................................................
\section{User Map Module}
\label{file-usermap}

 The user map module assigns Part-of-Speech tags to words matching a
 given regular expression. It can be used to customize the behaviour
 of the analysis chain to specific applications, or to process
 domain-specific special tokens. The API of the class is the
 following:
\begin{verbatim}  
class RE_map {
  public:
    /// Constructor
    RE_map(const std::wstring &); 
 
    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

 The constructor receives as parameter the name of a file containing a
 list of regular expressions, and the list of pairs lemma-PoS tag to be
 assigned to each word matching the expression.

 Note that this module will be applied afer the tokenizer, so, it will
 only annotate symbols that have been separated at the tokenization
 step. So, customizing your application to recognize certain special
 tokens will require modifying also the tokenizer configuration file.

 Note also that if you introduce in this file PoS-tags which are not
 in the tagset known to the tagger, it may not be able to properly
 disambiguate the tag sequence.

 Note that this module sequentially checks each regular expression in
 the list against each word in the text. Thus, it should be used for
 patterns (not for fixed strings, which can be included in a
 dictionary file), and with moderation: using a very long list of
 expressions may severely slow down your analysis chain.

\subsection{User Map File}

 The format of the file containing the user map from regular
 expression to pairs lemma-PoS is one regular expression per line,
 each line with the format: {\tt regex lemma1 tag1 lemma2 tag2 ...}.
 
 The lemma may be any string literal, or \verb#$$# meaning that the
 string matching the regular expression is to be used as a
 lemma.\\
 E.g.:
\begin{verbatim}  
   @[a-z][0-9] $$ NP00000
   <.*> XMLTAG Fz
   hulabee hulaboo JJS hulaboo NNS
\end{verbatim}

  The first rule will recognize tokens such as \verb#@john# or
  \verb#@peter4#, and assign them the tag \verb#NP00000# (proper noun)
  and the matching string as lemma.

  The second rule will recognize tokens starting with ``\verb#<#'' and
  ending with ``\verb#>#'' (such as \verb#<HTML># or \verb#<br/>#) and
  assign them the literal \verb#XMLTAG# as lemma and the tag \verb#Fz#
  (punctuation:others) as PoS.

  The third rule will assign the two pairs lemma-tag to each occurrence
  of the word ``hulabee''. This is just an example, and if you want to
  add a word to your dictionary, the dictionary module is the right place.
  

%..................................................
\section{Dates Detection Module}
\label{file-dates}

  The dates detection module, as the number detection module in
  section \ref{file-numb}, is a collection of language-specific
  finite-state automata, and for this reason needs no data file to be
  provided at instantiation time.

  For languages that do not have a specific automata, a default
  analyzer is used that detects simple date patterns (e.g. {\tt
    DD-MM-AAAA}, {\tt MM/DD/AAAA}, etc.)

  The API of the class is:
\begin{verbatim}        
class dates {             
  public:   
    /// Constructor: receives the language code
    dates(const std::string &); 

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};  
\end{verbatim}        

   The only parameter expected by the constructor is the language of
   the text to analyze, in order to be able to apply the appropriate
   specific automata, or select the default one if none is available.


%..................................................
\section{Dictionary Search Module}
\label{file-dict}

The dictionary search module has two functions: Search the word forms
in the dictionary to find out their lemmas and PoS tags, and apply
affixation rules to find the same information in the cases in which
the form is a derived form not included in the dictionary (e.g. the
word {\tt quickly} may not be in the dictionary, but a suffixation
rule may state that removing {\tt -ly} and searching for the obtained
adjective is a valid way to form and adverb).

The decision of what is included in the dictionary and what is dealt
with through affixation rules is left to the linguist building the 
linguistic data resources.

\noindent The API for this module is the following:
\begin{verbatim}
class dictionary {
  public:
    /// Constructor
    dictionary(const std::wstring &, const std::wstring &, 
               bool, const std::wstring &, bool invDic=false, bool retok=true);
    /// Destructor
    ~dictionary();

    /// add analysis to dictionary entry (create entry if not there)
    void add_analysis(const std::wstring &, const analysis &);
    /// remove entry from dictionary
    void remove_entry(const std::wstring &);

    /// Get dictionary entry for a given form, add to given analysis list.
    void search_form(const std::wstring &, std::list<analysis> &) const;
    /// Fills the analysis list of a word, checking for suffixes and contractions.
    /// Returns true iff the form is a contraction.
    bool annotate_word(word &, std::list<word> &, bool override=false) const;
    /// Fills the analysis list of a word, checking for suffixes and contractions.
    /// Never retokenizing contractions, nor returning component list.
    /// It is just a convenience equivalent to "annotate_word(w,dummy,true)"
    void annotate_word(word &) const;
    /// Get possible forms for a lemma+pos (only if created with invDic=true)
    std::list<std::wstring> get_forms(const std::wstring &, const std::wstring &) const;

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
}
\end{verbatim}

   The parameters of the constructor are:
\begin{itemize}
 \item The language of the processed text. This is required by the
   affixation submodule to properly handle graphical accent rules in
   latin languages.
 \item The dictionary file name. See below for details.
 \item A boolean stating whether affixation analysis has to be
   applied.
 \item The affixation rules file name (it may be an empty string if
   the boolean above is set to false)
 \item An optional boolean (default: false) stating whether the dictionary
   must be created with inverse access, to enable the use of \verb#get_forms# 
   to obtain form given lemma+pos.
 \item An optional boolean (default: true) stating whether the contractions found
   in the dictionary must be retokenized right away, or left for later modules to
   decide.
 \end{itemize}

\subsection{Form Dictionary File}

  The form dictionary contais two required sections:
  \verb#<IndexType># and \verb#<Entries>#, and two optional sections:
  \verb#<LemmaPreferences># and \verb#<PosPreferences>#

  Section \verb#<IndexType># conatins a single line, that may be either
  \verb#DB_PREFTREE# or \verb#DB_MAP#.  With \verb#DB_MAP# the dictionary is
  stored in a C++ STL \texttt{map} container. With \verb#DB_PREFTREE#
  it is stored in a prefix-tree structure.
  Depending on the size of the dictionary and on the morphological
  variation of the language, one structure may yield slightly better
  access times than the other.

  Optional sections \verb#<LemmaPreferences># and
  \verb#<PosPreferences># contain a list of pairs of lemmas or PoS
  tags, respectively.
  The meaning of each pair is that the first element is prefereble to
  the second in case the tagger can not decide between them and is asked to.

  For instance, the section:
  \begin{verbatim}
    <LemmaPreferences>
    salir salgar
    </LemmaPreferences>
  \end{verbatim}
  solves the ambiguity for Spanish word \textsl{salgo}, which may correspond to 
  indicative first person singular of verb \textsl{salir} (go out), or to exactly the
  same tense of verb \textsl{salgar} (feed salt to cattle). 
  Since the PoS tag is the same for both lemmas, the tagger can not decide which is
  the right one. This preference solves the dilemma in favour of \textsl{salir} (go out).

  The section
  \begin{verbatim}
    <PosPreferences>
    VMII3S0 VMII1S0
    </PosPreferences>
  \end{verbatim}
  helps solving cases as the past tense for Spanish verbs such as \textsl{cantaba} 
  (I/he sung), which are shared by first and third person. 
  In this case, if the tagger is not able to make a decision (it may be it doesn't 
  take into account the person as a feature), a preference is set for 3rd person 
  (which is more frequent in standard text).
  
  Section \verb#<Entries># contains lines with one form per line.  
  Each form line has format: {\tt form lemma1 PoS1 lemma2 PoS2 ...}.  \\
  E.g.:
  \begin{verbatim}
  casa casa NCFS000 casar VMIP3S0 casar VMM02S0
  backs back NNS back VBZ
  \end{verbatim}
  Lines corresponding to words that are contractions may have an
  alternative format if the contraction is to be splitted. The format
  is {\tt form form1+form2+... PoS1+PoS2+...}\\
  where {\tt form1,form2,...} are the forms (not the lemmas) of the contracted words.
  For instance:\\
  \verb#del de+el SP+DA#
  
  This line expresses that whenever the form {\sl del} is found, it is
  replaced with two words: {\sl de} and {\sl el}. Each of the new two
  word forms are searched in the dictionary, and assigned any tag
  matching their correspondig tag in the third field. So, {\sl de}
  will be assigned all tags starting with {\tt SP} that this
  entry may have in the dictionary, and {\sl el} will get any
  tag starting with {\tt DA}.
  
  Note that a contraction included in the dictionary cannot be 
  splitted in two different ways corresponding to different 
  forms (e.g. {\tt he's = he+is | he+has}),
  so only a combination of forms and a combination of tags may appear
  in the dictionary.

   Nevertheless, a set of tags may be specified for a given form, e.g.:\\
  \verb#he'd he+'d PRP+VB/MD#

   This will produce two words: {\sl he} with {\tt PRP} analysis, and
   {\sl 'd} with its analysis matching any of the two given tags
   (i.e. {\tt have\_VBZ} and {\tt would\_MD}).  Note that this will
   work only if the form {\sl 'd} is found in the dictionary with
   those possible analysis.

   If all tags for one of the new forms are to be used, a wildcard may
   be written as a tag. e.g.:\\
  \verb#pal para+el SP+*#

   This will replace {\sl pal} with two words, {\sl para} with only its
   {\tt SP} analysis, plus {\sl el} with all its possible tags.  

  The contraction entries in the dictionary are intended for
  inambiguous contractions, or for cases such that it is not worth (or
  it is too difficult) to handle otherwise.  For splitting more
  sophisticated compound words, such as verb clitic pronouns in
  Spanish or Italian (e.g {\sl dale $\rightarrow$ dar+\'el}), derivation (e.g. {\sl
    quick $\rightarrow$ quickly}, {\sl r\'apida $\rightarrow$ r\'apidamente}),
  diminutive/augmentative sufixes, prefixes, or other similar
  behaviours, the affixation module should be used (see section
  \ref{file-suf} for details).

  An optional parameter in the constructor enables to control whether
  contractions are splitted by the dictionary module itself (thus
  passing two words instead of one to later modules) or the decision
  is left to later modules (which will receive a single word carrying
  retokenization information).

\subsection{Affixation Rules File}
\label{file-suf}

 The submodule of the dictionary handler that deals with affixes
 requires a set of affixation rules.

 The file consists of two (optional) sections: \verb#<Suffixes># and
 \verb#<Prefixes>#. The first one contains suffixation rules, and the
 second, prefixation rules. They may appear in any order.

 Both kinds of rules have the same format, and only differ in whether
 the affix is checked at the beggining or at the end of the word.

Each rule has to be written in a different line, and has 10 fields:
\begin{enumerate}
 \itemsep 0cm
 \item Affix to erase form word form (e.g: crucecita - cecita = cru)
 \item Affix (* for emtpy string) to add to the resulting root to
   rebuild the lemma that must be searched in dictionary (e.g. cru + z
   = cruz)
 \item Condition on the tag of found dictionary entry (e.g. cruz is
   NCFS). The condition is a perl RegExp
 \item Tag for suffixed word (* = keep tag in dictionary entry)
 \item Check lemma adding accents
 \item Enclitic suffix (special accent behaviour in Spanish)
 \item Prevent later modules (e.g. probabilities) from assigning
   additional tags to the word
 \item Lemma to assign: Any combination of: {\tt F}, {\tt R}, {\tt L},
   {\tt A}, or a string literal separated with a {\tt +} sign. For
   instance: {\tt R+A}, {\tt A+L}, {\tt R+mente}, etc.

  {\tt F} stands for the original form (before affix removal,
  e.g. {\em crucecitas}), {\tt R} stands for root found in dictionary
  (after affix removal and root reconstruction, e.g. {\em cruces}),
  {\tt L} stands for lemma in matching dictionary entry (e.g. {\em
    cruz}), {\tt A} stands for the affix that the rule removed

 \item Try the affix always, not only for unknown words.

 \item Retokenization info, explained below (``\verb#-#'' for none)
\end{enumerate}
\medskip

 \noindent \textit{Example of prefix rule}\\
 \verb#anti    *     ^NC    AQ0CN0   0  0  1  A+L 0  -#

  This prefix rule states that {\tt anti} should be removed from the
  beggining of the word, nothing ({\tt *}) should be added, and the
  resulting root should be found in the dictionary with a NC PoS
  tag. If that is satisfied, the word will receive the {\tt AQ0CN0}
  tag and its lemma will be set to the affix ({\tt anti}) plus the
  lemma of the root found in the dictionary.  For instance, the word
  {\tt antimisiles} would match this rule: {\tt misiles} would be
  found in the dictionary with lema {\tt misil} and PoS {\tt
    NCMP000}. Then, the word will be assigned the lemma {\tt
    antimisil} ({\tt A+L = anti+misil}) and the tag AQ0CN0.
\medskip

 \noindent \textit{Examples of sufix rules}\\
  \verb#cecita  z|za  ^NCFS  NCFS00A  0  0  1  L   0  -#\\
  \verb#les     *     ^V      *       0  1  0  L   1  $$+les:$$+PP#

  The first suffix rule above ({\tt cecita}) states a suffix rule that
  will be applied to unknown words, to see whether a valid feminine
  singular noun is obtained when substituting the suffix {\tt cecita}
  with {\tt z} ot {\tt za}. This is the case of {\tt crucecita}
  (diminutive of {\tt cruz}). If such a base form is found, the
  original word is analyzed as diminutive suffixed form. No
  retokenization is performed.

  The second rule ({\tt les}) applies to all words and tries to check
  whether a valid verb form is obtained when removing the suffix {\tt
    les}. This is the case of words such as {\tt viles} (which may
  mean {\sl I saw them}, but also is the plural of the adjective {\tt
    vil}). In this case, the retokenization info states that if
  eventually the verb tag is selected for this word, it may be
  retokenized in two words: The base verb form (referred to as {\tt
    \$\$}, {\tt vi} in the example) plus the word {\tt les}. The tags
  for these new words are expressed after the colon: The base form
  must keep its PoS tag (this is what the second {\tt \$\$} means) and
  the second word may take any tag starting with PP it may have in the
  dictionary.

  So, for word {\tt viles}, we would obtain its adjective analysis
  from the dictionary, plus its verb + clitic pronoun from the suffix
  rule:\\ 
  \verb#  viles vil AQ0CP0 ver VMIS1S0#

  The second analysis will carry the retokenization information, so if
  eventually the PoS tagger selects the {\tt VMI} analysis (and the
  TaggerRetokenize option is set), the word will be retokenized into:
\begin{verbatim}
   vi ver VMIS1S0
   les ellos PP3CPD00
\end{verbatim}

\subsection{Dictionary Management}

 In many NLP applications you want to deal with text in a particular domain, 
which contain words or expressions (terminology, proper nouns, etc.) 
that are specific or have a particular meaning in that domain.
 
 Thus, you may want to extend you dictionary to include such
 words. There are two main ways of doing this:
\begin{itemize}
\item \textbf{Extend your dictionary:} Dictionaries are created at
  installation time from sources in the directory
  \texttt{data/XX/dictionary} (where XX is the language code).  Those
  files contain one triplet word-lemma-tag per line, and are fused in
  a single dictionary at installation time.

The script in \texttt{src/utilities/dicc-management/bin/build-dict.sh}
will read the given files and build a dictionary with all
them.\footnote{Check the file
  \texttt{src/utilities/dicc-management/README} for details} Thus, if
you have a domain dictionary with a list of triplets word-lemma-tag,
you can build a new dictionary fusing the original FreeLing entries
with your domain entries. The resulting file can be given to the
constructor of the dictionary class.

\item \textbf{Instantiate more than one dictionary module:} Another
  option is to instatiate several dictionary modules (creating each
  one with a different dictionary file), and run the text through each
  of them sequentially. When the word is found in one dictionary along
  the chain, the following dictionary modules will ignore it and will
  not attempt to look it up.
\end{itemize}


%..................................................
\section{Multiword Recognition Module}
\label{file-mw}

  This module aggregates input tokens in a single word object if they
  are found in a given list of multiwords.

  The API for this class is:
\begin{verbatim}

class locutions: public automat {
  public:
    /// Constructor, receives the name of the file
    ///  containing the multiwords to recognize.
    locutions(const std::string &);

    /// Detect multiwords starting at given sentence position
    bool matching(sentence &, sentence::iterator &);

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

  Class {\tt automat} implements a generic FSA. The {\tt locutions}
  class is a derived class which implements a FSA to recognize the
  word patterns listed in the file given to the constructor.

\subsection{Multiword Definition File}

The file contains a list of multiwords to be recognized. The format of
the file is one multiword per line. Each line has the format: \\
\verb#form lemma1 pos1 lemma2 pos2 ... [A|I]#

The multiword form may contain lemmas in angle brackets, meaning that
any form with that lemma will be considered a valid component for the
multiword.

The form may also contain PoS tags. Any uppercase component in the form
will be treated as a PoS tag.

 Any number of pairs lemma-tag may be assigned to the multiword. The
 PoS tagger will select the most probable given the context, as with
 any other word.

 For instance:
\begin{verbatim}
a_buenas_horas a_buenas_horas RG A
a_causa_de a_causa_de SPS00 I
<accidente>_de_trabajo accidente_de_trabajo $1:NC I
<acabar>_de_VMN0000 acabar_de_$L3 $1:VMI I
Z_<vez> TIMES:$L1 Zu I
\end{verbatim}
 
 The tag may be specified directly, or as a reference to the tag of
some of the multiword components. In the previous example, the third
multiword specification will build a multiword with any of the forms {\tt
 accidente de trabajo} or {\tt accidentes de trabajo}. The tag of the
multiword will be that of its first form ({\tt \$1}) which starts with
{\tt NC}.  This will assign the right singular/plural tag to the
multiword, depending on whether the form was ``accidente'' or ``accidentes''.

 The lemma of the multiword may be specified directly, or as a reference to the 
form of lemma of some of the multiword components.  In the previous example, the 
fourth multiword specification will build a multiword with phrases such as 
{\tt acabo de comer}, {\tt acababa de salir}, etc.  The lemma will be 
{\tt acabar\_de\_XXX} where {\tt XXX} will be replaced with the lemma of the 
third multiword component ({\tt \$L3}). 

 Lemma replacement strings can be {\tt \$F1}, {\tt \$F2}, {\tt \$F3},
 etc. to select the lowercased {\sl form} of any component, or {\tt
   \$L1}, {\tt \$L2}, {\tt \$L3}, etc. to select the {\sl lemma} of
 any component. Component numbers can range from 1 to 9.

 The last field states whether the multiword is ambiguous {\tt A} or
 not {\tt I} with respect to its segmentation (i.e. that it may be a
 multiword or not, depending on the context). The multiword is built
in any case, but the ambiguity information is stored in the {\tt word}
object, so the calling applicacion can consult it and take the necessary
decisions (e.g. un-glue the multiword) if needed.

%..................................................
\section{Named Entity Recognition Module}
\label{file-ner}

  There are two different modules able to perform NE recognition. 
  They can be instantiated directly, or via a wrapper that will create
  the right module depending on the configuration file.

  The API for the wrapper is the following:
\begin{verbatim}
class WINDLL ner {
  public:
    /// Constructor
    ner(const std::wstring &);
    /// Destructor
    ~ner();

    /// analyze given sentence
    void analyze(sentence &) const;
    /// analyze given sentences
    void analyze(std::list<sentence> &) const;
    /// analyze sentence, return analyzed copy
    sentence analyze(const sentence &) const;
    /// analyze sentences, return analyzed copy
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

  The parameter to the constructor is the absolute name of a
  configuration file, which must contain the desired module type
  (\verb#basic# or \verb#bio#) in a line enclosed by the tags
  \verb#<Type># and \verb#</Type>#.

  The rest of the file must contain the configuration options specific
  for the selected NER type, described below.

  The {\tt basic} module is simple and fast, and easy to adapt for use
  in new languages, provided capitalization is the basic clue for NE
  detection in the target language. The estimated performance of this
  module is about 85\% correctly recognized named entities.

  The {\tt bio} module, is based on machine learning algorithms.  It
  has a higher precision (over 90\%), but it is remarkably slower than
  {\tt basic}, and adaptation to new languages requires a training
  corpus plus some feature engineering.

\subsection{Basic NER module ({\tt np}) }

  The first NER module is the {\tt np} class, which is a just a FSA
  that basically detects sequences of capitalized words, taking into
  account some functional words (e.g. {\em Bank of England}) and
  capitalization at sentence begginings.
  
 It can be instantiated via the {\tt ner} wrapper described above, or
 directly via its own API:
\begin{verbatim}
class np: public ner_module, public automat {
  public:
    /// Constructor, receives a configuration file.
    np(const std::string &); 

    /// Detect multiwords starting at given sentence position
    bool matching(sentence &, sentence::iterator &) const;

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

  The file that controls the behaviour of the simple NE recognizer
  consists of the following sections:

\begin{itemize}
  \item Section \verb#<FunctionWords># lists the function words that can be
  embeeded inside a proper noun (e.g. preposisions and articles such
  as those in ``Banco de España'' or ``Foundation for the Eradication
  of Poverty''). For instance:
\begin{verbatim}
<FunctionWords>
el
la
los
las
de
del
para
</FunctionWords>
\end{verbatim}

 \item Section \verb#<SpecialPunct># lists the PoS tags (according to
  punctuation tags definition file, section \ref{file-punt}) after
  which a capitalized word {\sl may} be indicating just a sentence or clause
  beggining and not necessarily a named entity. Typical cases are
  colon, open parenthesis, dot, hyphen..
\begin{verbatim}
<SpecialPunct>
Fpa
Fp
Fd
Fg
</SpecialPunct>
\end{verbatim}

  \item Section \verb#<NE_Tag># contains only one line with the PoS tag that
  will be assigned to the recognized entities. If the NE classifier is
  going to be used later, it will have to be informed of this tag at
  creation time.
\begin{verbatim}
<NE_Tag>
NP00000
</NE_Tag>
\end{verbatim}

  \item Section \verb#<Ignore># contains a list of forms (lowercased)
    or PoS tags (uppercased) that are not to be considered a named
    entity even when they appear capitalized in the middle of a
    sentence.  For instance, the word {\em Spanish} in the sentence
    {\em He started studying Spanish two years ago} is not a named
    entity. If the words in the list appear with other capitalized
    words, they are considered to form a named entity (e.g. {\em An
      announcement of the Spanish Bank of Commerce was issued
      yesterday}). The same distinction applies to the word {\em I} in
    the sentences {\em whatever you say, I don't believe}, and {\em
      That was the death of Henry I}.

     Each word or tag is followed by a $0$ or $1$ indicating whether
     the {\sl ignore} condition is strict ($0$: non-strict, $1$:
     strict).  The entries marked as non-strict will have the
     behaviour described above.  The entries marked as strict will
     {\sl never} be considered named entities or NE parts.

     For instance, the following \verb#<Ignore># section states that
     the word ``I'' is not to be a proper noun ({\em whatever you say,
       I don't believe}) unless some of its neighbour words are ({\em
       That was the death of Henry I}). It also states that any word
     with the {\tt RB} tag, and any of the listed language names must
     {\sl never} be considered as possible NEs.
\begin{verbatim}
<Ignore>
i  0
RB 1
english 1
dutch 1
spanish 1
</Ignore>
\end{verbatim}

  \item Section \verb#<Names># contains a list of lemmas that may be
    names, even if they conflict with some of the heuristic criteria
    used by the NE recognizer. This is useful when they appear
    capitalized at sentence beggining. For instance, the basque name
    {\em Miren} (Mary) or the nickname {\em Pel\'e} may appear at the
    beggining of a Spanish sentence. Since both of them are verbal
    forms in Spanish, they would not be considered candidates to form
    named entities.
 
   Including the form in the \verb#<Names># section, causes the NE
   choice to be added to the possible tags of the form, giving the
   tagger the chance to decide whether it is actually a verb or a
   proper noun.
\begin{verbatim}
<Names>
miren
pelé
zapatero
china
</Names>
\end{verbatim}

  \item Section \verb#<Affixes># contains a list of words that may be
    part of a NE --either prefixing or suffixing it-- even if they are
    lowercased.  For instance, this is the case of the word {\sl don}
    in Spanish (e.g. {\sl don\_Juan} should be a NE, even if {\sl don}
    is lowercased), or the word {\sl junior} or {\sl jr.} in English
    (e.g. {\sl Peter\_Grasswick\_jr.} should be a NE, even if {\sl
      jr.} is lowercased).
 
  The section should containt a word per line, followed by the keyword
  {\tt PRE} or {\tt SUF} stating whether the word may be attached
  before or after an NE. It a word should be either a prefix or a
  suffix, it must be declared in two different lines, one with each
  keyword.
\begin{verbatim}
<Affixes>
don  PRE
doña PRE
jr.  SUF
<Affixes>
\end{verbatim}

  \item Sections \verb#<RE_NounAdj># \verb#<RE_Closed># and
    \verb#<RE_DateNumPunct># allow to modify the default regular
    expressions for Part-of-Speech tags. This regular expressions are
    used by the NER to determine whether a sentence-beginning word has
    some tag that is Noun or Adj, or any tag that is a closed
    category, or one of date/punctuation/number. The default is to
    check against Eagles tags, thus, the recognizer will fail to
    identifiy these categories if your dictionary uses another tagset,
    unless you specify the right patterns to look for.

  For instance, if our dictionary uses Penn-Treebank-like tags, we
  should define:
\begin{verbatim}
<RE_NounAdj>
^(NN$|NNS|JJ)
</RE_NounAdj>
<RE_Closed>
^(D|IN|C)
</RE_Closed>
\end{verbatim}
  
  \item Section \verb#<TitleLimit># contains only one line with an integer
  value stating the length beyond which a sentence written {\sl
  entirely} in uppercase will be considered a title and not a proper
  noun. Example:
\begin{verbatim}
<TitleLimit>
3
</TitleLimit>
\end{verbatim}

  If \verb#TitleLimit=0# (the default) title detection is
  deactivated (i.e, all-uppercase sentences are always marked as
  named entities).

  The idea of this heuristic is that newspaper titles are usually
  written in uppercase, and tend to have at least two or three
  words, while named entities written in this way tend to be acronyms
  (e.g. IBM, DARPA, ...) and usually have at most one or two words.

  For instance, if \verb#TitleLimit=3# the sentence 
  {\tt FREELING ENTERS NASDAC UNDER CLOSE OB\-SER\-VA\-TION OF MARKET ANALYSTS}
  will not be recognized as a named entity, and will have its words analyzed
  independently. On the other hand, the sentence {\tt IBM INC.}, having less than
  3 words, will be considered a proper noun.

  Obviously this heuristic is not 100\% accurate, but in some cases
  (e.g. if you are analyzing newspapers) it may be preferrable to the
  default behaviour (which is not 100\% accurate, either).
  
    \item Section \verb#<SplitMultiwords># contains only one line with
      either \verb#yes# or \verb#no#. If \verb#SplitMultiwords# is
      activated Named Entities still will be recognized but they will
      not be treated as a unit with only one Part-of-Speech tag for
      the whole compound. Each word gets its own Part-of-Speech tag
      instead.\\ Capitalized words get the Part-of-Speech tag as
      specified in \verb#NE_Tag#, The Part-of-Speech tags of
      non-capitalized words inside a Named Entity (typically,
      prepositions and articles) will be left untouched.
\begin{verbatim}
<SplitMultiwords>
no
</SplitMultiwords>
\end{verbatim}
\end{itemize}

%%%%%

\subsection{{\em BIO} NER module ({\tt bioner})}

  The machine-learning based NER module uses a classification algorithm 
 to decide whether each word is at a NE begin ({\tt B}), inside ({\tt I})
 or outside ({\tt O}). Then, a simple viterbi algorithm is applied to
 guarantee sequence coherence.

 It can be instantiated via the {\tt ner} wrapper described above, or
 directly via its own API:
\begin{verbatim}
class bioner: public ner_module {
  public:
    /// Constructor, receives the name of the configuration file.
    bioner ( const std::string & );

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

  The configuration file sets the required model and lexicon files, which
  may be generated from a training corpus using the scripts provided 
  with FreeLing (in folder {\tt src/utilities/nerc}).
  Check the README and comments in the scripts to find out what to do.

  The most important file in the set is the {\tt .rgf} file, which contains
  a definition of the context features that must be extracted for each
  named entity.  
  The feature rule language is described in section \ref{sec-rgf}.

  The sections of the configuration file for {\sl bioner} module are:
\begin{itemize}
  \item Section \verb#<RGF># contains one line with the path to the
    RGF file of the model. This file is the definition of the features
    that will be taken into account for NER.  These features are
    processed by {\tt libfries}.
\begin{verbatim}
<RGF>
ner.rgf
</RGF>
\end{verbatim}

  \item Section \verb#<Classifier># contains one line with the kind of
    classifier to use. Valid values are \verb#AdaBoost# and
    \verb#SVM#.
\begin{verbatim}
<Classifier>
Adaboost
</Classifier>
\end{verbatim}

  \item Section \verb#<ModelFile># contains one line with the path to
    the model file to be used. The model file must match the
    classifier type given in section \verb#<Classifier>#.
\begin{verbatim}
<ModelFile>
ner.abm
</ModelFile>
\end{verbatim}
  The {\tt .abm} files contain AdaBoost models based on shallow
  Decision Trees (see \cite{carreras03} for details). You don't need
  to understand this, unless you want to enter into the code of the
  AdaBoost classifier.

  The {\tt .svm} files contain Support Vector Machine models generated
  by {\tt libsvm} \cite{chang11}. You don't need to understand this, unless you want 
  to enter into the code of {\tt libsvm}.

\item Section \verb#<Lexicon># contains one line with the path to the
  lexicon file of the learnt model. The lexicon is used to translate
  string-encoded features generated by {\tt libfries} to
  integer-encoded features needed by {\tt libomlet}. The lexicon file
  is generated by {\tt libfries} at training time.
\begin{verbatim}
<Lexicon>
ner.lex
</Lexicon>
\end{verbatim}
  The {\tt .lex} file is a dictionary that assigns a number to each
  symbolic feature used in the AdaBoost or SVM model. You don't need to
  understand this either unless you are a Machine Learning student or
  the like.

\item Section \verb#<UseSoftMax># contains only one line with {\em
  yes} or {\em no}, indicating whether the classifier output must be
  converted to probabilities with the SoftMax function. Currently,
  AdaBoost models need that conversion, and SVM models do not.
\begin{verbatim}
<UseSoftMax>
yes
</UseSoftMax>
\end{verbatim}

\item Section \verb#<Classes># contains only one line with the classes
  of the model and its translation to B, I, O tag.
\begin{verbatim}
<Classes>
0 B 1 I 2 O
</Classes>
\end{verbatim}

  \item Section \verb#<NE_Tag># contains only one line with the PoS tag that
  will be assigned to the recognized entities. If the NE classifier is
  going to be used later, it will have to be informed of this tag at
  creation time.
\begin{verbatim}
<NE_Tag>
NP00000
</NE_Tag>
\end{verbatim}

\item Section \verb#<InitialProb># Contains the probabilities of
  seeing each class at the begining of a sentence. These probabilities
  are necessary for the Viterbi algorithm used to annotate NEs in a
  sentence.
\begin{verbatim}
<InitialProb>
B 0.200072
I 0.0
O 0.799928
</InitialProb>
\end{verbatim}

\item Section \verb#<TransitionProb># Contains the transition
  probabilities for each class to each other class, used by the
  Viterbi algorithm.
\begin{verbatim}
<TransitionProb>
B B 0.00829346
B I 0.395481
B O 0.596225
I B 0.0053865
I I 0.479818
I O 0.514795
O B 0.0758838
O I 0.0
O O 0.924116
</TransitionProb>
\end{verbatim}


\item Section \verb#<TitleLimit># contains only one line with an integer
  value stating the length beyond which a sentence written {\sl
  entirely} in uppercase will be considered a title and not a proper
  noun. Example:
\begin{verbatim}
<TitleLimit>
3
</TitleLimit>
\end{verbatim}

  If \verb#TitleLimit=0# (the default) title detection is
  deactivated (i.e, all-uppercase sentences are always marked as
  named entities).

  The idea of this heuristic is that newspaper titles are usually
  written in uppercase, and tend to have at least two or three
  words, while named entities written in this way tend to be acronyms
  (e.g. IBM, DARPA, ...) and usually have at most one or two words.

  For instance, if \verb#TitleLimit=3# the sentence 
  {\tt FREELING ENTERS NASDAC UNDER CLOSE OB\-SER\-VA\-TION OF MARKET ANALYSTS}
  will not be recognized as a named entity, and will have its words analyzed
  independently. On the other hand, the sentence {\tt IBM INC.}, having less than
  3 words, will be considered a proper noun.

  Obviously this heuristic is not 100\% accurate, but in some cases
  (e.g. if you are analyzing newspapers) it may be preferrable to the
  default behaviour (which is not 100\% accurate, either).
  
  \item Section \verb#<SplitMultiwords># contains only one line with
    either \verb#yes# or \verb#no#. If \verb#SplitMultiwords# is
    activated Named Entities still will be recognized but they will
    not be treated as a unit with only one Part-of-Speech tag for the
    whole compound. Each word gets its own Part-of-Speech tag
    instead.\\ Capitalized words get the Part-of-Speech tag as
    specified in \verb#NE_Tag#, The Part-of-Speech tags of
    non-capitalized words inside a Named Entity (typically,
    prepositions and articles) will be left untouched.
\begin{verbatim}
<SplitMultiwords>
no
</SplitMultiwords>
\end{verbatim}
\end{itemize}


%..................................................
\section{Quantity Recognition Module}
\label{file-quant}

  The {\tt quantities} class is a FSA that recognizes ratios,
  percentages, and physical or currency magnitudes (e.g. {\em twenty
    per cent}, {\em 20\%}, {\em one out of five}, {\em 1/5}, {\em one
    hundred miles per hour}, etc.

  This module depends on the numbers detection module (section
  \ref{file-numb}). If numbers are not previously detected and
  annotated in the sentence, quantities will not be recognized.

   This module, similarly to number recognition, is language
   dependent: That is, a FSA has to be programmed to match the
   patterns of ratio expressions in that language.

   Currency and physical magnitudes can be recognized in any language,
   given the appropriate data file.
\begin{verbatim}
class quantities {
  public:
    /// Constructor: receives the language code, and the data file.
    quantities(const std::string &, const std::string &); 

    /// Detect magnitude expressions starting at given sentence position
    bool matching(sentence &, sentence::iterator &) const;

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}


\subsection {Quantity Recognition Data File}
This file contains the data necessary to perform currency amount and
physical magnitude recognition.  
It consists of three sections:  \verb#<Currency>#, \verb#<Measure>#,
and \verb#</MeasureNames>#.

Section \verb#<Currency># contains a single line indicating which is
the code, among those used in section \verb#<Measure>#, that stands for
 'currency amount'.  This is used to assign to currency ammounts a different
 PoS tag than physical magnitudes.
E.g.:
\begin{verbatim}
<Currency>
CUR
</Currency>
\end{verbatim}

Section \verb#<Measure># indicates the type of measure corresponding
to each possible unit. Each line contains two fields: the measure code
and the unit code. The codes may be anything, at user's choice, and
will be used to build the lemma of the recognized quantity multiword.

E.g., the following section states that {\tt USD} and {\tt FRF} are of
type {\tt CUR} (currency), {\tt mm} is of type {\tt LN} (length), and 
{\tt ft/s} is of type {\tt SP} (speed):
\begin{verbatim}
<Measure>
CUR USD
CUR FRF
LN mm
SP ft/s
</Measure>
\end{verbatim}

Finally, section \verb#<MeasureNames># describes which multiwords have
to be interpreted as a measure, and which unit they represent. The
unit must appear in section \verb#<Measure># with its associated code.
Each line has the format:
\begin{verbatim}
multiword_description code tag
\end{verbatim}
where {\tt multiword\_description} is a multiword pattern as in
multiwords file described in section~\ref{file-mw}, {\tt code} is the
type of magnitude the unit describes (currency, speed, etc.), and {\tt
tag} is a constraint on the lemmatized components of the multiword,
following the same conventions than in multiwords file
(section~\ref{file-mw}).

E.g., 
\begin{verbatim}
<MeasureNames>
french_<franc> FRF $2:N
<franc> FRF $1:N
<dollar> USD $1:N
american_<dollar> USD $2:N
us_<dollar> USD $2:N
<milimeter> mm $1:N
<foot>_per_second ft/s $1:N
<foot>_Fh_second ft/s $1:N
<foot>_Fh_s ft/s $1:N
<foot>_second ft/s $1:N
</MeasureNames>
\end{verbatim}

This section will recognize strings such as the following:
\begin{verbatim}
 234_french_francs CUR_FRF:234 Zm
 one_dollar CUR_USD:1 Zm
 two_hundred_fifty_feet_per_second SP_ft/s:250 Zu
\end{verbatim}

 Quantity multiwords will be recognized only when following a number,
 that is, in the sentence {\em There were many french francs}, the
 multiword won't be recognized since it is not assigning units to a
 determined quantity.

 It is important to note that the lemmatized multiword expressions
 (the ones that containt angle brackets) will only be recognized if
 the lemma is present in the dictionary with its corresponding
 inflected forms.


%..................................................
\section{Probability Assignment and Unkown Word Guesser Module}
\label{file-prob}

 This class ends the morphological analysis subchain, and has two
 functions: first, it assigns an {\em a priori} probability to each
 analysis of each word. These probablities will be needed for the PoS
 tagger later. Second, if a word has no analysis (none of the previously
 applied modules succeeded to analyze it), this module tries to guess
 which are its possible PoS tags, based on the word ending.
\begin{verbatim}
class probabilities {
  public:
    /// Constructor: receives the name of the file
    // containing probabilities, and a threshold.
    probabilities(const std::string &, double);

    /// Assign probabilities for each analysis of given word
    void annotate_word(word &) const;
    /// Turn guesser on/of
    void set_activate_guesser(bool);

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

  The method \verb#set_activate_guesser# will turn on/off the guessing
 of likely PoS tags for words with no analysis. Note that the guesser
 is turned on/off for any thread using the same probabilities instance.

  The constructor receives:
\begin{itemize}
\item The probabilities file name: The file that contains all needed
  configuration and statistical information. This file can be
  generated from a tagged training corpus using the scripts in {\tt
    src/utilities}. Its format is described below.
\item A threshold: This is used for unknown words, when the
  probability of each possible tag has been estimated by the guesser
  according to word endings, tags with a value lower than this
  threshold are discarded.
\end{itemize}

\subsection{Lexical Probabilities File}

This file can be generated from a tagged corpus using the script
 {\tt src/utilities/train-tagger/bin/TRAIN.sh} provided in FreeLing package.
 See  {\tt src/utilities/train-tagger/README} find out how to use it.

The probabilities file has nine sections: \verb#<TagsetFile>#, \verb#<UnknownTags>#,
\verb#<Theeta>#, \verb#<Suffixes>#, \verb#<SingleTagFreq>#,
\verb#<ClassTagFreq>#, \verb#<FormTagFreq>#, \verb#<BiassSuffixes>#, 
 \verb#<LidstoneLambda>#.
 Each section is closed by
its corresponding tag  \verb#</TagsetFile>#, \verb#</UnknownTags>#, \verb#</Theeta>#,
\verb#</Suffixes>#, \verb#</SingleTagFreq>#, \verb#</ClassTagFreq>#,
\verb#</FormTagFreq>#, \verb#</BiassSuffixes>#, \verb#</LidstoneLambda>#.

 \begin{itemize} 

  \item Section \verb#<TagsetFile>#.  This section contains a single
    line with the path to a tagset description file (see section
    \ref{file-tagset}) to be used when computing short versions for PoS
    tags.  If the path is relative, the location of the lexical
    probabilities file is used as the base directory.

  \item Section \verb#<FormTagFreq>#.  Probability data of some high frequency forms.

   If the word is found in this list, lexical probabilities are
   computed using data in \verb#<FormTagFreq># section.
 
   The list consists of one form per line, each line with format: \\
   {\tt form ambiguity-class, tag1 \#observ1 tag2 \#observ2 ...}
   
   E.g. {\tt japonesas AQ-NC AQ 1 NC 0}

   Form probabilities are smoothed to avoid zero-probabilities. 

   \item Section \verb#<ClassTagFreq>#. Probability data of ambiguity classes.

   If the word is not found in the \verb#<FormTagFreq>#, frequencies
   for its ambiguity class are used.

   The list consists of class per line, each line with format:\\
   {\tt class tag1 \#observ1 tag2 \#observ2 ...}

   E.g. {\tt AQ-NC AQ 2361 NC 2077}

   Class probabilities are smoothed to avoid zero-probabilities.  

   \item Section \verb#<SingleTagFreq>#. Unigram probabilities. 

   If the ambiguity class is not found in the \verb#<ClassTagFreq>#, individual
   frequencies for its possible tags are used.

   One tag per line, each line with format: {\tt tag \#observ}

    E.g. {\tt AQ 7462}

   Tag probabilities are smoothed to avoid zero-probabilities.


  \item Section \verb#<Theeta>#.  Value for parameter {\it theeta}
    used in smoothing of tag probabilities based on word suffixes.

   If the word is not found in dictionary (and so the list of its
   possible tags is unknown), the distribution is computed using the
   data in the \verb#<Theeta>#, \verb#<Suffixes>#, and
   \verb#<UnknownTags># sections.

   The  section has exactly one line, with one real number.

   E.g. \\
   {\tt \verb#<Theeta>#\\
   0.00834\\
   \verb#</Theeta>#}

   \item Section \verb#<BiassSuffixes>#. Weighted interpolation factor between 
    class probability and word suffixes.
   
   The  section has exactly one line, with one real number.

   E.g. \\
   {\tt \verb#<BiassSuffixes>#\\
   0.4\\
   \verb#</BiassSuffixes>#}

   Default value is 0.3.

   The probability of the tags belonging to words unobserved in the
   training corpus, is computed backing off to the distribution of all
   words with the same ambiguity class.  This obviously
   overgeneralizes and for some words, the estimated probabilities may
   be rather far from reality.

   To palliate this overgeneralization, the ambiguity class probabilities
   can me interpolated with the probabilities assigned by the guesser 
   according to the word suffix.

   This parameter specifies the weight that suffix information is given in the iterpolation,
   i.e. if \verb#BiassSuffixes=0# only the ambiguity class information is used.
   If  \verb#BiassSuffixes=1#, only the probabilities provided by the guesser are used.

  \item Section \verb#<Suffixes>#.  List of suffixes obtained from a
    train corpus, with information about which tags were assigned to
    the word with that suffix. 

   The list has one suffix per line, each line with format: {\tt
     suffix \#observ tag1 \#observ1 tag2 \#observ2 ...}

   E.g. \\
   {\tt orada 133 AQ0FSP 17 VMP00SF 8 NCFS000 108}

  \item Section \verb#<UnknownTags>#. List of open-category tags to
  consider as possible candidates for any unknown word. 

   One tag per line, each line with format: {\tt tag \#observ}.  The
   tag is the complete label. The count is the number of occurrences
   in a training corpus.

   E.g. {\tt NCMS000 33438}


   \item Section \verb#<LidstoneLambda># specifies the $\lambda$ parameter for
   Lidstone's Law smoothing.

   The  section has exactly one line, with one real number.

   E.g. \\
   {\tt \verb#<LidstoneLambda>#\\
   0.2\\
   \verb#</LidstoneLambda>#}

   Default value is 0.1.

   This parameter is used only to smooth the lexical probabilities of 
   words that have appeared in the training corpus, and thus are listed
   in the \verb#<FormTagFreq># section described above. 

\end{itemize}

%..................................................
\section{Alternatives Suggestion Module}
\label{file-alter}

  This module is able to retrieve from its dictionary the entries most
 similar to the input form.  
  The similarity is computed according to a configurable string edit 
 distance (SED) measure.

   The alternatives module can be created to perform a direct search 
 of the form in a dictionary, or either to perform a search of the phonetic
 transcription of the form in a dictionary of phonetic transcriptions.
   In the later case, the orthographic forms corresponding to the phonetically
 similar words are returned.
  For instance, if a mispelled word such as {\em spid} is found,
  this module will find out that it sounds very close to a correct
  word in the dictionary ({\em speed}), and return the correctly 
  spelled alternatives.
  This module is based on the fast search algorithms on FSMs included 
  in the finite-state libray FOMA (\texttt{http://code.google.com/p/foma}).

  The API for this module is the following:

\begin{verbatim}
class alternatives {
  public:
    /// Constructor
    alternatives(const std::wstring &);
    /// Destructor
    ~alternatives();

    /// direct access to results of underlying FSM
    void get_similar_words(const std::wstring &, 
                           std::list<std::pair<std::wstring,int> > &) const;
    
    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
\end{verbatim}

  This module will find alternatives for words in the sentences, and enrich
 them with a list of forms, each with the corresponding SED value.
  The forms are added to the \texttt{alternatives} member of class \texttt{word},
 which is a \texttt{std::list<pair<std::wstring,int>>}.  The list can
 be traversed using the iterators \texttt{word::alternatives\_begin()} and
 \texttt{word::alternatives\_end()}.

  The constructor of this module expects a configuration file
 containing the following sections:

 \begin{itemize}
   \item Section \verb#<General># contains values for general parameters, expressed
    in lines of the form \verb#key value#.

    More specifically, it must contain a line\\
    \verb#Type (orthographic|phonetic)#\\
    stating whether the similar words must be searched using direct SED between
    of orthographic forms, or between their phonetic encoding.

    This section must also contain one line\\
    \verb#Dictionary filename#\\
    or either a line\\
    \verb#PhoneticDictionary filename#\\
    stating the dictionary where the 
    similar words are going to be searched.
    If \verb#PhoneticDictionary# is stated, then an additional line\\
    \verb#PhoneticRule filename#\\
    is expected, detailing the configuration file 
    for a phonetic encoding module (see section~\ref{file-phon}) that
    will be used to encode the input forms before the search.

    The \verb#Dictionary# can be any file containing one form per line. 
    Only first field in the line will be considered, which makes it possible
    to use a basic FreeLing dictionary (see section~\ref{file-dict}), since
    the morphological information will be ignored.

    The \verb#PhoneticDictionary# must contain one phonetic form per line, followed
    by a list of orthographic forms mapping to that sound. E.g., valid lines are:
    \begin{verbatim}
    f@Ur fore four
    tu too two
    \end{verbatim}

  \item Section \verb#<Distance># contains \verb#key value# lines stating
    parameters related to the SED measure to use.

    A line \verb#CostMatrix filename# is expected, stating the file where
    the SED cost matrix to be used. The \verb#CostMatrix# file must comply
    with FOMA requirements for cost matrices (see FOMA documentation, or
    examples provided in \verb#data/common/alternatives# in FreeLing tarball).

    A line \verb#Threshold int-value# can be provided stating the maximum distance
    of desired alternatives.  Note that a very high value will cause the module 
    to produce a long list of similar words, and a too low value may result in no
    similar forms found.
 
    A line \verb#MaxSizeDiff int-value# may also be provided. Similar strings with 
    a length difference greater than this parameter will be filtered out of the 
    result. To deactivate this feature, just set the value to a large number (e.g. 99).

  \item Section \verb#<Target># contains \verb#key value# lines describing 
    which words in the sentence must be checked for similar forms.

    The line \verb#UnknownWords (yes|no)# states whether similar forms are 
    to be searched for unknown words (i.e. words
    that didn't receive any analysis from any module so far).

    The line \verb#KnownWords regular-expression# states which words with analysis
    have to be checked.  The regular expression is matched against the PoS tag of the
    words.  If the regular-expression is \verb#none#, no known word is checked for
    similar forms.
\end{itemize}

%..................................................
\section{Sense Labelling Module}
\label{mod-sense}

  This module searches the lemma of each analysis in a sense dictionary,
  and enriches the analysis with the list of senses found there.

  Note that this is not disambiguation, all senses for the lemma are returned. 

  The module receives a file containing several configuration options, which
  specify the sense dictionary to be used, and some mapping rules that can be 
  used to adapt FreeLing PoS tags to those used in the sense dictionary.

  FreeLing provides WordNet-based \cite{fellbaum98,vossen98a}
  dictionaries, but the results of this module can be changed to any
  other sense catalogue simply providing a different sense dictionary
  file.

\begin{verbatim}
class senses {
  public:
    /// Constructor: receives the name of the configuration file
    senses(const std::string &); 
 
    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

   The constructor of this class receives the name of a configuration file
 which is expected to contain the following sections:
\begin{itemize}
\item A section \verb#<WNposMap># with the mapping rules of FreeLing PoS tags to
   sense dictionary PoS tags. See details in section \ref{semdb}.

\item A section \verb#<DataFiles># containing at least the keyword \verb#SenseDictFile#
   defined to a valid sense dictionary file name. E.g.:
  \begin{verbatim}
    <DataFiles>
    SenseDictFile  ./senses30.src
    </DataFiles>
  \end{verbatim}
    The sense dictionary must follow the format described in section \ref{file-sense}.

    If the mapping rules \verb#<WNposMap># require a form dictionary, a keyword
   \verb#formDictFile# with the dictionary to use must be provided in this section.
    More details are given in section \ref{semdb}.

\item A section \verb#<DuplicateAnalysis># containing a single line
  with either \verb#yes# or \verb#no#, stating whether the analysis
  with more than one senses must be duplicated. If this section is
  ommitted, \verb#no# is used as default value.
    The effect of activating this option is described in the following example:
  
  For instance, the word {\em crane} has the follwing analysis:
 \begin{verbatim}
    crane 
       crane NN  0.833
       crane VB  0.083
       crane VBP 0.083
 \end{verbatim}

  If the list of senses is simply added to each of them (that is,
  {\tt DuplicateAnalysis} is set to {\tt false}), you will get:
 \begin{verbatim}
    crane 
       crane NN  0.833  02516101:01524724
       crane VB  0.083  00019686
       crane VBP 0.083  00019686
 \end{verbatim}

   But if you set {\tt DuplicateAnalysis} to {\t true}, the {\tt NN}
   analysis will be duplicated for each of its possible senses:
 \begin{verbatim}
    crane 
       crane NN  0.416  02516101
       crane NN  0.416  01524724
       crane VB  0.083  00019686
       crane VBP 0.083  00019686
 \end{verbatim}
\end{itemize}

%..................................................
\section{Word Sense Disambiguation Module}
\label{mod-ukb}

  This module performs word-sense-disambiguation on content words in
  given sentences. 
  This module is to be used if word sense disambiguation (WSD) is
  desired.  If no disambiguation (or basic most-frequent-sense
  disambiguation) is needed, the senses module described in section
  \ref{mod-sense} is a lighter and faster option.

  The module is an implementation of UKB algorithm \cite{agirre09}.
  UKB relies on a semantic relation network (in this case,
  WN and XWN) to disambiguate the most likely senses for words in a
  text using PageRank algorithm. See \cite{agirre09} for details on the
  algorithm.

  The module expects the input words to have been annotated with a list 
  of candidate senses by the senses module (section \ref{mod-sense})
  It ranks the candidate senses and sorts the list, according to the 
  PageRank for each sense. The rank value is also provided in the result.

  The API of the class is the following:
\begin{verbatim}
class ukb {
  public:
    /// Constructor. Receives a relation file for UKB, a sense dictionary,
    /// and two UKB parameters: epsilon and max iteration number.
    ukb(const std::string &);
 
    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &const);
};
\end{verbatim}

  The constructor receives a file name where module configuration
  options are found.  The contents of the configuration files are the
  following:

\begin{itemize}

  \item A section \verb#<PageRankParameters># specifying values for UKB 
      stopping criteria. E.g.:
\begin{verbatim}
    <PageRankParameters>
    Epsilon 0.03
    MaxIterations 10 
    Damping 0.85
    </PageRankParameters>
\end{verbatim}
    These parameters are UKB parameters: The an {\sl
    epsilon} float value that controls the precision with with the end
    of PageRank iterations is decided, and a {\sl MaxIterations}
    integer, that controls the maximum number of PageRank iterations,
    even is no convergence is reached. The {\sl Damping} parameter is 
    the standard parameter in PageRank algorithm.

 \item A section \verb#<RelationFile># specifying
  the knowledge base required by the algorithm.
  This section must one lines, with the path to a 
  file containing a list of relations between senses.
  \begin{verbatim}
    <RelationFile>
    ../common/xwn.dat
    </RelationFile>
  \end{verbatim}
  The path may be absolute, or relative to the position of the 
  ukb module configuration file.

 The {\sl RelationFile} contains the semantic relationship graph to
 load. It is a text filecontaining pairs of related senses (WN synsets
 in this case). Relations are not labelled nor directed.
 
 An example of the content of this file is:
 \begin{verbatim}
   00003431-v 14877585-n
   00003483-r 00104099-r
   00003483-r 00890351-a
 \end{verbatim}

\end{itemize}

%..................................................
\section{Part-of-Speech Tagger Module}
\label{sec-pos}

  There are two different modules able to perform PoS tagging. The
  application should decide which method is to be used, and
  instantiate the right class.

  The first PoS tagger is the {\tt hmm\_tagger} class, which is a
  classical trigam Markovian tagger, following \cite{brants00}.

  The second module, named {\tt relax\_tagger}, is a hybrid system
  capable to integrate statistical and hand-coded knowledge, following
  \cite{padro98a}.

  The {\tt hmm\_tagger} module is somewhat faster than {\tt
    relax\_tagger}, but the later allows you to add manual constraints
  to the model. Its API is the following:
\begin{verbatim}
class hmm_tagger: public POS_tagger {
  public:
    /// Constructor
    hmm_tagger(const std::string &, bool, unsigned int, unsigned int kb=1);

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;

    /// given an analyzed sentence find out probability 
    /// of the k-th best sequence
    double SequenceProb_log(const sentence &, int k=0) const;

};
\end{verbatim}

  The {\tt hmm\_tagger} constructor receives the following parameters:
\begin{itemize}
\item The HMM file, which containts the model parameters. \\
  The format
  of the file is described below. This file can be generated from a
  tagged corpus using the script  {\tt src/utilities/train-tagger/bin/TRAIN.sh}
   provided in FreeLing package. See  {\tt src/utilities/train-tagger/README}
  to find out the details.
\item A boolean stating whether words that carry retokenization
  information (e.g. set by the dictionary or affix handling modules)
  must be retokenized (that is, splitted in two or more words) after
  the tagging.
\item An integer stating whether and when the tagger must select only
  one analysis in case of ambiguity. Possbile values are: {\tt
    FORCE\_NONE (or 0)}: no selection forced, words ambiguous after
  the tagger, remain ambiguous.  {\tt FORCE\_TAGGER (or 1)}: force
  selection immediately after tagging, and before retokenization. {\tt
    FORCE\_RETOK (or 2)}: force selection after retokenization.
\item An integer stating how many best tag sequences the tagger must
  try to compute. If not specified, this parameter defaults to 1.
  Since a sentence may have less possible tag sequences than the given
  {\tt k} value, the results may contain a number of sequences smaller
  than {\tt k}.
\end{itemize}

  The {\tt relax\_tagger} module can be tuned with hand written
  constraint, but is about 2 times slower than {\tt hmm\_tagger}.
  It is not able to produce {\tt k} best sequences either.
\begin{verbatim}
class relax_tagger : public POS_tagger {
  public:
    /// Constructor, given the constraint file and config parameters
    relax_tagger(const std::string &, int, double, double, bool, unsigned int);

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

  The {\tt relax\_tagger} constructor receives the following parameters:
\begin{itemize}
\item The constraint file. The format of the file is described
  below. This file can be generated from a tagged corpus using the
  script {\tt src/utilities/train-tagger/bin/TRAIN.sh} provided in
  FreeLing package. See {\tt src/utilities/train-tagger/README} for details.
\item An integer stating the maximum number of iterations to wait for
   convergence before stopping the disambiguation algorithm.
\item A real number representing the scale factor of the constraint weights.
\item A real number representing the threshold under which any changes
  will be considered too small. Used to detect convergence.
\item A boolean stating whether words that carry retokenization
  information (e.g. set by the dictionary or affix handling modules)
  must be retokenized (that is, splitted in two or more words) after
  the tagging.
\item An integer stating whether and when the tagger must select only
  one analysis in case of ambiguity. Possbile values are: {\tt
    FORCE\_NONE (or 0)}: no selection forced, words ambiguous after
  the tagger, remain ambiguous.  {\tt FORCE\_TAGGER (or 1)}: force
  selection immediately after tagging, and before retokenization. {\tt
    FORCE\_RETOK (or 2)}: force selection after retokenization.
\end{itemize}

  The iteration number, scale factor, and threshold parameters are
  very specific of the relaxation labelling algorithm. Refer to
  \cite{padro98a} for details.


\subsection{HMM-Tagger Parameter File}
\label{file-hmm}

   This file contains the statistical data for the Hidden Markov
   Model, plus some additional data to smooth the missing values.
   Initial probabilities, transition probabilities, lexical
   probabilities, etc.

   The file may be generated by your own means, or using a tagged
   corpus and the script {\tt src/utilities/train-tagger/bin/TRAIN.sh}
   provided in FreeLing package.\\
    See {\tt src/utilities/train-tagger/README} for details.

  The file has eight sections: \verb#<TagsetFile>#, \verb#<Tag>#, \verb#<Bigram>#,
  \verb#<Trigram>#, \verb#<Initial>#, \verb#<Word>#,
  \verb#<Smoothing>#, and \verb#<Forbidden>#. Each section is 
  closed by it corresponding tag \verb#</Tag>#, \verb#</Bigram>#,
  \verb#</Trigram>#, etc.

  The tag (unigram), bigram, and trigram probabilities are used in
  Linear Interpolation smoothing by the tagger to compute state
  transition probabilities ($\alpha_{ij}$ parameters of the HMM).
 
  \begin{itemize} 
  \item Section \verb#<TagsetFile>#.  This section contains a single
    line with the path to a tagset description file (see section
    \ref{file-tagset}) to be used when computing short versions for PoS
    tags.  If the path is relative, the location of the lexical
    probabilities file is used as the base directory.

    This section has to appear {\em before} section \verb#<Forbidden>#.

    \item Section \verb#<Tag>#. List of unigram tag probabilities
      (estimated via your preferred method).  Each line is a tag
      probability {\tt P(t)} with format \\ {\tt Tag Probability}

   Lines for zero tag (for initial states) and for {\tt x} (unobserved
   tags) must be included.

  E.g.\\
    {\tt 0  0.03747}\\
    {\tt AQ 0.00227}\\
    {\tt NC 0.18894}\\
    {\tt x  1.07312e-06}

    \item Section \verb#<Bigram>#. List of bigram transition
      probabilities (estimated via your preferred method).  Each line
      is a transition probability, with the format:\\ {\tt Tag1.Tag2
        Probability}

  Tag zero indicates sentence-beggining.

    E.g. the following line indicates the transition probability
    between a sentence start and the tag of the first word being {\tt
      AQ}.\\ {\tt 0.AQ 0.01403}

    E.g. the following line indicates the transition probability
    between two consecutive tags.\\ {\tt AQ.NC 0.16963}

    \item Section \verb#<Trigram>#. List of trigram transition
      probabilities (estimated via your preferred method).  Each line
      is a transition probability, with the format:\\ {\tt
        Tag1.Tag2.Tag3 Probability}.

   Tag zero indicates sentence-beggining.

    E.g. the following line indicates the probability that a word 
    has {\tt NC} tag just after a {\tt 0.AQ} sequence.\\
    {\tt 0.AQ.NC 0.204081}

    E.g. the following line indicates the probability of a tag {\tt
    SP} appearing after two words tagged {\tt DA} and {\tt NC}.\\
    {\tt DA.NC.SP 0.33312}

    \item Section \verb#<Initial>#. List of initial state probabilities
  (estimated via your preferred method), i.e. the $\pi_{i}$ parameters of
  the HMM.    Each line is an initial probability, with the format\\
 {\tt InitialState LogProbability}.

   Each {\tt InitialState} is a PoS-bigram code with the
   form {\tt 0.tag}. Probabilities are given in logarithmic form to avoid
   underflows.

    E.g. the following line indicates the probability that the
    sequence starts with a determiner.\\
    {\tt 0.DA -1.744857}

    E.g. the following line indicates the probability that the
    sequence starts with an unknown tag.\\
    {\tt 0.x -10.462703}

    \item Section \verb#<Word>#. Contains a list of word probabilities
  {\tt P(w)} (estimated via your preferred method). It is used, toghether with
  the tag probabilities above, to compute emission probabilities
  ($b_{iw}$ parameters of the HMM).

   Each line is a word probability {\tt P(w)} with format {\tt word
     LogProbability}. A special line for \verb#<UNOBSERVED_WORD># must
   be included. Sample lines for this section are:

    \verb#  afortunado -13.69500# \\
    \verb#  sutil -13.57721# \\
    \verb#  <UNOBSERVED_WORD> -13.82853# 

    \item Section \verb#<Smoothing># contains three lines with the coefficients
    used for linear interpolation of unigram (\verb#c1#), bigram (\verb#c2#),
    and trigram (\verb#c3#) probabilities.
    The section looks like:

    \verb#  <Smoothing># \\
    \verb#  c1 0.120970620869314# \\
    \verb#  c2 0.364310868831106# \\
    \verb#  c3 0.51471851029958# \\
    \verb#  </Smoothing># 

    \item Section \verb#<Forbidden># is the only that is {\em not}
    generated by the training scripts, and is supposed to be manually
    added (if needed).
     The utility is to prevent smoothing of some combinations that are
    known to have zero probability.

     Lines in this section are trigrams, in the same format than above:\\
   \verb#Tag1.Tag2.Tag3#
   
    Trigrams listed in this section will be assigned
   zero probability, and no smoothing will be performed.
     This will cause the tagger to avoid any solution including
   these subsequences.

   The first tag may be a wildcard (\verb#*#), which will match any tag, or
   the tag \verb#0# which denotes sentence beginning. These two special tags
   can only be used in the first position of the trigram.

   In the case of an EAGLES tagset, the tags in the trigram may be either
   the short or the long version.
    The tags in the trigram (except the special tags \verb#*# and \verb#0#)
   can be restricted to a certain lemma, suffixing them with the lemma in
   angle brackets.

     For instance, the following rules will assign zero probability to
     any sequence containing the specified trigram:

    \verb# *.PT.NC#: a noun after an interrogative pronoun.\\ 
    \verb# 0.DT.VMI#: a verb in indicative following a determiner just after sentence beggining.\\  
    \verb# SP.PP.NC#: a noun following a preposition and a personal pronoun.

    Similarly, the set of rules: 

    \verb#  *.VAI<haber>.NC# \\
    \verb#  *.VAI<haber>.AQ# \\
    \verb#  *.VAI<haber>.VMP00SF# \\
    \verb#  *.VAI<haber>.VMP00PF# \\
    \verb#  *.VAI<haber>.VMP00PM# 

   \noindent will assign zero probability to any sequence containing
   the verb ``haber'' tagged as an auxiliar (VAI) followed by any of
   the listed tags. Note that the masculine singular participle is not
   excluded, since it is the only allowed after an auxiliary
   ``haber''.

\end{itemize}

%..................................................
\subsection{Relaxation-Labelling Constraint Grammar File}
\label{file-relax}

   The syntax of the file is based on that of Constraint Grammars
   \cite{karlsson95}, but simplified in many aspects, and modified to 
   include weighted constraints.

   An initial file based on statistical constraints may be generated
   from a tagged corpus using the  {\tt src/utilities/train-tagger/bin/TRAIN.sh}
   script provided with FreeLing.
   Later, hand written constraints can be added to the file to improve
   the tagger behaviour.

   The file consists of two sections: {\tt SETS} and {\tt CONSTRAINTS}.

\subsubsection{Set definition}
   The {\tt SETS} section consists of a list of set definitions, each of the form
   {\tt Set-name = element1 element2 ... elementN ; }
    
    Where the {\tt Set-name} is any alphanumeric string starting with
    a capital letter, and the elements are either forms, lemmas, plain
    PoS tags, or senses. Forms are enclosed in parenthesis
    --e.g. \verb#(comimos)#--, lemmas in angle brackets
    --e.g. \verb#<comer>#--, PoS tags are alphanumeric strings
    starting with a capital letter --e.g. \verb#NCMS000#--, and senses
    are enclosed in square brackets --e.g. \verb#[00794578]#.  The
    sets must be homogeneous: That is, all the elements of a set have
    to be of the same kind.

   Examples of set definitions:
\begin{verbatim}
   DetMasc = DA0MS0 DA0MP0 DD0MS0 DD0MP0 DI0MS0 DI0MP0 DP1MSP DP1MPP
             DP2MSP DP2MPP DT0MS0 DT0MP0 DE0MS0 DE0MP0 AQ0MS0 AQ0MP0;
   VerbPron = <dar_cuenta> <atrever> <arrepentir> <equivocar> <inmutar>
              <morir> <ir> <manifestar> <precipitar> <referir> <venir>;
   Animal = [00008019] [00862484] [00862617] [00862750] [00862871] [00863425]
            [00863992] [00864099] [00864394] [00865075] [00865379] [00865569]
            [00865638] [00867302] [00867448] [00867773] [00867864] [00868028]
            [00868297] [00868486] [00868585] [00868729] [00911889] [00985200]
            [00990770] [01420347] [01586897] [01661105] [01661246] [01664986] 
            [01813568] [01883430] [01947400] [07400072] [07501137];
\end{verbatim}

\subsubsection{Constraint definition}
   The {\tt CONSTRAINTS} section consists of a series of context
   constraits, each of the form: {\tt weight core context;}

    Where:
    \begin{itemize}
      \item {\tt weight} is a real value stating the compatibility (or
	incompatibility if negative) degree of the {\tt label} with the
	{\tt context}.
      \item {\tt core} indicates the analysis or
	analyses  (form interpretation) in a word that will 
        be affected by the constraint. It may be:
	\begin{itemize}
	\item Plain tag:  A plain complete PoS tag,  e.g. {\tt VMIP3S0}
	\item Wildcarded tag:  A PoS tag prefix, right-wilcarded, 
              e.g. {\tt VMI*}, {\tt VMIP*}. 
	\item Lemma: A lemma enclosed in angle brackets, optionaly
	  preceded by a tag or a wildcarded tag.
	  e.g.  \verb#<comer>#, \verb#VMIP3S0<comer>#,
	  \verb#VMI*<comer># will match any
	  word analysis with those tag/prefix and lemma.
	\item Form: Form enclosed in parenthesis, preceded by a PoS tag (or a
	  wilcarded tag).
	  e.g.  {\tt VMIP3S0(comi\'o)},  {\tt VMI*(comi\'o)} will match any
	  word analysis with those tag/prefix and form.
          Note that the form alone {\em is not} allowed in the rule core,
	  since the rule woull to distinguish among different analysis of
	  the same form.
	\item Sense: A sense code enclosed in square brackets, optionaly
	  preceded by a tag or a wildcarded tag.
	  e.g.  \verb#[00862617]#, \verb#NCMS000[00862617]#,
          \verb#NC*[00862617]# will match any
	  word analysis with those tag/prefix and sense.
	\end{itemize}
      \item {\tt context} is a list of conditions that the context of
        the word must satisfy for the constraint to be applied.
	Each condition is enclosed in parenthesis and the list (and
        thus the constraint) is finished with a semicolon.
	Each condition has the form: \\
	{\tt (position terms)} \\
	or either:\\
	{\tt (position terms barrier terms)} 
 
        Conditions may be negated using the token {\tt not},
         i.e. {\tt (not pos terms)}

	Where: 
	\begin{itemize}
	\item {\tt position} is the relative position where the condition
	  must be satisfied: -1 indicates the previous word and +1 the
	  next word. A position with a star (e.g. -2*) indicates that
	  any word is allowed to match starting from the indicated
	  position and advancing towards the beggining/end of the sentence.
 	\item {\tt terms} is a list of one or more terms separated by
 	the token {\tt or}. Each term may be:
    	   \begin{itemize}
  	     \item Plain tag:  A plain complete PoS tag,  e.g. {\tt VMIP3S0}
	     \item Wildcarded tag:  A PoS tag prefix, right-wilcarded, 
               e.g. {\tt VMI*}, {\tt VMIP*}. 
	     \item Lemma: A lemma enclosed in angle brackets, optionaly
	       preceded by a tag or a wildcarded tag.
	       e.g.  \verb#<comer>#, \verb#VMIP3S0<comer>#,
	       \verb#VMI*<comer># will match any
	       word analysis with those tag/prefix and lemma.
	     \item Form: Form enclosed in parenthesis, optionally
	       preceded by a PoS tag (or a wilcarded tag).
	       e.g.  {\tt (comi\'o)}, {\tt VMIP3S0(comi\'o)}, 
               {\tt VMI*(comi\'o)} will match any
	       word analysis with those tag/prefix and form.
               Note that --contrarily to when defining the rule core-- 
               the form alone {\em is} allowed in the context.
    	     \item Sense: A sense code enclosed in square brackets, optionaly
	       preceded by a tag or a wildcarded tag.
	       e.g.  \verb#[00862617]#, \verb#NCMS000[00862617]#, 
               \verb#NC*[00862617]# will match any
	       word analysis with those tag/prefix and sense.
    	     \item Set reference: A name of a previously defined {\em SET}
	       in curly brackets.
	       e.g.  \verb#{DetMasc}#, \verb#{VerbPron}#  will match any
	       word analysis with a tag, lemma or sense in the
	       specified set.
	   \end{itemize}

	\item {\tt barrier} states that the a match of the first term
	list is only acceptable if between the focus word and the
	matching word there is no match for the second term list.
	\end{itemize}
    \end{itemize}

    Note that the use of sense information in the rules of 
    the constraint grammar (either in the core or in the context) 
    only makes sense when this information distinguishes one analysis
    from another. If the sense tagging has been performed with the 
    option \verb#DuplicateAnalysis=no#, each PoS tag will have a list
    with all analysis, so the sense information will not distinguish
    one analysis from the other (there will be only one analysis with
    that sense, which will have at the same time all the other senses
    as well). 
    If the option \verb#DuplicateAnalysis# was active, the sense
    tagger duplicates the analysis, creating a new entry for each
    sense. So, when a rule selects an analysis having a certain sense,
    it is unselecting the other copies of the same analysis with 
    different senses.

\subsubsection{Examples}

	Examples:\\
	The next constraint states a high incompatibility for a word
	being a definite determiner ({\tt DA*}) if the next word is a personal form
	of a verb ({\tt VMI*}):\\
        {\tt -8.143  DA*  (1 VMI*); }
	
        The next constraint states a very high compatibility for the
        word {\sl mucho} (much) being an indefinite determiner ({\tt DI*}) 
        --and thus not being a pronoun or an adverb, or any
        other analysis it may have-- if the following word is a noun ({\tt NC*}):\\
        {\tt 60.0 DI* (mucho) (1 NC*);}

	The next constraint states a positive compatibility value for
	a word being a noun ({\tt NC*}) if somewhere to its left
	there is a determiner or an adjective ({\tt DA* or AQ*}), and
	between them there is not any other noun:\\
        {\tt 5.0 NC* (-1* DA* or AQ* barrier NC*);}

	The next constraint states a positive compatibility value for
	a word being a masculine noun ({\tt NCM*}) if the word to its
	left is a masculine determiner. It refers to a previously
	defined {\em SET} which should contain the list of all tags
	that are masculine determiners. This rule could be useful to
	correctly tag Spanish words which have two different NC
	analysis differing in gender: e.g. {\em el cura} (the priest)
	vs. {\em la cura} (the cure):\\
        {\tt 5.0 NCM* (-1* {DetMasc};)}

	The next constraint adds some positive compatibility to a
	3rd person personal pronoun being of undefined gender and
	number ({\tt PP3CNA00}) if it has the possibility of being
        masculine singular ({\tt PP3MSA00}), the next word may have
        lemma {\sl estar} (to be), and the second word to the right
	is not a gerund ({\tt VMG}). This rule is intended to solve the 
        different behaviour of the Spanish word {\sl lo} (it) in sentences 
        such as ``¿Cansado? Si, lo estoy.'' ({\sl Tired? Yes, I am [it]}) or 
        ``lo estoy viendo.'' ({\sl I am watching it}).\\
	{\tt 0.5 PP3CNA00 (0 PP3MSA00) (1 \verb#<estar>#) (not 2 VMG*);}


%..................................................
\section{Phonetic Encoding Module}
\label{file-phon}

 The phonetic encoding module enriches words with their SAMPA\footnote{\texttt{www.phon.ucl.ac.uk/home/sampa/}} phonetic codification.

 The module applies a set of rules to transform the written form to
 the output phonetic form, thus the rules can be changed to get the
 output in any other phonetic alphabet. The module can also use an
 exception dictionary to handle forms that do not follow the default
 rules, or for languages with highly irregular ortography.

 The API of the module is the following:

\begin{verbatim}
class phonetics {
  
 public:
  /// Constructor, given config file
  phonetics(const std::wstring&);
  
  /// Returns the phonetic sound of the word
  std::wstring get_sound(const std::wstring &) const;

  /// analyze given sentence, enriching words with phonetic encoding
  void analyze(sentence &) const;
  /// analyze given sentences
  void analyze(std::list<sentence> &) const;
  /// analyze sentence, return analyzed copy
  sentence analyze(const sentence &) const;
  /// analyze sentences, return analyzed copy
  std::list<sentence> analyze(const std::list<sentence> &) const;
};

\end{verbatim}

 The constructor receives a configuration file that contains the
 transformation rules to apply and the exception dictionary.

 The module can be used to transform a single string (method
 \verb#get_sound#) or to enrich all words in a sentence (or sentence
 list) with their phonetic information.

 The phonetic encoding is stored in the word object and can be retrieved
 with the method \verb#word::get_ph_form#.

 The format of the configuration file is the following:

 There can be at most one exception dictionary, delimited by
 \verb#<Exceptions># and \verb#</Exceptions>#.  Each entry in the
 dictionary contains two fields: a lowercase word form and the output
 phonetic encoding. E.g.:
\begin{verbatim}
addition @dISIn
varieties B@raIItiz
worcester wUst@r
\end{verbatim}

  If a word form is found in the exceptions dictionary, the corresponding
phonetic string is returned and no transformation rules are applied.

 There can be one or more rulesets delimited by \verb#<Rules># and
 \verb#</Rules>#.  Rulesets are applied in the order they are
 defined. Each ruleset is applied on the result of the previous.
 
 Rulesets can contain two kind of lines:  Category definitions and rules.

 Category definitions are of the form \verb#X=abcde# and define a set of
 characters under a single name. Category names must have exactly one character,
 which should not be part of the input or output alphabet to avoid ambiguities. 
 e.g.: 
\begin{verbatim}
U=aeiou
V=aeiouäëïöüâêîôûùò@
L=äëïöüäëïöüäëïöüùò@
S=âêîôûâêîôûâêîôûùò@
A=aâä
E=eêë
\end{verbatim}

 Categories are only active for rules in the same ruleset where the category is defined.

 Rules are of the form \verb#source/target/context#. Both
 \verb#source# and \verb#target# may be either a category name or a
 terminal string. 

 Simple string rules replace the first string with the second if and
 only if it occurs in the given context. Contexts must contain a "\_"
 symbol indicating where the source string is located. They may also
 contain characters, categories, and the symbols \^~(word beginning),
 or \$ (word end). The empty context "\_" is always satisfied.

 Rules can only change terminal strings to terminal strings, or
 categories to categories (i.e. both \verb#source# and \verb#target#
 have to be of the same type).  If a category is to be changed to
 another category, they should contain the same number of
 characters. Otherwise the second category will have its last letter
 repeated until it has the same length as the first (if it is
 shorter), or characters in the second category that don't match
 characters in the first will be ignored.

 Some example rules for English:
\begin{verbatim}
qu/kw/_
h//^r_
a/ò/_lmV$
U/L/C_CV
\end{verbatim}

 First rule \verb#qu/kw/_# replaces string \verb#qu# with \verb#kw# in
 any context. \\
 Second rule \verb#h//^r_# removes character \verb#h#
 when it is preceeded by \verb#r# at word beginning.\\ 
 Rule \verb#a/ò/_lmV$# replaces \verb#a# with \verb#ò# when followed by
 \verb#l#, \verb#m#, any character in category \verb#V# and the end of
 the word.\\ 
 Rule \verb#U/L/C_CV# replaces any character in category
 \verb#U# with the character in the same position in category
 \verb#L#, when preceeded by any character in category \verb#C# and
 followed by any character in category \verb#C# and any character in 
 category \verb#V#. 

 Note that uppercase characters for categories is just a convention.
 An uppercase letter may be a terminal symbol, and a lowercase may be
 a category name. Non-alphabetical characters are also allowed.
 If a character is not defined as a category name, it will be considered 
 a terminal character.




%..................................................
\section{Named Entity Classification Module}
\label{file-nec}

  The mission of the Named Entity Classification module is to assing a
  class to named entities in the text. It is a Machine-Learning based
  module, so the classes can be anything the model has been trained to
  recognize.

  When classified, the PoS tag of the word is changed to the label
  defined in the model.

  This module depends on a NER module being applied previously. If no
  entities are recognized, none can be classified.

  Models provided with FreeLing distinguish four classes: Person (tag
  {\tt NP00SP0}), Geographical location ({\tt NP00G00}), Organization
  ({\tt NP00O00}), and Others ({\tt NP00V00}).

  If you have an anotated corpus, the models can be trained using the
  scripts in {\tt src/utilities/nerc}.  See the README there and the comments
  inside the script for details.

  The most important file in the set is the {\tt .rgf} file, which contains
  a definition of the context features that must be extracted for each
  named entity.  
  The feature rule language is described in section \ref{sec-rgf}.

  The API of the class is the following:
\begin{verbatim}
class nec {
  public:
    /// Constructor
    nec(const std::string &); 

    /// analyze given sentence.
    void analyze(sentence &) const;
    /// analyze given sentences.
    void analyze(std::list<sentence> &) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &) const;
    /// return analyzed copy of given sentences
    std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

  The constructor receives one parameter with the name of the configuration file for the module. 
  Its content is described below.

\subsection{NEC Data File}

  The machine-learning based Named Entity Classification module reads a configuration file with
 the following sections

\begin{itemize}
  \item Section \verb#<RGF># contains one line with the path to the RGF file of the model. This file is the definition of the features that will be taken into account for NEC.
\begin{verbatim}
<RGF>
ner.rgf
</RGF>
\end{verbatim}

  \item Section \verb#<Classifier># contains one line with the kind of classifier to use. Valid values are
\verb#AdaBoost# and \verb#SVM#.
\begin{verbatim}
<Classifier>
Adaboost
</Classifier>
\end{verbatim}

  \item Section \verb#<ModelFile># contains one line with the path to the model file to be used. The model file must match the classifier type given in section \verb#<Classifier>#.
\begin{verbatim}
<ModelFile>
ner.abm
</ModelFile>
\end{verbatim}
  The {\tt .abm} files contain AdaBoost models based on shallow
  Decision Trees (see \cite{carreras03} for details). You don't need
  to understand this, unless you want to enter into the code of the
  AdaBoost classifier.

  The {\tt .svm} files contain Support Vector Machine models generated
  by {\tt libsvm} \cite{chang11}. You don't need to understand this, unless you want 
  to enter into the code of {\tt libsvm}.


\item Section \verb#<Lexicon># contains one line with the path to the lexicon file of the learnt model. The lexicon is used to translate string-encoded features generated by {\tt libfries} to integer-encoded features needed by {\tt libomlet}. The lexicon file is generated by {\tt libfries} at training time.
\begin{verbatim}
<Lexicon>
ner.lex
</Lexicon>
\end{verbatim}
  The {\tt .lex} file is a dictionary that assigns a number to each
  symbolic feature used in the AdaBoost or SVM model. You don't need to
  understand this either unless you are a Machine Learning student or
  the like.

\item Section \verb#<Classes># contains only one line with the classes of the model and its translation to B, I, O tag.
\begin{verbatim}
<Classes>
0 NP00SP0 1 NP00G00 2 NP00O00 3 NP00V00
</Classes>
\end{verbatim}

  \item Section \verb#<NE_Tag># contains only one line with the PoS tag 
   assigned by the NER module, which will be used to select named entities
   to be classified.
\begin{verbatim}
<NE_Tag>
NP00000
</NE_Tag>
\end{verbatim}

\end{itemize}


%..................................................
\section{Chart Parser Module}

  The chart parser enriches each {\tt sentence} object with a {\tt
    parse\_tree} object, whose leaves have a link to the sentence
  words.

  The API of the parser is:
\begin{verbatim}
class chart_parser {
 public:
   /// Constructor
   chart_parser(const std::string&);
   /// Get the start symbol of the grammar
   std::string get_start_symbol(void) const;

   /// analyze given sentence.
   void analyze(sentence &) const;
   /// analyze given sentences.
   void analyze(std::list<sentence> &) const;
   /// return analyzed copy of given sentence
   sentence analyze(const sentence &) const;
   /// return analyzed copy of given sentences
   std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

  The constructor receives a file with the CFG grammar to be used by
  the grammar, which is described in the next section

  The method {\tt get\_start\_symbol} returns the initial symbol of the grammar, and
 is needed by the dependency parser (see below).

\subsection{Shallow Parser CFG file}
\label{file-cfg}

   This file contains a CFG grammar for the chart parser, and some
  directives to control which chart edges are selected to build the
  final tree.
  Comments may be introduced in the file, starting with ``\%'', the
  comment will finish at the end of the line.

   Grammar rules have the form: {\tt x ==> y, A, B.} 

   That is, the head of the rule is a non-terminal specified at the
   left hand side of the arrow symbol. The body of the rule is a
   sequence of terminals and nonterminals separated with commas and
   ended with a dot.

   Empty rules are not allowed, since they dramatically slow chart
   parsers. Nevertheless, any grammar may be written without empty
   rules (assuming you are not going to accept empty sentences).
  
   Rules with the same head may be or-ed using the bar symbol,
  as in: {\tt x ==> A, y | B, C.}

   The head component for the rule maybe specified prefixing it with a
   plus (+) sign, e.g.: {\tt nounphrase ==> DT, ADJ, +N, prepphrase. }.  
   If the head is not specified, the first symbol on
   the right hand side is assumed to be the head.  The head marks are
   not used in the chart parsing module, but are necessary for later
   dependency tree building.

   The grammar is case-sensitive, so make sure to write your terminals
  (PoS tags)  exactly as they are output by the tagger. Also, make
  sure that you capitalize your non-terminals in the same way
  everywhere they appear.

   Terminals are PoS tags, but some variations are allowed for
   flexibility:
   \begin{itemize}
     \item Plain tag:  A terminal may be a plain complete PoS tag,
     e.g. {\tt VMIP3S0}
     \item Wildcarding:  A terminal may be a PoS tag prefix,
     right-wilcarded, e.g. {\tt VMI*}, {\tt VMIP*}. 
    \item Specifying lemma: A terminal may be a PoS tag (or a
    wilcarded prefix) with a lemma enclosed in angle brackets,
     e.g  \verb#VMIP3S0<comer>#,  \verb#VMI*<comer># will match only
    words with those tag/prefix and lemma.
    \item Specifying form: A terminal may be a PoS tag (or a
    wilcarded prefix) with a form enclosed in parenthesis,
     e.g  {\tt VMIP3S0(comi\'o)},  {\tt VMI*(comi\'o)} will match only
    words with those tag/prefix and form.
    \item If a double-quoted string is given inside the angle brackets
      or parenthesis (for instance: \verb#VMIP3S0<"mylemmas.dat">#,
      or \verb#VMI*("myforms.dat")#) it is interpreted as a file
      name, and the terminal will match any lemma (or word form) found
      in that file.  If the file name is not an absolute path, it is
      interpreted as a relative path based at the location of the
      grammar file.
   \end{itemize}

   The grammar file may contain also some directives to help
   the parser decide which chart edges must be selected to build the
   tree.
   Directive commands start with the directive name (always prefixed
   with ``@''), followed by one or  more non-terminal symbols,
   separated with spaces. The list must end with a dot.
   \begin{itemize}
     \item {\tt @NOTOP} Non-terminal symbols listed under this
     directive will not be considered as valid tree roots, even if
     they cover the complete sentence.
     %% \item {\tt @ONLYTOP} Non-terminal symbols listed under this
     %% directive will be considered only if they are rooting a tree 
     %% covering the whole sentence.  (NOT IMPLEMENTED)
     \item {\tt @START} Specify which is the start symbol of the
       grammar. Exactly one non-terminal must be specified under this
       directive. 
       The parser will attempt to build a tree with this symbol as a
       root. If the result of the parsing is not a complete tree, or 
       no valid root nodes are found, a fictitious root node is
       created  with this label, and all created trees are attached to it.
     \item {\tt @FLAT} Subtrees for "flat" non-terminal symbols are flattened when
     the symbol is recursive. Only the highest occurrence appears 
     in the final parse tree.
     \item {\tt @HIDDEN} Non-teminal symbols specified under this
     directive will not appear in the final parse tree (their
     descendant nodes will be attached to their parent).
     \item {\tt @PRIOR} lists of non-terminal symbols in decreasing 
     priority order (the later in the list, the lower priority).
     When a top cell can be covered with two different non-terminals,
     the one with highest priority is chosen.  This has no effect
     on non-top cells (in fact, if you want that, your grammar
     is probably ambiguous and you should rethink it...)
   \end{itemize}

%..................................................

\section{Dependency Parser Module}

  The Txala dependency parser \cite{atserias05} gets parsed sentences --that is, {\tt sentence} objects which have been enriched with a {\tt parse\_tree} by the {\tt chart\_parser} (or by any other means).

\begin{verbatim}
class dep_txala : public dependency_parser {
 public:   
   /// constructor
   dep_txala(const std::string &, const std::string &);

   /// analyze given sentence.
   void analyze(sentence &) const;
   /// analyze given sentences.
   void analyze(std::list<sentence> &) const;
   /// return analyzed copy of given sentence
   sentence analyze(const sentence &) const;
   /// return analyzed copy of given sentences
   std::list<sentence> analyze(const std::list<sentence> &) const;
};
\end{verbatim}

  The constructor receives two strings: the name of the file containging the dependency rules to be used, and the start symbol of the grammar used by the {\tt chart\_parser} to parse the sentence.

   The dependency parser works in three stages:
\begin{itemize}
\item At the first stage, the
   \verb#<GRPAR># rules are used to complete the shallow parsing
   produced by the chart into a complete parsing tree.  The rules are
   applied to a pair of adjacent chunks. At each step, the selected
   pair is fused in a single chunk. The process stops when only one chunk remains

\item The next step is an automatic conversion of the complete parse tree to
  a dependency tree. Since the parsing grammar encodes information about the head of
  each rule, the conversion is straighforward

\item The last step is the labeling. Each edge in the dependeny tree is labeled with a 
  syntactic function, using the \verb#<GRLAB># rules
\end{itemize}

  The syntax and semantics of \verb#<GRPAR># and \verb#<GRLAB># rules are described in 
section \ref{file-dep}.

\subsection{Dependency Parsing Rule File}
\label{file-dep}

  The dependency rules file contains a set of rules to perform dependency parsing.

  The file consists of four sections:
  sections: \verb#<GRPAR>#, \verb#<GRLAB>#, \verb#<SEMDB>#, and \verb#<CLASS>#,
  respectively closed by tags \verb#</GRPAR>#, \verb#</GRLAB>#, \verb#</SEMDB>#, and \verb#</CLASS>#.

\subsubsection{Parse--tree completion rules}

  Section \verb#<GRPAR># contains rules to complete the
    partial parsing provided by the chart parser. The tree is
    completed by combining chunk pairs as stated by the rules. Rules
    are applied from highest priority (lower values) to lowest
    priority (higher values), and left-to right.
    That is, the pair of adjacent chunks matching the most prioritary
    rule is found, and the rule is applied, joining both chunks in
    one. The process is repeated until only one chunk is left.

    The rules can be enabled/disabled via the activation of global flags.
    Each rule may be stated to be enabled only if certain flags are on. 
    If none of its enabling flags are on, the rule is not applied.
    Each rule may also state which flags have to be toggled on/off after
    its application, thus enabling/disabling other rule subsets.
    \medskip

    Each line contains a rule, with the format:
\begin{verbatim}
priority flags context (ancestor,descendant) operation op-params flag-ops
\end{verbatim}
  where:
  \begin{itemize}
    \item \verb#priority# is a number stating the priority of a rule
      (the lower the number, the higher the priority).

    \item\verb#flags# is a list of strings separated by vertical bars
      (``\verb#|#'').  Each string is the name of a flag that will
      cause the rule to be enabled.  If \verb#enabling_flags# equals
      ``\verb#-#'', the rule will be always enabled.

    \item \verb#context# is a context limiting the application of the
      rule only to chunk pairs that are surrounded by the appropriate
      context (``\verb#-#'' means no limitations, and the rule is applied to
      any matching chunk pair) (see below).

    \item \verb#(ancestor,descendant)# are the labels of the adjacent pair of
    chunks the rule will be applied to. The labels are either assigned by
    the chunk parser, or by a \verb#RELABEL# operation on some other completion rule.
    The pair must be enclosed in parenthesis, separated by a comma, and 
    contain NO whitespaces.

    The chunk labels may be suffixed with one extra condition of the form:
    \verb#(form)#, \verb#<lemma>#, \verb#[class]#, or \verb#{PoS_regex}#.

    For instance,
 
    \begin{tabular}{|l|l|}
    The label: & Would match: \\ \hline\hline
    \verb#np#  & any chunk labeled \verb#np# by the chunker \\ \hline
    \verb#np(cats)# & any chunk labeled \verb#np# by the chunker \\
                    & with a head word with form \verb#cats# \\ \hline
    \verb#np<cat>#  & any chunk labeled \verb#np# by the chunker \\
                    & with a head word with lemma \verb#cat# \\ \hline
    \verb#np[animal]# & any chunk labeled \verb#np# by the chunker \\
                      & with a head word with a lemma in \verb#animal# \\
                      & category (see \verb#CLASS# section below) \\ \hline
    \verb#np{^N.M}# & any chunk labeled \verb#np# by the chunker \\
                    & with a head word with a PoS tag matching \\
                    & the \verb#^N.M# regular expression\\ \hline
    \end{tabular}

    \item \verb#operation# is the way in which \verb#ancestor#
    and \verb#descendant# nodes are to be combined (see below).

    \item The \verb#op-params# component has two meanings, depending
      on the \verb#operation# field: \verb#top_left# and
      \verb#top_right# operations must be followed by the literal
      \verb#RELABEL# plus the new label(s) to assign to the chunks.
      Other operations must be followed by the literal \verb#MATCHING# 
      plus the label to be matched.

      For \verb#top_left# and \verb#top_right# operations the labels
      following the keyword \verb#RELABEL# state the labels with which
      each chunk in the pair will be relabelled, in the format
      \verb#label1:label2#.  If specified, \verb#label1# will be the
      new label for the left chunk, and \verb#label2# the one for the
      right chunk. A dash ( ``\verb#-#'') means no relabelling. In none of
      both chunks is to be relabelled,  ``\verb#-#'' may be used instead of
       ``\verb#-:-#''.\\
      For example, the rule:\\
      \verb#20 - - (np,pp<of>) top_left RELABEL np-of:-  - #\
      will hang the \verb#pp# chunk as a daughter of the left chunk 
      in the pair (i.e. \verb#np#), then
      relabel the \verb#np# to \verb#np-of#, and leave the label for 
      the \verb#pp# unchanged.

       For \verb#last_left#, \verb#last_right# and
       \verb#cover_last_left# operations, the label following the keyword
       \verb#MATCHING# states the label that a node must have in
       order to be considered a valid ``last'' and get the subtree as
       a new child. This label may carry the same modifying suffixes
       than the chunk labels. If no node with this label is found in
       the tree, the rule is not applied.\\
      For example, the rule:\\
      \verb#20 - - (vp,pp<of>) last_left MATCHING np -#\\
      will hang the \verb#pp# chunk as a daughter of the last subtree 
      labeled \verb#np# found inside the \verb#vp# chunk.

   \item The last field \verb#flag-ops# is a space-separated list of
     flags to be toggled on/off.  The list may be empty (meaning that
     the rule doesn't change the status of any flag).  If a flag name
     is preceded by a ``\verb#+#'', it will be toggled on. If the leading
     symbol is a ``\verb#-#'', it will be toggled off.

  \end{itemize}

  For instance, the rule:
\begin{verbatim}
  20 - - (np,pp<of>) top_left RELABEL - -
\end{verbatim}

  states that if two subtrees labelled \verb#np# and \verb#pp# are
  found contiguous in the partial tree, and the second head word has
  lemma \verb#of#, then the later (rightmost) is added as a new child
  of the former (leftmost), whatever the context is, without need of
  any special flag active, and performing no relabelling of the new
  tree root.

   The supported tree-building operations are the following:
  \begin{itemize}
   \item \verb#top_left#: The right subtree is added as a daughter of
     the left subtree. The root of the new tree is the root of the
     left subtree. If a \verb#label# value other than ``\verb#-#'' is
     specified, the root is relabelled with that string.
   \item \verb#last_left#: The right subtree is added as a daughter of
     the last node inside the left subtree matching \verb#label# value
     (or to the root if none is found). The root of the new tree is
     the root of the left subtree.
   \item \verb#top_right#: The left subtree is added as a new daughter
     of the right subtree. The root of the new tree is the root of the
     right subtree. If a \verb#label# value other than ``\verb#-#'' is
     specified, the root is relabelled with that string.
   \item \verb#last_right#: The left subtree is added as a daughter of
     the last node inside the right subtree matching \verb#label#
     value (or to the root if none is found). The root of the new tree
     is the root of the right subtree.
   \item \verb#cover_last_left#: The left subtree ($s$) takes the
     position of the last node ($x$) inside the right subtree matching
     \verb#label# value. The node $x$ is hanged as new child of $s$.
     The root of the new tree is the root of the right subtree.
  \end{itemize}

  The context may be specified as a sequence of chunk labels,
  separated by underscores ``\verb#_#''.
   One of the chunk labels must be \verb#$$#, and refers to the pair of chunks
  which the rule is being applied to.

  For instance, the rule:
\begin{verbatim}
   20 - $$_vp (np,pp<of>) top_left RELABEL -
\end{verbatim}

  would add the rightmost chunk in the pair (\verb#pp<of>#) under the
  leftmost (\verb#np#) only if the chunk immediate to the right of the pair
  is labeled \verb#vp#.

  Other admitted labels in the context are: \verb#?# (matching exactly
  one chunk, with any label), \verb#*# (matching zero or more chunks
  with any label), and \verb#OUT# (matching a sentence boundary).

  For instance the context \verb#np_$$_*_vp_?_OUT# would match a
  sentence in which the focus pair of chunks is immediately after an
  \verb#np#, and the second-to-last chunk is labeled \verb#vp#.

  Context conditions can be globally negated preceding them with an exclamation
  mark (\verb#!#). E.g. \verb#!np_$$_*_vp# would cause the rule to be applied only
  if that particular context is {\em not satisfied}.

  Context condition components may also be individually negated
  preceding them with the symbol \verb#~#. E.g. the rule
  \verb#np_$$_~vp# would be satisfied if the preceding chunk is
  labeled \verb#np# and the following chunk has any label but
  \verb#vp#.

  Enabling flags may be defined and used at the grammarian's will. 
  For instance, the rule:
\begin{verbatim}
20 INIT|PH1 $$_vp (np,pp<of>) last_left MATCHING npms[animal] +PH2 -INIT -PH1
\end{verbatim}

   Will be applied if either \verb#INIT# or \verb#PH1# flags are
   on, the chunk pair is a \verb#np# followed by a \verb#pp# with head
   lemma \verb#of#, and the context (one \verb#vp# chunk following the
   pair) is met. Then, the deepest rightmost node matching the label
   \verb#npms[animal]# will be sought in the left chunk, and the right
   chunk will be linked as one of its children. If no such node is found,
   the rule will not be applied. 
   
    After applying the rule, the flag \verb#PH2# will be toggled
    on, and the flags \verb#INIT# and \verb#PH1# will be toggled
    off.

    The only predefined flag is \verb#INIT#, which is toggled on when
    the parsing starts.  The grammarian can define any alphanumerical
    string as a flag, simply toggling it on in some rule.

\subsubsection{Dependency function labeling rules}
 Section \verb#<GRLAB># contains two kind of lines.

  The first kind are the lines defining \verb#UNIQUE# labels, which
  have the format:
\begin{verbatim}
  UNIQUE label1 label2 label3 ...
\end{verbatim}

  You can specify many \verb#UNIQUE# lines, each with one or more
  labels. The effect is the same than having all of them in a single
  line, and the order is not relevant.

  Labels in \verb#UNIQUE# lists will be assigned only once per
  head. That is, if a head has a daugther with a dependency already
  labeled as \verb#label1#, rules assigning this label will be ignored
  for all other daugthers of the same head. (e.g. if a verb has got a
  \verb#subject# label for one of its dependencies, no other
  dependency will get that label, even if it meets the conditions to
  do so).

   The second kind of lines state the rules to label the
  dependences extracted from the full parse tree build with the
  rules in previous section:
  
  Each line contains a rule, with the format:
\begin{verbatim}
  ancestor-label dependence-label condition1 condition2 ...
\end{verbatim}

  where:
  \begin{itemize}
    \item  \verb#ancestor-label# is the label of the node which is
    head of the dependence.
    \item \verb#dependence-label# is the label to be assigned to the dependence
    \item \verb#condition# is a list of conditions that the dependence
    has to match to satisfy the rule.
  \end{itemize}

   Each \verb#condition# has one of the forms:
\begin{verbatim}
  node.attribute = value
  node.attribute != value
\end{verbatim}

   Where \verb#node# is a string describing a node on which the
   \verb#attribute# has to be checked.  The \verb#value# is a string
   to be matched, or a set of strings (separated by ``\verb#|#''). The
   strings can be right-wildcarded (e.g. \verb#np*# is allowed, but
   not \verb#n*p#. For the \verb#pos# attribute, \verb#value# can be
   any valid regular expression.

   The \verb#node# expresses a path to locate the node to be
   checked. The path must start with \verb#p# (parent node) or
   \verb#d# (descendant node), and may be followed by a
   colon-separated list of labels. For instance \verb#p:sn:n# refers
   to the first node labeled \verb#n# found under a node labeled
   \verb#sn# which is under the dependency parent \verb#p#.

   The \verb#node# may be also \verb#As# ({\em All siblings}) or
   \verb#Es# ({\sl Exists sibling}) which will check the list of all
   children of the ancestor (\verb#p#), excluding the focus daughter
   (\verb#d#).  \verb#As# and \verb#Es# may be followed by a path,
   just like \verb#p# and \verb#d#. For instance, \verb#Es:sn:n# 
   will check for a sibling with that path, and  \verb#As:sn:n# 
   will check that all sibling have that path.
  
   Possible {\tt attribute} to be used:
  \begin{itemize}
    \item \verb#label#: chunk label (or PoS tag) of the node.
    \item \verb#side#: (left or right) position of the specified node with respect to the other.  Only valid for \verb#p# and \verb#d#.
    \item \verb#lemma#: lemma of the node head word.
    \item \verb#pos#: PoS tag of the node head word
    \item \verb#class#: word class (see below) of lemma of the node head word.
    \item \verb#tonto#: EWN Top Ontology properties of the node head word.
    \item \verb#semfile#: WN semantic file of the node head word.
    \item \verb#synon#: Synonym lemmas of the node head word (according to WN).
    \item \verb#asynon#: Synonym lemmas of the node head word ancestors (according to WN).
  \end{itemize}

  Note that since no disambiguation is required, the attributes dealing with semantic properties will be satisfied if any of the word senses matches the condition.

   For instance, the rule:
\begin{verbatim}
verb-phr    subj    d.label=np*      d.side=left
\end{verbatim}
  states that if a \verb#verb-phr# node has a daughter to its left, with a label
  starting by \verb#np#, this dependence is to be labeled as \verb#subj#.
 
   Similarly, the rule:
\begin{verbatim}
verb-phr    obj    d.label=np*  d:sn.tonto=Edible  p.lemma=eat|gulp
\end{verbatim}
  states that if a \verb#verb-phr# node has {\tt eat} or {\tt gulp} as
  lemma, and a descendant with a label starting by \verb#np# and containing
  a daughter labeled \verb#sn# that has {\tt Edible} property in EWN
  Top ontology, this dependence is to be labeled as \verb#obj#.

   Another example:
\begin{verbatim}
verb-phr    iobj   d.label=pp* d.lemma=to|for  Es.label=np*
\end{verbatim}
  states that if a \verb#verb-phr# has a descendant with a label
  starting by \verb#pp# (prepositional phrase) and lemma {\em to} or
  {\em for}, and there is another child of the same parent which is 
  a noun phrase (\verb#np*#), this dependence is to be
  labeled as \verb#iobj#.

   Yet another:
\begin{verbatim}
verb-phr    dobj   d.label=pp* d.lemma=to|for  As.label!=np*
\end{verbatim}
  states that if a \verb#verb-phr# has a descendant with a label
  starting by \verb#pp# (prepositional phrase) and lemma {\em to} or
  {\em for}, and {\em all} the other children of the same parent are {\em not}
  noun phrases (\verb#np*#), this dependence is to be
  labeled as \verb#dobj#.

\subsubsection{Semantic database location}
  Section \verb#<SEMDB># is only necessary if the dependency labeling rules in section \verb#<GRLAB># use conditions on semantic values (that is, any of \verb#tonto#, \verb#semfile#, \verb#synon#, or \verb#asynon#).  Since it is needed by \verb#<GRLAB># rules, section \verb#<SEMDB># must be defined {\em before} section \verb#<GRLAB>#.
  The section must contain a single line specifying a configuration file for a semanticDB object. The filename may be absolute or relative to the location of the dependency rules file.
\begin{verbatim}
<SEMDB>
../semdb.dat
</SEMDB>
\end{verbatim}

  The configuration file must follow the format described in section \ref{semdb}.

\subsubsection{Class definitions}
    Section \verb#<CLASS># contains class definitions which may
   be used as attributes in the dependency labelling rules.

   Each line contains a class assignation for a lemma, with two possible formats:
\begin{verbatim}
  class-name  lemma      comments
  class-name  "filename"   comments
\end{verbatim}

   For instance, the following lines assign to the class \verb#mov#
   the four listed verbs, and to the class \verb#animal# all lemmas
   found in \verb#animals.dat# file.  In the later case, if the file
   name is not an absolute path, it is interpreted as a relative path
   based at the location of the rule file.

    Anything to the right of the second field is considered a comment and ignored.
\begin{verbatim}
mov     go      prep= to,towards
mov     come    prep= from
mov     walk    prep= through
mov     run     prep= to,towards   D.O.

animal "animals.dat"
\end{verbatim}



%..................................................
\section{Coreference Resolution Module}
\label{mod-coref}

  This module is a machine-learning based coreference solver,
  following the algorithm proposed by \cite{soon01}.  It takes a
  document parsed by the shallow parser to detect noun phrases, and
  decides which noun phrases are coreferential.

  The api of the module is the following:
\begin{verbatim}
class coref {
   public:
    /// Constructor
    coref(const std::string &, const int);

    /// Classify in coreference chains noun phrases in given document
    void analyze(document &) const;
};
\end{verbatim}

  The parameters received by the constructor are a filename, and an
  integer bitmask specifying which attributes have to be used by the
  classifier.

  The meaning of the attributes can be found in the source file {\tt include/freeling/coref\_fex.h}. If you just want to use the module, set the value of this parameter to 0xFFFFFF to select all the attributes.

  The string parameter is the name of the configuration file, which is described below:

\subsection{Coreference Solver configuration file}

  The Coreference Solver module reads this file to find out some needed parameters.  The file has three sections:
\begin{itemize}
   \item Section \verb#<ABModel># path to the file containing the
     trained AdaBoost model.  The {\tt .abm} file contains an AdaBoost
     model based on shallow Decision Trees (see \cite{carreras03} for
     details). You don't need to understand this, unless you want to
     enter into the code of the AdaBoost classifier.

     The name of the file may be either absolute or relative to the 
     location of the Coreference Solver config file.\\
     e.g:
\begin{verbatim}
<ABModel>
coref.abm
</ABModel>
\end{verbatim}

     It may be generated from an annotated corpus using the training
     programs that can be found in {\tt src/utilities/coref}.

     If you need to know more about this (e.g. to develop a
     Coreference Solver for your language) please contact FreeLing
     authors.

   \item Section \verb#<SemDB># specifies the files that contain a
    semantic database. This is required to compute some WN-based attributes
    used by the solver.  

    The section must contain two lines specifying two semantic
    information files, a {\tt SenseFile} and a {\tt WNFile}. The
    filenames may be absolute or relative to the location of the
    dependency rules file. For example:
\begin{verbatim}
<SEMDB>
SenseFile ../senses30.src
WNFile    ../../common/wn30.src
</SEMDB>
\end{verbatim}

   \item Section \verb#<MaxDistance># states the maximum distance (in
     words) at which possible corefernces will be considered. Short
     values will cause the solver to miss distant coreferents.
     Long distances will introduce a huge amount of possible coreferent 
     candidate pairs, slow the system, and produce a larger amount of
     false positives.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Other useful modules}

 FreeLing contains some internal classes used by the analysis modules 
described in Chapter~\ref{chap-modules}. 

 Some applications may be interested in directly accessing these lower-level
utilities, the most relevant of which are described in this chapter.

%..................................................
\section{Tag Set Managing Module}
\label{file-tagset}

 This module is able to store information about a tagset, and offers some 
useful functions on PoS tags and morphological features.

 This module is internally used by some analyzers (e.g. probabilities module, 
HMM tagger, feature extraction, ...) but can be instantiated and called by
any user application that requires it.

  The API of the module is: 
\begin{verbatim}
class tagset {
 
  public:
    /// constructor: load a tag set description file
    tagset(const std::wstring &f);
    /// destructor
    ~tagset();

    /// get short version of given tag
    std::wstring get_short_tag(const std::wstring &tag) const;

    /// get list of <feature,value> pairs with morphological
    /// information for given tag
    std::list<std::pair<std::wstring,std::wstring> >
              get_msf_features(const std::wstring &tag) const;

    /// get list <feature,value> pairs with morphological 
    /// information, in a string format
    std::wstring get_msf_string(const std::wstring &tag) const;
};
\end{verbatim}

  The class constructor receives a file name with a tagset description. Format of the file 
  is described below.  The class offers two services: 
  \begin{enumerate} 
  \item Get the short version of a tag. This is useful for EAGLES tagsets, and required by some 
   modules (e.g. PoS tagger). The length of a short tag depends on the language and part-of-speech,
   and the criteria to select it is usually to have a tag informative enough (capturing relevant
   features such as category, subcategory, case, etc) but also general enough so that significative
   statistics for PoS tagging can be acquired from reasonably-sized corpora.
  \item Decompose a tag into a list of pairs feature-value (e.g. {\tt
    gender=masc}, {\tt num=plural}, {\tt case=dative}, etc).  This can be retrieved 
    as a list of string pairs, or as a formatted string.
  \end{enumerate} 

\subsection{Tagset Description File}

  Tagset description file has two sections: \verb#<DecompositionRules># 
  and \verb#<DirectTranslations>#, which describe how tags are converted to their 
  short version and decomposed into morphological feature-value pairs

   \begin{itemize}
   \item Section \verb#<DirectTranslations># describes a direct mapping from a tag 
    to its short version and to its feature-value pair list. Each line in the section
    corresponds to a tag, and has the format: \verb#tag  short-tag  feature-value-pairs#

    For instance the line: \verb#NCMS000 NC postype=common|gender=masc|number=sg#
    states that the tag \verb#NCMS000# is shortened as \verb#NC# and that its list
    of feature-value pairs is the one specified. 
    
    This section has precedence over section
    \verb#<DecompositionRules>#, and can be used as an exception list.
    If a tag is found in section \verb#<DirectTranslations>#, the rule
    is applied and any rule in section \verb#<DecompositionRules># for
    this tag is ignored.

   \item Section \verb#<DecompositionRules># encodes rules to compute the morphological features
    from an EAGLES tag digits. The form of each line is:
\begin{verbatim}
tag short-tag-size digit-description-1 digit-description-2 ...
\end{verbatim}
    where \verb#tag# is the digit for the category in the EAGLES PoS
    tag (i.e. the first digit: \verb#N#, \verb#V#, \verb#A#, etc.),
    and \verb#short-tag-size# is an integer stating the length of the
    short version of the tag (e.g. if the value is 2, the first two
    digits of the EAGLES PoS tag will we used as short
    version). Finally, fields \verb#digit-description-n# contain
    information on how to interpret each digit in the EAGLES PoS tag

    There should be as many \verb#digit-description# fields as digits
    there are in the PoS tag for that category. Each
    \verb#digit-description# field has the format:\\
    \verb#feature/digit:value;digit:value;digit:value;...#\\
    That is: the name of the feature encoded by that digit, followed by a
    slash, and then a semicolon-separated list of translation pairs
    that, for each possible digit in that position give the feature
    value.

    For instance, the rule for Spanish noun PoS tags is (in a single line):\\
    \verb#N 2 postype/C:common;P:proper gen/F:f;M:m;C:c num/S:s;P:p;N:c#\\
    \verb#                         neclass/S:person;G:location;O:organization;V:other#\\
    \verb#                         grade/A:augmentative;D:diminutive#\\
    and states that any tag starting with {\tt N} (unless it is 
    found in section \verb#<DirectTranslations>#) will be shortened
    using its two first digits (e.g. {\tt NC}, or {\tt NP}). Then, the
    description of each digit in the tag follows, encoding the information:
    \begin{enumerate}
     \item {\tt postype/C:common;P:proper} - second digit is the
       subcategory (feature {\tt postype}) and its possible values are {\tt C}
       (translated as {\tt common}) and {\tt P} (translated as {\tt
         proper}).
     \item {\tt gen/F:f;M:m;C:c} - third digit is the gender (feature {\tt gen}) and 
        its possible values are {\tt F} (feminine, translated as {\tt f}), {\tt M} (masculine,
        translated as {\tt m}), and {\tt C} (common/invariable, translated as {\tt c}).
     \item {\tt num/S:s;P:p;N:c} - fourth digit is the number (feature {\tt num}) and its 
       possible values are {\tt S} (singular, translated as {\tt s}), {\tt P} (plural,
        translated as {\tt p}), and {\tt C} (common/invariable, translated as {\tt c}).       
     \item {\tt neclass/S:person;G:location;O:organization;V:other} -
       Fifth digit is the semantic class for proper nouns (feature
       {\tt neclass}), with possible values {\tt S} (translated as
       {\tt person}), {\tt G} (translated as {\tt location}), {\tt O}
       (translated as {\tt organization}), and {\tt V} (translated as
       {\tt other}).
     \item {\tt grade/A:augmentative;D:diminutive} - sixth digit is
       the grade (feature {\tt grade}) with possible values {\tt A}
       (translated as {\tt augmentative}), and {\tt D} (translated as
       {\tt diminutive}).
    \end{enumerate}
    
    If a feature is underspecified or not appliable, a zero ({\tt 0})
    is expected in the appropriate position of the PoS tag.

    With the example rule described above, the tag translations in table
    \ref{tr-tags} would take place:
\begin{table}[htb] \center
\begin{tabular}{|l|ll|} 
 EAGLES PoS tag & short version & morphological features \\ \hline
 \verb#NCMS00#   & \verb#NC# & \verb#postype=common|gen=m|num=s# \\
 \verb#NCFC00#   & \verb#NC# & \verb#postype=common|gen=f|num=c# \\
 \verb#NCFP0A#   & \verb#NC# & \verb#postype=common|gen=f|num=p|grade=augmentative# \\
 \verb#NP0000#   & \verb#NP# & \verb#postype=proper# \\
 \verb#NP00G0#   & \verb#NP# & \verb#postype=proper|neclass=location# \\ \hline
\end{tabular}
\label{tr-tags}
\caption{Example results for tagset managing module.}
\end{table}

\end{itemize}

%..................................................
\section{Semantic Database Module}
\label{semdb}

  This module is not a main processor in the default analysis chain,
  but it is used by the other modules that need access
  to the semantic database: The sense annotator {\tt senses}, the word sense 
  disambiguator {\tt ukb\_wrap}, the
  dependency parser {\tt dep\_txala}, and the coreference solver {\tt coref}.

  Moreover, this module can be used by the applications to enrich or 
  post process the results of the analysis.
  
  The API for this module is 

\begin{verbatim}
class semanticDB {
  public:
    /// Constructor
    semanticDB(const std::string &); 

    /// Compute list of lemma-pos to search in WN for given word,
    /// according to mapping rules.
    void get_WN_keys(const std::wstring &, 
                     const std::wstring &, 
                     const std::wstring &,
                     std::list<std::pair<std::wstring,std::wstring> > &) const;

    /// get list of words for a sense+pos
    std::list<std::string> get_sense_words(const std::string &, 
                                           const std::string &) const;

    /// get list of senses for a lemma+pos
    std::list<std::string> get_word_senses(const std::string &, 
                                           const std::string &) const;

    /// get sense info for a sensecode+pos
    sense_info get_sense_info(const std::string &, const std::string &) const;
};
\end{verbatim}

  The constructor receives a configuration file, with the following contents:
\begin{itemize}
 \item A section \verb#<WNPosMap># which establishes which PoS found
   in the morphological dictionary should be mapped to each WN
   part-of-speech. Rule format is described in section \ref{wnmap}.

 \item A section \verb#<DataFiles># specifying
  the knowledge bases required by the algorithm.
  This section may contain up to three keywords, with the format:
  \begin{verbatim}
    <DataFiles>
    senseDictFile  ./senses30.src
    wnFile  ../common/wn30.src
    formDictFile  ./dicc.src
    </DataFiles>
  \end{verbatim}

  \verb#senseDictFile# is the sense repository, with the format
  described in section \ref{file-sense}.

  \verb#wnFile# is a file stating hyperonymy relations and other semantic
   information for each sense. The format is described in section \ref{file-wn}.

  \verb#formDictFile# may be needed if mapping rules in
  \verb#<WNPosMap># require it.  It is a regular form file with morphological information,
   as described in section \ref{file-dict}.
\end{itemize}
 
\subsection{PoS mapping rules} 
\label{wnmap}

  Each line in section \verb#<WNPosMap># defines a mapping rule, with
  the format \verb#FreeLing-PoS WN-PoS search-key#, where
  \verb#FreeLing-PoS# is a prefix for a FreeLing PoS tag,
  \verb#WN-Pos# must be one of {\tt n}, {\tt a}, {\tt r}, or {\tt v},
  and \verb#search-key# defines what should be used as a lemma to
  search the word in WN files.

   The given \verb#search-key# may be one of {\tt L}, {\tt F}, or a
   FreeLing PoS tag.  If {\tt L} ({\tt F}) is given, the word lemma
   (form) will be searched in WN to find candidate senses.  If a
   FreeLing PoS tag is given, the form for that lemma with the given
   tag will be used.
  \medskip
    
  \noindent \textbf{Example 1:} For English, we could have a mapping like:
   \begin{verbatim}
    <WNposMap>
    N n L
    J a L
    R r L
    V v L
    VBG a F
    </WNposMap>
   \end{verbatim}
   which states that for words with FreeLing tags starting with {\tt
     N}, {\tt J}, {\tt R}, and {\tt V}, lemma will be searched in
   wordnet with PoS {\tt n}, {\tt a}, {\tt r}, and {\tt v}
   respectively.  It also states that words with tag {\tt VBG}
   (e.g. {\sl boring}) must be searched as adjectives ({\tt a}) using
   their form (that is, {\sl boring} instead of lemma {\sl bore}).
   This may be useful, for instance, if FreeLing English dictionary
   assigns to that form a gerund analysis ({\sl bore VBG}) but not an 
   adjective one.
  \medskip
   
  \noindent \textbf{Example 2:} A similar example for Spanish, could be:
   \begin{verbatim}
     <WNposMap>
     N n L
     A a L
     R r L
     V v L
     VMP a VMP00SM
     </WNposMap>
   \end{verbatim}
   which states that for words with FreeLing tags starting with {\tt
     N}, {\tt A}, {\tt R}, and {\tt V}, lemma will be searched in
   wordnet with PoS {\tt n}, {\tt a}, {\tt r}, and {\tt v}
   respectively.  It also states that words with tag starting with
   {\tt VMP} (e.g. {\sl cansadas}) must be searched as adjectives
   ({\tt a}) using the form for the same lema (i.e. {\sl cansar}) that
   matches the tag {\tt VMP00SM} (resulting in {\sl cansado}).  This
   is useful to have participles searched as adjectives, since
   FreeLing Spanish dictionary doesn't contain any participle as adjective, 
   but esWN does.


\subsection{Sense Dictionary File}
\label{file-sense}

  This source file (e.g. {\tt senses30.src} provided with FreeLing)
  must contain the word list for each synset, one entry per line.

  Each line has format: {\tt sense word1 word2 ...}.\\
  E.g. \\
  \verb#00045250-n actuation propulsion# \\
  \verb#00050652-v assume don get_into put_on wear# \\
  
  Sense codes can be anything (assuming your later processes know what
  to do with them) provided they are unambiguous (there are not two 
  lines with the same sense code).
  The files provided in FreeLing contain WordNet 3.0 synset codes. 


\subsection{WordNet file}
\label{file-wn}

  This source file (e.g. {\tt wn30.src} provided with FreeLing)
  must contain at each line the information relative to a sense, 
  with the following format:
\begin{verbatim}
sense hypern:hypern:...:hypern  semfile  TopOnto:TopOnto:...:TopOnto  sumo  cyc
\end{verbatim}

 That is: the first field is the sense code.
 The following fields are:
 \begin{itemize}
  \item A colon-separated list of hypernym synsets.
  \item WN semantic file the synset belongs to.
  \item A colon-separated list of EuroWN TopOntology codes valid for the synset.
  \item A code for an equivalent (or near) concept in SUMO ontology.  
        See SUMO documentation for a description of the code syntax.
  \item A code for an equivalent concept in CyC ontology.
  \end{itemize}

  Note that the only WN relation encoded here is hypernymy. Note also that
  semantic codes such as WN semantic file or EWN TopOntology features
  are simply (lists of) strings. Thus, you can include in this file any
  ontological or semantic information you need, just substituing the
  WN-related codes by your own semantic categories.

%..................................................
\section{Approximate search dictionary}
\label{sec-foma}

  This class wraps a \texttt{libfoma} FSM and allows fast retrieval of similar 
words via string edit distance based search.

  The API of the class is the following:
\begin{verbatim}
class foma_FSM {

  public:
    /// build automaton from a file
    foma_FSM(const std::wstring &, const std::wstring &mcost=L""); 
    /// delete FSM
    ~foma_FSM();

    /// Use automata to obtain closest matches to given form, and 
    //add them to given list.
    void get_similar_words(const std::wstring &, 
                           std::list<std::pair<std::wstring,int> > &) const;    
    /// set maximum edit distance of desired results
    void set_cutoff_threshold(int);
    /// set maximum number of desired results
    void set_num_matches(int);
    /// Set default cost for basic SED operations
    void set_basic_operation_cost(int);
  };
\end{verbatim}

  The constructor of the module requests one parameter stating the
  file to load, and a second optional parameter stating a file with
  the cost matrix for SED operations.  If the cost matrix is not 
  given, all operations default to a cost of 1 (or to the value set 
  with the method \verb#set_basic_operation_cost#).

  The automata file may have extension \verb#.src# or  \verb#.bin#.
  If the extension is \verb#.src#, the file is intepreted as a text
  file with one word per line. The FSM is built to recognize the 
  vocabulary contained in the file.

  If the extension is \verb#.bin#, the file is intepreted as a binary
  \texttt{libfoma} FSM.  To compile such a binary file, FOMA command
  line front-end must be used.  The front-end is not included in
  FreeLing. You will need to install FOMA if you want to create binary
  FSM files. See \texttt{http://code.google.com/p/foma} for details.

  A cost matrix for SED operations may be specified \textit{only} for 
  text FSMs (i.e., for \verb#.src# files). 
  To use a cost matrix with a \verb#.bin# file, you can compile
  it into the automata using FOMA front-end.

  The format of the cost matrix must comply with FOMA formats.  See
  FOMA documentation, or examples provided in
  \verb#data/common/alternatives# in FreeLing tarball.

  The method \verb#get_similar_words# will receive a string and 
  return a list of entries in the FSM vocabulary sorted by string 
  edit distance to the input string.

%..................................................
\section{Feature Extraction Module}
\label{sec-rgf}

  Machine Learning based modules (such as BIO named entity recognition
  or classification modules) require the encoding of each word to
  classify as a feature vector.  The conversion of words in a sentence
  to feature vectors, is performed by this module. The features are
  task-oriented, so they vary depending on what is being
  classified. For this reason, the encoding is not hard-wired in the
  code, but dinamically performed interpreting a set of feature rules.

  Thus, the Feature Extraction Module converts words in a sentence to
  feature vectors, using a given set of rules.

  The API of this module is the following:
\begin{verbatim}
class fex {
  private:

  public:
    /// constructor, given rule file, lexicon file (may be empty), and custom functions
    fex(const std::wstring&, const std::wstring&, 
        const std::map<std::wstring,const feature_function *> &);

    /// encode given sentence in features as feature names. 
    void encode_name(const sentence &, std::vector<std::set<std::wstring> > &);
    /// encode given sentence in features as integer feature codes
    void encode_int(const sentence &, std::vector<std::set<int> > &);
    /// encode given sentence in features as integer feature codes and as features names
    void encode_all(const sentence &, std::vector<std::set<std::wstring> > &, 
                    std::vector<std::set<int> > &);

    /// encode given sentence in features as feature names. 
    /// Return result suitable for Java/perl APIs
    std::vector<std::list<std::wstring> > encode_name(const sentence &);
    /// encode given sentence in features as integer feature codes.
    /// Return result suitable for Java/perl APIs
    std::vector<std::set<int> > encode_int(const sentence &);

    /// clear lexicon
    void clear_lexicon(); 
    /// encode sentence and add features to current lexicon
    void encode_to_lexicon(const sentence &);
    /// save lexicon to a file, filtering features with low occurrence rate
    void save_lexicon(const std::wstring &, double) const;
};
\end{verbatim}

  The class may be used to encode a corpus and generate a feature lexicon,
  or to encode a corpus filtering the obtained features using a previously 
  generated feature lexicon.

  The used feature rules must follow the format described in 
  section \ref{fex-file}. The rules may call custom feature functions, provided
  the instantianting program provides pointers to call the apropriate code
  to compute them.

  Once the class is instantiated, it can ben used to encode sentences
  in feature vectors.  Features may be obtained as strings (feature
  names) or integers (feature codes).

  The constructor of the class receives a \texttt{.rgf} file
  containing feature extraction rules, a feature lexicon (mapping
  feature names to integer codes), and a map used to define 
  custom feature functions, with associations between
  \texttt{feature\_function} objects and function names (see section \ref{s:fex-custom}).

  If the lexicon file name is empty, features will be assigned an integer
  code, and the generated lexicon can be saved. This is useful when
  encoding training corpus and feature codes have not been set yet.

\subsection{Feature Extraction Rule File}
\label{fex-file}

  Feature extraction rules are defined in a {\tt .rgf} file. This
  section describes the format of the file. The syntax of the
  rules is described in section \ref{s:fex-rules}

  Rules are grouped in packages. Begin and end of a package is marked
  with the keywords \verb#RULES# and \verb#ENDRULES#.
  Packages are useful to simplify the rules, and to speed up
  feature computation avoiding computing the same features several
  times.

  A line with format \verb#TAGSET filename# may precede the rule
  packages definition. The given \verb#filename# will be interpreted
  as a relative path (based on the {\tt .rgf} location) to a tagset
  definition file (see section \ref{file-tagset}) that will be used to
  obtain short versions of PoS tags. The \verb#TAGSET# line is 
  needed only if the short tag property {\tt t} is used in some
  rule (see section \ref{s:fex-cond} below).

  The \verb#RULES# package starting keyword must be followed by a 
  \textit{condition} (see section \ref{s:fex-cond}).

  The rules in a package will onlly be applied to those words matching 
  the package condition, thus avoiding unnecessary tests.

  \noindent For instance, the rules in the package:
\begin{verbatim} 
RULES t matches ^NP
 ...
ENDRULES
\end{verbatim} 
  \noindent will be applied only for words with a PoS tag ({\tt t})
  starting with {\tt NP}. The same result could have been obtained
  without the package if the same condition was added to each rule,
  but then, applicability tests for each rule on each word would be
  needed, resulting in a higher computational cost.

  The package condition may be {\tt ALL}. In this case, rules
  contained in the package will be checked for all words in the
  sentence.  This condition has also an extra effect: the features
  extracted by rules in this package are cached, in order to avoid
  repeating computations if a rule uses a window to get features from
  neighbour words.

\noindent For instance, the rule:
\begin{verbatim} 
RULES ALL
 punct_mark@   [-2,2]   t matches ^F
ENDRULES
\end{verbatim} 
 \noindent will generate, for each word, features indicating which words in 
  the surrounding two words (left and right) are punctuation symbols.

  With this rule applied to the sentence \textit{Hi ! , said John .},
  the word \textit{said} would get the features
  \texttt{punct\_mark@-1}, \texttt{punct\_mark@-2}, and
  \texttt{punct\_mark@2}. The word \textit{John} would get the
  features \texttt{punct\_mark@-2} and \texttt{punct\_mark@1}.  Since
  the package has condition \texttt{ALL}, the features are computed
  once per word, and then reused (that is, the fact that the comma is a 
  punctuation sign will be checked only once, regardless of the size of the 
  sentence and the size of the windows in the rules).

 \subsection{Rule Syntax}
\label{s:fex-rules}

  Each rule has following syntax:
\begin{verbatim}
 feature-name-pattern window condition
\end{verbatim}

 \begin{itemize} 
   \item \verb#feature-name-pattern# is a string that describes what
     the generated feature name will be.  Some special characters
     allow the insertion of variable values in the feature name. See
     section \ref{s:fex-pattern} for a description of the syntax of
     feature name patterns.

   \item \verb#window# is a range in the format \verb#[num,num]#, and
     states the words around the target word for which the feature has
     to be computed.  A window of \verb#[0,0]# means that the feaure
     is only checked for the target word.

   \item \verb#condition# is the condition that a word has to satisfy
     in order to get the features extracted by the rule. See section
     \ref{s:fex-cond} for a description of condition syntax.

 \end{itemize}

\subsection{Feature Name Pattern Syntax}
 \label{s:fex-pattern}

  Each feature rule has a \verb#feature-name-pattern# that describes how
  the generated feature name will be.

  The following characters are
  special and are interpreted as variables, and replaced by the
  corresponding values:
  \begin{itemize}
    \item Character \verb#@#: will be replaced with the relative
      position of the matching word with respect to the target
      word.  Thus, the rule\\ 
      \verb#punct_mark@ [-2,2] t matches ^F# \\
      will generate a different feature for each word in the window 
      that is a punctuation sign (e.g.  \texttt{punct\_mark@-2} and
      \texttt{punct\_mark@1} for the word \textit{John} in the
      avobe example).
      
      But the rule \verb#punct_mark [-2,2] t matches ^F# \\ will
      generate the same feature for all words in the window that
      are punctuation signs (e.g. it will generate \texttt{punct\_mark} 
      twice for the word \textit{John} in the avobe example).
      Repeated features are stored only once.
      
    \item Character \verb#$# introduces a variable that must have
      the format: \verb#$var(position)#. 
      
      Allowed variable names
      are: \verb#W# (word form, in its original casing), \verb#w#
      (word form, lowercased), \verb#l# (word lemma), \verb#T#
      (word full PoS tag), \verb#t# (word short PoS tag), \verb#a#
      (word lemma+Pos tag).  All above variables refer to the
      analysis selected by the tagger.  Variable names may be
      prefixed with \verb#p# (e.g. \verb#pT#, \verb#pl#, \verb#pa#,
      etc.)  which will generate the feature for all
      \textit{possible} analysis of the word, not just the one selected
      by the tagger.
      
      The \verb#position)# indicates from which word (relative to
      the focus word) the value for the variable must be taken.
      
      For instance, the pattern: \verb#pbig@:$w(0)_$pt(1)# will
      extract features that will contain the relative position
      (\verb#@#), plus a bigram made of the word form of the
      current word in the window (\verb#$w(0)#) plus each possible
      PoS tag of the word right of it (\verb#$pt(1)#).

      In the sentence \textit{John lives here.}, the features for
      word \textit{here} in a window of [-2,0] with the above
      pattern would be: \verb#pbig@-2:john_VBZ#,
      \verb#pbig@-2:john_NNS#, \verb#pbig@-1:lives_RB#, and
      \verb#pbig@0:here_Fp#. Note that there are two features
      generated for window position $-2$ because the word 
      \textit{lives} has two possible PoS tags.
      
    \item Curly brackets {\tt \{ \}} have two possible
      interpretations, depending on what they contain:
      \begin{enumerate}
       \item If the brackets enclose a regex match variable (e.g
         \$0,\$1,\$2,...), then they are replaced with the strig
         matching the corresponding (sub)expression.  This only makes
         sense if the condition of the rule included a regular
         expression match (see section~\ref{s:fex-cond}). If it is not
         the case, results are undefined (probably a segmentation
         violation).
       \item If the bracket content is not a regex match variable,
         then it is interpreted as call to a custom feature
         function. It must have the format \verb#{functname(position)}#,
         where \verb#functname# is the name of the function as
         declared in the custom feature functions map (see section
         \ref{s:fex-custom}). The \verb#position# parameter is the relative
         position to the focus word, and is interpreted in the same 
         way than in the primitive features \verb#$w(position)#, 
         \verb#$t(position)#, etc., described above.\\
         E.g., the pattern:\\
         \verb#   {quoted(-1)}_{quoted(0)}#\\
         would generate a feature similar to that of the pattern:\\
         \verb#   t(-1)_t(0)#\\
         but using the result of the custom function \verb#quoted#
         instead of the PoS tag for the corresponding word.
      \end{enumerate}

  \end{itemize}

\subsection{Feature Rules Condition Syntax}
\label{s:fex-cond}

   Conditions control the applicability of a rule or a rule package. 

   A condition may be {\tt ALL} which is satisfied by any word.  A
   condition may be simple, or compund of several conditions, combined
   with the logical operadors {\tt AND} and {\tt OR}. The operators in
   a condition must be homogeneous (i.e. either all of them {\tt AND}
   or all of them {\tt OR}), mixed conditions are not allowed (note
   that an OR condition is equivalent to writing two rules that only
   differ on the condition).

   Single conditions consist of a word property, an operation, and an
   argument.  Available word properties are:
   \begin{itemize}
     \itemsep 0cm
   \item {\tt W}: Word form, original casing.
   \item {\tt w}: Word form, lowercased.
   \item {\tt l}: Lemma of the analysis selected by the tagger.
   \item {\tt t}: PoS tag (short version) of the analysis selected by the tagger.
   \item {\tt T}: PoS tag (full version) of the analysis selected by the tagger.
   \item {\tt pl}: List of all possible lemmas for the word.
   \item {\tt pt}: List of all possible short PoS tags for the word.
   \item {\tt pT}: List of all possible full PoS tags for the word.
   \item {\tt na}: Number of analysis of the word. 
   \item {\tt u.}$i$: $i$-th element of the word {\tt user} field.
   \end{itemize}

   Note that all word properties (including {\tt na}) are either
   strings or lists of strings.

   The available primitive operations to build single conditions are
   the following:
   \begin{enumerate}
     \itemsep 0cm
     \item {\tt <property> is <string>} :  String identity.
     \item {\tt <property> matches <regexp>} : Regex match. If the
       regex is parenthesized, (sub)expression matches {\tt \$0}, {\tt
         \$1}, {\tt \$2}, etc. are stored and can be used in the
       feature name pattern.
     \item {\tt <property-list> any\_in\_set <filename>}  (or simply {\tt in\_set}): True if any property in the list is found in the given file.
     \item {\tt <property-list> all\_in\_set <filename>} True if all properties in the list are found in the given file.
     \item {\tt <property-list> some\_in\_set <filename>} True if at least two properties in the list are found in the given file.
   \end{enumerate}

   Operators can be negated with the character {\tt !}. 
   E.g. {\tt !is}, {\tt !matches}, etc.

   For file operators expecting lists, the property may be a single string (list of one element).

   Some sample valid conditions:
   \begin{itemize}
     \itemsep 0cm
   \item[] \verb#t is NC#  ~~~true if the short version of the tag equals {\tt NC}.
   \item[] \verb#T matches ^NC.S..#  ~~~true if the long version of the tag matches the given regular expression.
   \item[] \verb#pl in\_set my/data/files/goodlemmas.dat# ~~~true if any possible lemma for the word is found in the given file.
   \item[] \verb#l !in\_set my/data/files/badlemmas.dat# ~~~true if selected lemma for the word is {\sl not} found in the given file.
   \item[] \verb#w matches ...$# ~~~Always true. Will set the match variable {\tt \$0} to the last three characters of the word, so it can be used in the feature name pattern.
   \end{itemize}

  
\subsection{Adding custom feature functions}
\label{s:fex-custom}
  Custom feature functions can be defined, and called from the
  \verb#.rgf# file enclosed in curly brackets (e.g.:
  \verb#{quoted(0)}#).  Calls to custom feature functions in the
  \verb#.rgf# file must have one integer parameter, indicating a
   word position relative to the target word.

  Actual code computing custom feature functions must be provided
  by the caller. A map \verb#std::map<std::wstring,const feature_function*>#
  needs to be given to the constructor, associating the custom function
  as used in the rule file with a \texttt{feature\_function} pointer.

  Custom feature functions must be classes derived from class
  \texttt{feature\_function}:
\begin{verbatim}
  class feature_function {  
    public: 
      virtual void extract (const sentence &, int, std::list<std::wstring> &) const =0;
      /// Destructor
      virtual ~feature_function() {};
  };
\end{verbatim}
 \noindent They must implement a method \texttt{extract} that receives the 
  sentence, the position of the target word, and a list of strings where the 
  resulting feature name (or names if more than one is to be generated) will be
  added.

  For instance, the example below generates the feature name \texttt{in\_quotes} when
  the target word is surrounded by words with the \texttt{Fe} PoS tag (which is 
  assigned to any quote symbol by the punctuation module).
\begin{verbatim}
  class fquoted : public feature_function {
    public:
      void extract (const sentence &sent, int i, std::list<std::wstring> &res) const {
        if ( (i>0 and sent[i-1].get_tag()==L"Fe") and
             (i<(int)sent.size()-1 and sent[i+1].get_tag()==L"Fe") )
          res.push_back(L"in_quotes");
      }
  };
\end{verbatim}

We can associate this function with the function name {\tt quoted} adding the pair to a map:
\begin{verbatim}
map<wstring,const feature_function*> myfunctions;
myfunctions.insert(make_pair(L"quoted", (feature_function *) new fquoted()));
\end{verbatim}

If we now create a \texttt{fex} object passing this map to the constructor, 
the created instance will call {\tt fquoted::extract} with the appropriate parameters
whenever a rule in the {\tt .rgf} file refers to e.g. \verb#{quote(0)}#.

 Note that there are three naming levels for custom feature functions:
  \begin{itemize}
  \item The name of the feature itself, which will be generated by the
    extractor and will appear in the feature vectors (\verb#in_quotes# in the
    above example).
  \item The name of the function that will be called from the
    extraction rules in the {\tt .rgf} file (\verb#quoted# in the
    above example).
  \item The name of the class derived from \verb#feature_function#
    that has a method \verb#extract# which actually computes the
    feature (\verb#fquoted# in the above example).
 \end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Using the library from your own application}

 The library may be used to develop your own NLP application (e.g. a
 machine translation system, an intelligent indexation module for a
 search engine, etc.)

  To achieve this goal you have to link your application to the
 library, and access it via the provided API.  Currently, the library
 provides a complete C++ API, a quite-complete Java API, and 
 half-complete perl and python APIs.

%..................................................
 \section{Basic Classes}

 This section briefs the basic C++ classes any application needs to
know. For detailed API definition, consult the technical documentation
in {\tt doc/html} and {\tt doc/latex} directories.

\subsection{Linguistic Data Classes}
\label{ssec-data}

  The different processing modules work on objects containing
  linguistic data (such as a word, a PoS tag, a sentence...).

   Your application must be aware of those classes in order to
  be able to provide to each processing module the right data,
  and to correctly interpret the module results.

   The Linguistic Data classes are defined in {\tt libfries} library. Refer 
  to the documentation in that library for the details on the classes.

   The linguistic classes are:
\begin{itemize}
\itemsep 0cm
\item {\tt analysis}: A tuple  \verb#<lemma, PoS tag, probability, sense list>#
\item {\tt word}:     A word form with a list of possible analysis.
\item {\tt sentence}: A list of words known to be a complete
  sentence. A sentence may have associated a {\tt parse\_tree} object and a {\tt dependency\_tree}.
\item {\tt parse\_tree}: An {\it n}-ary tree where each node contains
  either a non-terminal label, or --if the node is a leaf-- a pointer
  to the appropriate {\tt word} object in the sentence the tree
  belongs to.
\item {\tt dep\_tree}: An {\it n}-ary tree where each node contains a 
  reference to a node in a {\tt parse\_tree}. The structure of the {\tt dep\_tree}
  establishes syntactic dependency relationships between sentence constituents.
\end{itemize}

\subsection{Processing modules}

  The main processing classes in the library are:
\begin{itemize}
\itemsep 0cm
\item {\tt tokenizer}: Receives plain text and returns a list of {\tt word} objects.
\item {\tt splitter}: Receives a list of {\tt word} objects and
  returns a list of {\tt sentence} objects.
\item {\tt maco}: Receives a list of {\tt sentence} objects and
  morphologically annotates each {\tt word} object in the given
  sentences. Includes specific submodules (e.g, detection of date,
  number, multiwords, etc.) which can be activated at will.
\item {\tt tagger}: Receives a list of {\tt sentence} objects and
  disambiguates the PoS of each {\tt word} object in the given
  sentences.
\item {\tt nec}: Receives a list of {\tt sentence} objects and 
  modifies the tag for detected proper nouns to specify their class
  (e.g. person, place, organitzation, others).
\item {\tt ukb}:  Receives a list of {\tt sentence} objects 
  enriches the words with a ranked list of WordNet senses.
\item {\tt parser}: Receives a list of {\tt sentence} objects and
  associates to each of them a {\tt parse\_tree} object.
\item {\tt dependency}: Receives a list of parsed {\tt sentence}
 objects and associates to each of them a {\tt dep\_tree} object.
\item {\tt coref}: Receives a document (containing a list of parsed
  {\tt sentence} objects) and labels each noun phrase as belonging
  to a {\em coreference group}, if appropriate.
\end{itemize}

  You may create as many instances of each as you need. 
  Constructors for each of them receive the appropriate options
  (e.g. the name of a dictionary, hmm, or grammar file), so you can 
  create each instance with the required capabilities (for instance,
  a tagger for English and another for Spanish).

%..................................................
\section{Sample programs}
\label{sec-main}

 The directory {\tt src/main/simple\_examples} in the tarball contains 
 some example programs to illustrate how to call the library.

 See the README file in that directory for details on what does each of the 
 programs.

 The most complete program in that directory is {\tt sample.cc}, which
 is very similar to the program depicted below, which
 reads text from stdin, morphologically analyzes it, and processes 
 the obtained results. 

  Note that depending on the application, the input text
  could be obtained from a speech recongnition system, or from a 
  XML parser, or from any source suiting the application goals. 
  Similarly, the obtained analysis, instead of being output, could
  be used in a translation system, or sent to a dialogue control module, etc.

{\footnotesize
\begin{LSTverbatim}
int main() {
  wstring text;
  list<word> lw;
  list<sentence> ls;

  /// set locale to an UTF8 compatible locale
  util::init_locale(L"default");

  // if FreeLing was compiled with --enable-traces, you can activate
  // the required trace verbosity for the desired modules.
  //   traces::TraceLevel=4;
  //   traces::TraceModule=0xFFFFF;
  
  // ====== instantiate analyzers as needed =====

  wstring path=L"/usr/local/share/freeling/es/";
  tokenizer tk(path+L"tokenizer.dat"); 
  splitter sp(path+L"splitter.dat");
  
  // morphological analysis has a lot of options, and for simplicity they 
  // are packed up in a maco_options object. First, create the maco_options
  // object with default values.
  maco_options opt(L"es");  

  // then, set required options on/off  
  opt.UserMap=false;                 opt.AffixAnalysis = true;
  opt.MultiwordsDetection = true;    opt.NumbersDetection = true; 
  opt.PunctuationDetection = true;   opt.DatesDetection = true;
  opt.QuantitiesDetection = false;   opt.DictionarySearch = true; 
  opt.ProbabilityAssignment = true;  opt.NERecognition = true;   
  // alternatively, you can set active modules in a single call:
  // opt.set_active_modules(false,true,true,true,true,true,false,true,true,true);

  // and provide files for morphological submodules. Note that it is not necessary
  // to set opt.QuantitiesFile, since Quantities module was deactivated.
  opt.UserMapFile=L"";                 opt.LocutionsFile=path+L"locucions.dat";
  opt.AffixFile=path+L"afixos.dat";    opt.ProbabilityFile=path+L"probabilitats.dat"; 
  opt.DictionaryFile=path+L"dicc.src"; opt.NPdataFile=path+L"np.dat"; 
  opt.PunctuationFile=path+L"../common/punct.dat"; 
  // alternatively, you can set the files in a single call:
  // opt.set_data_files(L"", path+L"locucions.dat", L"", path+L"afixos.dat",
  //                   path+L"probabilitats.dat", opt.DictionaryFile=path+L"maco.db",
  //                   path+L"np.dat", path+L"../common/punct.dat");

  // create the analyzer with the just build set of maco_options
  maco morfo(opt); 
  // create a hmm tagger for spanish (with retokenization ability, and forced 
  // to choose only one tag per word)
  hmm_tagger tagger(L"es", path+L"tagger.dat", true, true); 
  // create chunker
  chart_parser parser(path+L"grammar-dep.dat");
  // create dependency parser 
  dep_txala dep(path+L"dep/dependences.dat", parser.get_start_symbol());
  
  // ====== Start text processing =====

  // get plain text input lines while not EOF.
  while (getline(wcin,text)) {
    
    // tokenize input line into a list of words
    lw=tk.tokenize(text);
    
    // accumulate list of words in splitter buffer, returning a list of sentences.
    // The resulting list of sentences may be empty if the splitter has still not 
    // enough evidence to decide that a complete sentence has been found. The list
    // may contain more than one sentence (since a single input line may consist 
    // of several complete sentences).
    ls=sp.split(lw, false);
    
    // perform  morphosyntactic analysis, disambiguation, and parsing
    morfo.analyze(ls);
    tagger.analyze(ls);
    parser.analyze(ls);
    dep.analyze(ls);

    // Do application-side processing with analysis results so far.
    ProcessResults(ls);
    
    // clear temporary lists;
    lw.clear(); ls.clear();    
  }
  
  // No more lines to read. Make sure the splitter doesn't retain anything  
  sp.split(lw, true, ls);   
 
  // analyze sentence(s) which might be lingering in the buffer, if any.
  morfo.analyze(ls);
  tagger.analyze(ls);
  parser.analyze(ls);
  dep.analyze(ls);

  // process remaining sentences, if any.
  ProcessResults(ls);
  
}
\end{LSTverbatim}
}



 The processing performed on the obtained results would obviously
 depend on the goal of the application (translation, indexation,
 etc.). In order to illustrate the structure of the linguistic data
 objects, a simple procedure is presented below, in which the processing
 consists of merely printing the results to stdout in XML format.

{\footnotesize
\begin{LSTverbatim}

void ProcessResults(const list<sentence> &ls) {
  
  list<sentence>::const_iterator is;
  word::const_iterator a;   //iterator over all analysis of a word
  sentence::const_iterator w;
  
  // for each sentence in list
  for (is=ls.begin(); is!=ls.end(); is++) {

    wcout<<L"<SENT>"<<endl;
    // for each word in sentence
    for (w=is->begin(); w!=is->end(); w++) {
      
      // print word form, with PoS and lemma chosen by the tagger
      wcout<<L"  <WORD form=\""<<w->get_form();
      wcout<<L"\" lemma=\""<<w->get_lemma();
      wcout<<L"\" pos=\""<<w->get_tag();
      wcout<<L"\">"<<endl;
      
      // for each possible analysis in word, output lemma, tag and probability
      for (a=w->analysis_begin(); a!=w->analysis_end(); ++a) {
	
	// print analysis info
	wcout<<L"    <ANALYSIS lemma=\""<<a->get_lemma();
	wcout<<L"\" pos=\""<<a->get_tag();
	wcout<<L"\" prob=\""<<a->get_prob();
	wcout<<L"\"/>"<<endl;
      }
      
      // close word XML tag after list of analysis
      wcout<<L"  </WORD>"<<endl;
    }
    
    // close sentence XML tag
    wcout<<L"</SENT>"<<endl;
  }
}  
\end{LSTverbatim}
}

 The above sample program may be found in {\tt /src/main/simple\_examples/sample.cc}
 in FreeLing tarball. The actual program also outputs tree structures resulting from 
 parsing, which is ommitted here for simplicity.

 Once you have compiled and installed FreeLing, you can build this
 sample program (or any other you may want to write) with the command:\\
 {\tt g++ -o sample sample.cc -lfreeling}

 Option {\tt -lfreeling} links with libfreeling library, which is the final result of the
 FreeLing compilation process. 
 Check the README file in the directory to learn more about compiling and using
 the sample programs.

   You may need to add some  {\tt -I} and/or  {\tt -L} options to the
 compilation command depending on where the headers and code of
 required libraries are located. For instance, if you installed some
 of the libraries in {\tt /usr/local/mylib} instead of the default 
place {\tt /usr/local}, you'll have to add the options 
 {\tt -I/usr/local/mylib/include -L/usr/local/mylib/lib} 
to the command above.

 Issuing {\tt make} in {\tt /src/main/simple\_examples} will compile 
 all sample programs in that directory. Make sure that the paths to 
 FreeLing installation directory in {\tt Makefile} are the right ones.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Using the sample main program to process corpora}
\label{cap-analyzer}

 The simplest way to use the FreeLing libraries is via the provided
 {\tt analyzer} sample main program, which allows the user to
 process an input text to obtain several linguistic processings.

  Since it is impossible to write a program that fits
  everyone's needs, the {\tt analyzer} program offers you almost all
  functionalities included in FreeLing, but if you want it to output
  more information, or do so in a specific format, or combine the
  modules in a different way, the right path to follow is building
  your own main program or adapting one of the existing, as described
  in section \ref{sec-main}

  The {\tt analyzer} program is usually called 
  with an option \verb#-f config-file#  (if ommitted, it will search
  for a file named {\tt analyzer.cfg} in the current directory).
  The given \verb#config-file# must be an absolute file name, or a 
  relative path to the current directory.

  You can use the default configuration files (located 
  at {\tt /usr/local/share/FreeLing/config} if you installed from tarball, 
  or at {\tt /usr/share/FreeLing/config} if you used a .deb package), or 
  either a config file that suits your needs. 
   Note that the default configuration
  files require the environment variable {\tt FREELINGSHARE} to be
  defined and to point to a directory with valid FreeLing data files
  (e.g. {\tt /usr/local/share/FreeLing}). 

   Environment variables are used for flexibility, but if you don't need
  them, you can replace {\tt FREELINGSHARE} in your configuration files
  with a static path.

  The {\tt analyzer} program provides also a server mode (use option
  {\tt --server}) which expects the input from a socket.  The program
  {\tt analyzer\_client} can be used to read input files and send
  requests to the server. The advantatge is that the server remains
  loaded after analyzing each client's request, thus reducing the
  start-up overhead if many small files have to be processed. Client
  and server communicate via sockets.  The client-server approach is
  also a good strategy to call FreeLing from a language or platform
  for which no API is provided: Just launch a server and use you preferred 
  language to program a client that behaves like \verb#analyzer_client#. 

  The {\tt analyze} (no final {\tt r}) script described below handles
  all these default paths and variables and makes everything easier if
  you want to use the defaults.

 \section{The easy way: Using the {\tt analyze} script}

  To ease the invocation of the program, a script named {\tt analyze}
  (no final {\tt r}) is provided. This is script is able to locate
  default configuration files, define library search paths, and 
  handle whether you want the client-server or the straight version.

 The sample main program is called with the command:
\begin{verbatim}
 analyze [-f config-file] [options]
\end{verbatim}

  If \verb#-f config-file# is not specified, a file named 
 {\tt analyzer.cfg} is searched in the current working 
 directory. 

  If \verb#-f config-file# is specified but not found in the current
  directory, it will be searched in FreeLing installation directory
  ({\tt /usr/local/share/FreeLing/config} if you installed from
  source, and \verb#/usr/share/FreeLing/config# if you used a binary {\tt
    .deb} package).

  Extra options may be specified in the command line to override any
  settings in \verb#config-file#. See section \ref{ss-options} for
  details.

  Server mode is triggered by option \verb#--server#. See section
  \ref{ss-options} for details.

\subsection{Stand-alone mode}

 The default mode will launch a stand-alone analyzer, which will load
 the configuration, read input from stdin, write results to stdout,
 and exit.\\ E.g.:
\begin{verbatim}
 analyze -f en.cfg  <myinput  >myoutput
\end{verbatim}

  When the input file ends, the analyzer will stop and it will have to be
  reloaded again to process a new file.

 The above command is equivalent to:
\begin{enumerate}
\itemsep -0.12cm
\item Define {\tt FREELINGSHARE} variable to point to a directory with
  FreeLing data\\ (e.g. {\tt /usr/local/share/freeling}).
\item Make sure the \verb#LD_LIBRARY_PATH# variable contains the
  directory where FreeLing libraries are installed (e.g. {\tt
    /usr/local/lib}).
\item Locate the given configuration file in the current directory or in
 the FreeLing installation directories (e.g. {\tt /usr/local/share/freeling/config}).
\item Run the {\tt analyzer} (final {\tt r}) program:\\ 
      \verb#analyzer -f /usr/local/share/freeling/config/en.cfg <myinput >myoutput#
\end{enumerate}

\subsection{Client/server mode}

  If \verb#--server# and \verb#--port# options are specified, a server 
  will be launched which starts listening for incoming
  requests.\\ E.g.:
\begin{verbatim}
 analyze -f en.cfg  --server --port 50005 &
\end{verbatim}
 To launch the server, the script follows the same steps described in
 previous section, but with the options \verb#--server --port 50005#
 passed to the final call to the \verb#analyzer# executable.
\medskip

\noindent Once the server is launched, clients can request analysis to
the server, with:
\begin{verbatim}
 analyzer_client 50005  <myinput  >myoutput
 analyzer_client localhost:50005  <myinput  >myoutput
\end{verbatim}
\noindent or, from a remote machine:
\begin{verbatim}
 analyzer_client my.server.com:50005  <myinput  >myoutput
 analyzer_client 192.168.10.11:50005  <myinput  >myoutput
\end{verbatim}

 The server will fork a new process to attend each new client,
 so you can have many clients being served at the same time.

 You can control the maximum amount of clients being attended
 simutaneously (in order to prevent a flood in your server) with the
 option \verb#--workers#.  You can control the size of the queue of
 pending clients with option \verb#--queue#. Clients trying to connect
 when the queue is full will receive a connection error.
 See section \ref{ss-options} for details on these options.
\medskip

\section{Using a threaded analyzer}

 If \verb#libboost_thread# is installed, the installation process will
 build the program \verb#threaded_analyzer#. This program behaves 
 like \verb#analyzer#, and has almost the same options. 

 The program \verb#threaded_analyzer# launches each processor in a
 separate thread, so while one sentece is being parsed, the next is
 being tagged, and the following one is running through the
 morphological analyzer.
 In this way, the multi-core capabilities of the host are better
 exploited and the analyzer runs faster.

 Although it is intended mainly as an example for developers wanting
 to build their own threaded applications, this program can also be 
 used to analyze texts, in the same way than \verb#analyzer#.

 Nevertheless, notice that this example program does {\sl not} include
 modules that are not token- or sentence-oriented, namely, language 
 identification and coreference resolution.


%\begin{enumerate}
%  \item Start the server:
%\begin{verbatim}
% analyzer_server_socket port_num [-f config-file] [options] &
%\end{verbatim}
%
%  The parameter {\tt port\_num} is the port where the server will be
%  listening for requests. 
%
%  The possible {\tt options} are the same
%  than for the {\tt analyze} script described above.
%
%   Note that defalt configuration files are located at {\tt
%     /usr/local/share/FreeLing/config}. Note also that those files
%   require the environment variable {\tt FREELINGSHARE} to be defined
%   and to point to a directory with valid FreeLing data files
%   (e.g. {\tt /usr/local/share/FreeLing}). The {\tt analyze} script
%   described above handles these paths and variables, but if you are
%   calling the {\tt analyzer\_server\_socket} directly, you need to
%   handle them yourself.
%
% \item  Once the server is started, you can start clients that send requests to it:
%\begin{verbatim}
%analyzer_client_socket host port_num [--utf] < myinput > myoutput
%\end{verbatim}
%
%  The parameter {\tt host} indicates the name or IP of the host where
%  the server is running ({\tt localhost} if it is the same machine
%  than the client) and {\tt port\_num} is the port where the server
%  is listening for requests.
%
%  The flag \verb#--utf# must be set if the input is in utf8. 
%
% \end{enumerate}

%..................................................
\section{Usage example}

   Assuming we have the following input file {\tt mytext.txt}:
{\small {\tt 
\begin{center}
\begin{tabular}{p{6cm}}
El gato come pescado. Pero a Don Jaime no le gustan los gatos.
\end{tabular}
\end{center}
}}

\noindent we could issue the command:
\begin{verbatim}
  analyze -f myconfig.cfg <mytext.txt >mytext.mrf
\end{verbatim}
   Assuming that {\tt myconfig.cfg} is the file presented in
   section~\ref{ss-config}. Given the options there, the produced
   output would correspond to {\tt morfo} format (i.e. morphological
   analysis but no PoS tagging). The expected results are:
{\small
\begin{verbatim}
 El el DA0MS0 1 
 gato gato NCMS000 1 
 come comer VMIP3S0 0.75 comer VMM02S0 0.25 
 pescado pescado NCMS000 0.833333 pescar VMP00SM 0.166667 
 . . Fp 1 

 Pero pero CC 0.99878 pero NCMS000 0.00121951 Pero NP00000 0.00121951 
 a a NCFS000 0.0054008 a SPS00 0.994599 
 Don_Jaime Don_Jaime NP00000 1 
 no no NCMS000 0.00231911 no RN 0.997681 
 le él PP3CSD00 1 
 gustan gustar VMIP3P0 1 
 los el DA0MP0 0.975719 lo NCMP000 0.00019425 él PP3MPA00 0.024087 
 gatos gato NCMP000 1 
 . . Fp 1 

\end{verbatim}
}

\noindent If we also wanted PoS tagging, we could have issued the command:
\begin{verbatim}
  analyze -f myconfig.cfg --outf tagged <mytext.txt >mytext.tag
\end{verbatim}
\noindent  to obtain the tagged output:
{\small 
\begin{verbatim}
 El el DA0MS0
 gato gato NCMS000
 come comer VMIP3S0
 pescado pescado NCMS000
 . . Fp

 Pero pero CC
 a a SPS00
 Don_Jaime Don_Jaime NP00000
 no no RN
 le él PP3CSD00
 gustan gustar VMIP3P0
 los el DA0MP0
 gatos gato NCMP000
 . . Fp

\end{verbatim}
}

\noindent We can also ask for the senses of the tagged words:
\begin{verbatim}
  analyze -f myconfig.cfg --outf sense --sense all  <mytext.txt >mytext.sen
\end{verbatim}
\noindent obtaining the output:
{\small 
\begin{verbatim}
 El el DA0MS0
 gato gato NCMS000 01630731:07221232:01631653
 come comer VMIP3S0 00794578:00793267
 pescado pescado NCMS000 05810856:02006311
 . . Fp

 Pero pero CC
 a a SPS00
 Don_Jaime Don_Jaime NP00000
 no no RN
 le él PP3CSD00
 gustan gustar VMIP3P0 01244897:01213391:01241953
 los el DA0MP0
 gatos gato NCMP000 01630731:07221232:01631653
 . . Fp

\end{verbatim}
}

   Alternatively, if we don't want to repeat the first steps that we
   had already performed, we could use the output of the morphological
   analyzer as input to the tagger:
\begin{verbatim}
analyze -f myconfig.cfg --inpf morfo --outf tagged <mytext.mrf >mytext.tag
\end{verbatim}
 
   See options InputFormat and OutputFormat in
   section~\ref{ss-options} for details on which are valid input and
   output formats.

%..................................................
\section{Configuration File and Command Line Options}

   Almost all options may be specified either in the configuration
   file or in the command line, having the later precedence over the
   former.

   Valid options are presented in section~\ref{ss-options}, both in
   their command-line and configuration file notations. Configuration
   files follow the usual linux standards. A sample file may be seen
   in section~\ref{ss-config}.

  The FreeLing package includes default configuration files. They can
  be found at the directory {\tt share/FreeLing/config} under the FreeLing
  installation directory ({\tt /usr/local}  if you installed from source, and
   \verb#/usr/share/FreeLing# if you used a binary {\tt .deb} package).
   The {\tt analyze} script will try to locate the configuration file in
  that directory if it is not found in the current working directory.


%.......................................................
\subsection{Valid options}
\label{ss-options}

   This section presents the options that can be given to the {\tt
     analyzer} program (and thus, also to the {\tt analyzer\_server}
   program and to the {\tt analyze} script). All options can be
   written in the configuration file as well as in the command line.
   The later has always precedence over the former.

\begin{itemize}

\item {\bf Help}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-h#, \verb#--help#,  \verb#--help-cf#    &  {\tt N/A}   \\ \hline
\end{tabular}

 Prints to stdout a help screen with valid options and exits.

 \noindent \verb#--help# provides information about command line options.

 \noindent \verb#--help-cf# provides information about configuration file options. 

\item {\bf Version number}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-v#, \verb#--version#   &  {\tt N/A}   \\ \hline
\end{tabular}

 Prints the version number of currently installed FreeLing library.

\item {\bf Configuration file}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-f <filename>#  &  {\tt N/A}        \\ \hline 
\end{tabular}

 Specify configuration file to use (default: analyzer.cfg).



\item {\bf Server mode}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--server#  &  \verb#ServerMode=(yes|y|on|no|n|off)#        \\ \hline 
\end{tabular}

 Activate server mode. 
 Requires that option \verb#--port# is also provided.

 Default value is \verb#off#.

\item {\bf Server Port Number}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-p <int>#, \verb#--port <int>#   & \verb#ServerPort=<int>#  \\ \hline
\end{tabular}

  Specify port where server will be listening for requests.  This
  option must be specified if server mode is active, and it is ignored
  if server mode is off.

\item {\bf Maximum Number of Server Workers}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-w <int>#, \verb#--workers <int>#   & \verb#ServerMaxWorkers=<int>#  \\ \hline
\end{tabular}

  Specify maximum number of active workers that the server will launch.
  Each worker attends a client, so this is the maximum number of
  clients that are simultaneously attended.
  This option is ignored if server mode is off.

  Default vaule is 5. Note that a high number of simultaneous workers will 
  result in forking that many processes, which may overload the CPU and
  memory of your machine resulting in a system collapse.

  When the maximum number of workers is reached, new incoming requests
  are queued until a worker finishes.

  

\item {\bf Maximum Size of Server Queue}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-q <int>#, \verb#--queue <int>#   & \verb#ServerQueueSize=<int>#  \\ \hline
\end{tabular}

  Specify maximum number of pending clients that the server socket can hold.
  This option is ignored if server mode is off.
  
  Pending clients are requests waiting for a worker to be available. 
  They are queued in the operating system socket queue.

  Default value is 32. Note that the operating system has an internal 
  limit for the socket queue size (e.g. modern linux kernels set it to 128). 
  If the given value is higher than the operating system limit, it will
  be ignored.

  When the pending queue is full, new incoming requests get a
  connection error.

\item {\bf Trace Level}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-l <int>#, \verb#--tlevel <int>#   & \verb#TraceLevel=<int>#  \\ \hline
\end{tabular}

 Set the trace level (0 = no trace, higher values = more trace), for
 debugging purposes.

 This will work only if the library was compiled with tracing information, 
 using {\tt ./configure --enable-traces}.
 Note that the code with tracing information is slower than the code 
 compiled without it, even when traces are not active.

\item {\bf Trace Module}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-m <mask>#, \verb#--tmod <mask>#  & \verb#TraceModule=<mask>#  \\ \hline
\end{tabular}

  Specify modules to trace. Each module is identified with an hexadecimal flag.
 All flags may be OR-ed to specificy the set of modules to be traced.

 Valid masks are defined in file \verb#src/include/freeling/morfo/traces.h#, 
and are the following:

\begin{tabular}{ll}
   {\bf Module}           & {\bf Mask}  \\ \hline
Splitter               & 0x00000001 \\
Tokenizer              & 0x00000002 \\
Morphological analyzer & 0x00000004 \\
Options management     & 0x00000008 \\
Number detection       & 0x00000010 \\
Date identification         & 0x00000020 \\
Punctuation detection       & 0x00000040 \\
Dictionary search           & 0x00000080 \\
Affixation rules            & 0x00000100 \\
Multiword detection         & 0x00000200 \\
Named entity detection      & 0x00000400 \\
Probability assignment      & 0x00000800 \\
Quantities detection        & 0x00001000 \\
Named entity classification & 0x00002000 \\
Automata (abstract)         & 0x00004000 \\
Sense annotation            & 0x00010000 \\
Chart parser                & 0x00020000 \\
Parser grammar              & 0x00040000 \\
Dependency parser           & 0x00080000 \\
Correference resolution     & 0x00100000 \\
Utilities                   & 0x00200000 \\ 
Word sense disambiguation   & 0x00400000 \\ 
Ortographic correction      & 0x00800000 \\ 
Database storage            & 0x01000000 \\ 
Feature extraction          & 0x02000000 \\ 
Language identifier         & 0x04000000 \\ 
Omlet                       & 0x08000000 \\
Phonetics                   & 0x10000000 \\
 \hline
\end{tabular}

\item {\bf Language of input text}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--lang <language>#  & \verb#Lang=<language>#    \\ \hline
\end{tabular}

   Code for language of input text. Though it is not required, the
   convention is to use two-letter ISO codes (as: Asturian, es:
   Spanish, ca: Catalan, en: English, cy: Welsh, it: Italian, gl:
   Galician, pt: Portuguese, ru: Russian, old-es: old Spanish).

   Other languages may be added to the library. See
   chapter~\ref{c-adding-lang} for details.

\item {\bf Locale}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--locale <locale>#  & \verb#Locale=<locale>#    \\ \hline
\end{tabular}

  Locale to be used to interpret both input text and data files.
  Usually, the value will match the locale of the \verb#Lang# option
  (e.g. \verb#es_ES.utf8# for spanish, \verb#ca_ES.utf8# for
  Catalan, etc.).  The values \verb#default# (stands for
  \verb#en_US.utf8#) and \verb#system# (stands for currently active
  system locale) may also be used.

\item {\bf Splitter Buffer Flushing}

\begin{tabular}{|l|l|}
Command line                      & Configuration file   \\ \hline
\verb#--flush#, \verb#--noflush#  & \verb#AlwaysFlush=(yes|y|on|no|n|off)#   \\ \hline
\end{tabular}

   When this option is inactive (most usual choice) sentence splitter
   buffers lines until a sentence marker is found. Then, it outputs a
   complete sentence. 

   When this option is active, the splitter never buffers any token,
   and considers each newline as a sentence end, thus processing each
   line as an independent sentence.

\item {\bf Input Format}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--inpf <string>#  & \verb#InputFormat=<string># \\ \hline
\end{tabular}

  Format of input data (plain, token, splitted, morfo, tagged, sense).
 \begin{itemize}
  \item plain: plain text.
  \item token: tokenized text (one token per line).
  \item splitted : tokenized and sentence-splitted text (one token per line, sentences separated with one blank line).
  \item morfo: tokenized, sentence-splitted, and morphologically analyzed text. 
    One token per line, sentences separated with one blank line.\\
    Each line has the format:     {\tt word (lemma tag prob)$^+$ }
  \item tagged:  tokenized, sentence-splitted, morphologically analyzed, and PoS-tagged text. 
    One token per line, sentences separated with one blank line.\\
    Each line has the format:  {\tt word lemma tag}.
  \item sense:  tokenized, sentence-splitted, morphologically
    analyzed, PoS-tagged text, and sense-annotated. 
    One token per line, sentences separated with one blank line.\\
    Each line has the format: 
    {\tt word (lemma tag prob sense$_1$:\ldots:sense$_N$)$^+$}
 \end{itemize}

\item {\bf Output Format}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--outf <string>#  & \verb#OutputFormat=<string># \\ \hline
\end{tabular}

  Format of output data (token, splitted, morfo, tagged, shallow, parsed, dep).
 \begin{itemize}
  \item token: tokenized text (one token per line).
  \item splitted : tokenized and sentence-splitted text (one token per
    line, sentences separated with one blank line).
  \item morfo: tokenized, sentence-splitted, and morphologically analyzed text. 
    One token per line, sentences separated with one blank line.\\
    Each line has the format:  \\
    {\tt word (lemma tag prob)$^+$ }\\
    or (if sense tagging has been activated):\\
    {\tt word (lemma tag prob sense$_1$:\ldots:sense$_N$)$^+$}
  \item tagged: tokenized, sentence-splitted, morphologically
    analyzed, and PoS-tagged text.  One token per line, sentences
    separated with one blank line.\\ Each line has the format: {\tt
      word lemma tag prob}\\ or, if sense tagging has been activated:
    {\tt word lemma tag prob sense$_1$:\ldots:sense$_N$}
  \item shallow: tokenized, sentence-splitted, morphologically
    analyzed, PoS-tagged, optionally sense--annotated, and
    shallow-parsed text, as output by the \verb#chart_parser# module.
  \item parsed: tokenized, sentence-splitted, morphologically
    analyzed, PoS-tagged, optionally sense--annotated, and full-parsed
    text, as output by the first stage (tree completion) of the
    dependency parser.
  \item dep: tokenized, sentence-splitted, morphologically analyzed,
    PoS-tagged, optionally sense--annotated, and dependency-parsed
    text, as output by the second stage (transformation to
    dependencies and function labelling) of the dependency parser.
 \end{itemize}


\item {\bf Produce training output format}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--train# & N/A  \\ \hline
\end{tabular}

  When this option (only available at command line) is specified,
  \verb#OutputFormat# is forced to \verb#tagged# and results are
  printed in the format:
\begin{verbatim}
 word lemma tag # lemma1 tag1 lemma2 tag2 ...
\end{verbatim}
that is, one word per line, with the selected lemma and tag as fields
2 and 3, a separator (\verb/#/) and a list of all possible pairs
lemma-tag for the word (including the selected one).

 This format is expected by the training scripts. Thus, this option can
 be used to annotate a corpus, correct the output manually, and use it
 to retrain the taggers with the script {\tt src/utilities/train-tagger/bin/TRAIN.sh}
 provided in FreeLing package. See {\tt src/utilities/train-tagger/README} for
 details about how to use it.
  
\item {\bf Language Identification Configuration File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-I <filename>#, \verb#--fidn <filename># & \verb#N/A#  \\ \hline
\end{tabular}

  Configuration file for language identifier. See section~\ref{lang-ident} for details.


\item {\bf Tokenizer File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--abrev <filename># & \verb#TokenizerFile=<filename>#  \\ \hline
\end{tabular}

  File of tokenization rules. See section~\ref{file-tok} for details.

\item {\bf Splitter File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--fsplit <filename># & \verb#SplitterFile=<filename>#  \\ \hline
\end{tabular}

  File of splitter options rules. See section~\ref{file-split} for details.

\item {\bf Affix Analysis}

\begin{tabular}{|l|l|}
Command line                     & Configuration file   \\ \hline
\verb#--afx#, \verb#--noafx#   & \verb#AffixAnalysis=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

  Whether to perform affix analysis on unknown words. 
  Affix analysis applies a set of  affixation rules to the word to check whether it is a derived form of a known word.

\item {\bf Affixation Rules File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-S <filename>#, \verb#--fafx <filename>#  & \verb#AffixFile=<filename># \\ \hline  
\end{tabular}

   Affix rules file. See section \ref{file-suf} for details.

\item {\bf User Map}

\begin{tabular}{|l|l|}
Command line                     & Configuration file   \\ \hline
\verb#--usr#, \verb#--nousr#   & \verb#UserMap=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

  Whether to apply or not a file of customized word-tag mappings.

\item {\bf User Map File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-M <filename>#, \verb#--fmap <filename>#  & \verb#UserMapFile=<filename># \\ \hline  
\end{tabular}

   User Map file to be used. See section \ref{file-usermap} for details.


\item {\bf Multiword Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--loc#, \verb#--noloc#     & \verb#MultiwordsDetection=(yes|y|on|no|n|off)#  \\ \hline 
\end{tabular}

 Whether to perform multiword detection. This option requires that
 a multiword file is provided.

\item {\bf Multiword File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-L <filename>#, \verb#--floc <filename># & \verb#LocutionsFile=<filename>#  \\ \hline
\end{tabular}

  Multiword definition file. See section \ref{file-mw} for details.

\item {\bf Number Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--numb#, \verb#--nonumb#   & \verb#NumbersDetection=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

 Whether to perform nummerical expression detection. Deactivating this
 feature will affect the behaviour of date/time and ratio/currency
 detection modules.

\item {\bf Decimal Point}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--dec <string>#   & \verb#DecimalPoint=<string>#\\ \hline 
\end{tabular}

   Specify decimal point character for the number detection module
   (for instance, in English is a dot, but in Spanish is a comma).

\item {\bf Thousand Point} 

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--thou <string>#  & \verb#ThousandPoint=<string>#  \\ \hline
\end{tabular}

   Specify thousand point character for the number detection module
   (for instance, in English is a comma, but in Spanish is a dot).

\item {\bf Punctuation Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--punt#, \verb#--nopunt#    & \verb#PunctuationDetection=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

 Whether to assign PoS tag to punctuation signs.

\item {\bf Punctuation Detection File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-F <filename>#, \verb#--fpunct <filename># & \verb#PunctuationFile=<filename># \\ \hline
\end{tabular}

  Punctuation symbols file.  See section \ref{file-punt} for details.

\item {\bf Date Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--date#, \verb#--nodate#    & \verb#DatesDetection=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

  Whether to perform date and time expression detection.

\item {\bf Quantities Detection}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--quant#, \verb#--noquant#   & \verb#QuantitiesDetection=(yes|y|on|no|n|off)# \\ \hline
\end{tabular}

 Whether to perform currency amounts, physical magnitudes, and ratio
 detection.

\item {\bf Quantity Recognition File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-Q <filename>#, \verb#--fqty <filename>#  & \verb#QuantitiesFile=<filename>#  \\ \hline
\end{tabular}

 Quantitiy recognition configuration file. See section
 \ref{file-quant} for details.

\item {\bf Dictionary Search}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--dict#, \verb#--nodict#    & \verb#DictionarySearch=(yes|y|on|no|n|off)#    \\ \hline
\end{tabular}

 Whether to search word forms in dictionary. Deactivating this feature
 also deactivates AffixAnalysis option.

\item {\bf Dictionary File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-D <filename>#, \verb#--fdict <filename># & \verb#DictionaryFile=<filename>#  \\ \hline
\end{tabular}

 Dictionary database. 
 See section \ref{file-dict} and chapter \ref{c-adding-lang} for details.

\item {\bf Probability Assignment}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--prob#, \verb#--noprob#    & \verb#ProbabilityAssignment=(yes|y|on|no|n|off)#  \\ \hline
\end{tabular}

 Whether to compute a lexical probability for each tag of each word. 
 Deactivating this feature will affect the behaviour of the PoS tagger.

\item {\bf Lexical Probabilities File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-P <filename>#, \verb#--fprob <filename># & \verb#ProbabilityFile=<filename># \\ \hline
\end{tabular}

 Lexical probabilities file. The probabilities in this file are used
 to compute the most likely tag for a word, as well to estimate the
 likely tags for unknown words. See section \ref{file-prob} for details.

\item {\bf Unknown Words Probability Threshold.}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-e <float>#, \verb#--thres <float>#   & \verb#ProbabilityThreshold=<float>#   \\ \hline   
\end{tabular}

 Threshold that must be reached by the probability of a tag given the
 suffix of an unknown word in order to be included in the list of
 possible tags for that word. Default is zero (all tags are included
 in the list). A non--zero value (e.g. 0.0001, 0.001) is recommended.


\item {\bf Named Entity Recognition}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--ner [bio|basic|none]#   & \verb#NERecognition=(bio|basic|none)#    \\ \hline
\end{tabular}

  Whether to perform NE recognition and which recognizer to use:
  ``bio'' for AdaBoost based NER, ``basic'' for a simple heuristic NE
  recognizer and ``none'' to perform no NE recognition . Deactivating
  this feature will cause the NE Classification module to have no
  effect.

\item {\bf Named Entity Recognition}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--ner#, \verb#--noner#     & \verb#NERecognition=(yes|y|on|no|n|off)#    \\ \hline
\end{tabular}

   Whether to perform NE recognition.

\item {\bf Named Entity Recognizer File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-N <filename>#, \verb#--fnp <filename>#   & \verb#NPDataFile=<filename># \\ \hline
\end{tabular}

  Configuration data file for NE recognizer.

  See section \ref{file-ner} for details.


\item {\bf Named Entity Classification}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--nec#, \verb#--nonec#     & \verb#NEClassification=(yes|y|on|no|n|off)#    \\ \hline
\end{tabular}

   Whether to perform NE classification.


\item {\bf Named Entity Classifier File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--fnec <filename>#   & \verb#NECFile=<filename>#  \\ \hline
\end{tabular}

  Configuration file for Named Entity Classifier module

  See section \ref{file-nec} for details.


\item {\bf Phonetic Encoding}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--phon#, \verb#--nophon#     & \verb#Phonetics=(yes|y|on|no|n|off)#    \\ \hline
\end{tabular}

   Whether to add phonetic transcription to each word.

\item {\bf Phonetic Encoder File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--fphon <filename>#   & \verb#PhoneticsFile=<filename>#  \\ \hline
\end{tabular}

  Configuration file for phonetic encoding module

  See section \ref{file-phon} for details.


\item {\bf Sense Annotation}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-s <string>#, \verb#--sense <string>#    & \verb#SenseAnnotation=<string>#    \\ \hline
\end{tabular}

   Kind of sense annotation to perform
 \begin{itemize}
  \item no, none: Deactivate sense annotation.
  \item all: annotate with all possible senses in sense dictionary.
  \item mfs: annotate with most frequent sense.
  \item ukb: annotate all senses, ranked by UKB algorithm.
 \end{itemize}

   Whether to perform sense anotation.

  If active, the PoS tag
   selected by the tagger for each word is enriched with a list of all
   its possible WN synsets. The sense repository used depends on the
   options  ``Sense Annotation Configuration File'' and ``UKB Word 
   Sense Disambiguator Configuration File'' described below.

\item {\bf Sense Annotation Configuration File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-W <filename>#, \verb#--fsense <filename>#   & \verb#SenseConfigFile=<filename>#  \\ \hline
\end{tabular}

  Word sense annotator configuration file.
  See section \ref{mod-sense} for details.

\item {\bf UKB Word Sense Disambiguator Configuration File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-U <filename>#, \verb#--fukb <filename>#   & \verb#UKBConfigFile=<filename>#  \\ \hline
\end{tabular}

  UKB configuration file.
  See section \ref{mod-ukb} for details.

\item {\bf Tagger algorithm}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-t <string>#, \verb#--tag <string>#   & \verb#Tagger=<string>#  \\ \hline
\end{tabular}

   Algorithm to use for PoS tagging
 \begin{itemize}
  \item hmm:  Hidden Markov Model tagger, based on \cite{brants00}.
  \item relax: Relaxation Labelling tagger, based on \cite{padro98a}.
 \end{itemize}

\item {\bf HMM Tagger configuration File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-H <filename>#, \verb#--hmm <filename>#   & \verb#TaggerHMMFile=<filename>#  \\ \hline
\end{tabular}

  Parameters file for HMM tagger. 
  See section \ref{file-hmm} for details.

\item {\bf Relaxation labelling tagger constraints file}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-R <filename>#, \verb#--rlx <filename>#    & \verb#TaggerRelaxFile=<filename>#  \\ \hline
\end{tabular}

   File containing the constraints to apply to solve the PoS tagging.
   See section \ref{file-relax} for details.

\item {\bf Relaxation labelling tagger iteration limit}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-i <int>#, \verb#--iter <int>#   & \verb#TaggerRelaxMaxIter=<int>#  \\ \hline
\end{tabular}

   Maximum numbers of iterations to perform in case relaxation does
   not converge.

\item {\bf Relaxation labelling tagger scale factor}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-r <float>#, \verb#--sf <float>#   & \verb#TaggerRelaxScaleFactor=<float>#  \\ \hline
\end{tabular}

   Scale factor to normalize supports inside RL algorithm. It is
   comparable to the step lenght in a hill-climbing algorithm: The
   larger scale factor, the smaller step.

\item {\bf Relaxation labelling tagger epsilon value}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--eps <float>#   & \verb#TaggerRelaxEpsilon=<float>#  \\ \hline
\end{tabular}

   Real value used to determine when a relaxation labelling iteration
   has produced no significant changes. The algorithm stops when no
   weight has changed above the specified epsilon.


\item {\bf Retokenize contractions in dictionary}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--rtkcon#, \verb#--nortkcon#    & \verb#RetokContractions=(yes|y|on|no|n|off)#    \\ \hline
\end{tabular}

   Specifies whether the dictionary must retokenize contractions when found, 
   or leave the decision to the \verb#TaggerRetokenize# option.

   Note that if this option is active, contractions will be
   retokenized even if the \verb#TaggerRetokenize# option is not
   active.  If this option is not active, contractions will be
   retokenized depending on the value of the \verb#TaggerRetokenize#
   option.


\item {\bf Retokenize after tagging}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--rtk#, \verb#--nortk#    & \verb#TaggerRetokenize=(yes|y|on|no|n|off)#   \\ \hline
\end{tabular}

   Determine whether the tagger must perform retokenization after the
   appropriate analysis has been selected for each word.  This is
   closely related to affix analysis and PoS taggers, see sections
   \ref{file-suf} and \ref{sec-pos} for details.


\item {\bf Force the selection of one unique tag}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--force <string>#     & \verb#TaggerForceSelect=(none,tagger,retok)#    \\ \hline
\end{tabular}

   Determine whether the tagger must be forced to (probably randomly) make a unique choice and when.
   \begin{itemize}
    \item   {\tt none}: Do not force the tagger, allow ambiguous output.
    \item {\tt tagger}: Force the tagger to choose before
      retokenization (i.e. if retokenization introduces any ambiguity,
      it will be present in the final output).
    \item {\tt retok}: Force the tagger to choose after retokenization
      (no remaining ambiguity)
   \end{itemize}

 See \ref{sec-pos} for more information.

\item {\bf Chart Parser Grammar File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-G <filename>#, \verb#--grammar <filename>#   & \verb#GrammarFile=<filename>#  \\ \hline
\end{tabular}

   This file contains a CFG grammar for the chart parser, and some
  directives to control which chart edges are selected to build the
  final tree.
   See section \ref{file-cfg} for details.


\item {\bf Dependency Parser Rule File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-T <filename>#, \verb#--txala <filename>#   & \verb#DepTxalaFile==<filename>#  \\ \hline
\end{tabular}

  Rules to be used to perform dependency analysis.
  See section \ref{file-dep} for details.

\item {\bf Coreference Resolution}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#--coref#, \verb#--nocoref#     & \verb#CoreferenceResolution=(yes|y|on|no|n|off)#    \\ \hline
\end{tabular}

   Whether to perform coreference resolution.


\item {\bf Named Entity Classifier File}

\begin{tabular}{|l|l|}
Command line       & Configuration file   \\ \hline
\verb#-C <filename>#, \verb#--fcorf <filename>#   & \verb#CorefFile=<filename>#  \\ \hline
\end{tabular}

  Configuration file for coreference resolution module.

  See section \ref{mod-coref} for details.

\end{itemize}


\newpage


\subsection{Sample Configuration File}
\label{ss-config}

  A sample configuration file follows.  This is only a sample, and
  probably won't work if you use it as is.  You can start using
  freeling with the default configuration files which --after
  installation-- are located in {\tt /usr/local/share/FreeLing/config}
  (note than prefix {\tt /usr/local} may differ if you specified an
  alternative location when installing FreeLing. If you
  installed from a binary {\tt .deb} package), it will be 
  at {\tt /usr/share/FreeLing/config}.

  You can use those files as a starting point to customize 
  one configuration file to suit your needs.

  Note that file paths in the sample configuration file contain
  \verb#$FREELINGSHARE#, which is supposed to be an environment
  variable.  If this variable is not defined, the analyzer will
  abort, complaining about not finding the files.

  If you use the {\tt analyze} script, it will define the variable for
  you as {\tt /usr/local/share/Freeling} (or the right installation
  path), unless you define it to point somewhere else.

  You can also adjust your configuration files to use normal paths for
  the files (either relative or absolute) instead of using variables.

{\small
\begin{verbatim}
##
#### default configuration file for Spanish analyzer
##

TraceLevel=3
TraceModule=0x0000

## Options to control the applied modules. The input may be partially
## processed, or not a full analysis may me wanted. The specific
## formats are a choice of the main program using the library, as well
## as the responsability of calling only the required modules.  
## Valid input formats are: plain, token, splitted, morfo, tagged, sense.
## Valid output formats are: : plain, token, splitted, morfo, tagged,
## shallow, parsed, dep.
InputFormat=plain
OutputFormat=tagged

# consider each newline as a sentence end
AlwaysFlush=no

#### Tokenizer options
TokenizerFile=$FREELINGSHARE/es/tokenizer.dat

#### Splitter options
SplitterFile=$FREELINGSHARE/es/splitter.dat

#### Morfo options
AffixAnalysis=yes
MultiwordsDetection=yes
NumbersDetection=yes
PunctuationDetection=yes
DatesDetection=yes
QuantitiesDetection=yes
DictionarySearch=yes
ProbabilityAssignment=yes
OrthographicCorrection=no
DecimalPoint=,
ThousandPoint=.
LocutionsFile=$FREELINGSHARE/es/locucions.dat 
QuantitiesFile=$FREELINGSHARE/es/quantities.dat
AffixFile=$FREELINGSHARE/es/afixos.dat
ProbabilityFile=$FREELINGSHARE/es/probabilitats.dat
DictionaryFile=$FREELINGSHARE/es/dicc.src
PunctuationFile=$FREELINGSHARE/common/punct.dat
ProbabilityThreshold=0.001

# NER options 
NERecognition=yes
NPDataFile=$FREELINGSHARE/es/np.dat
## comment line above and uncomment that below, if you want 
## a better NE recognizer (higer accuracy, lower speed)
#NPDataFile=$FREELINGSHARE/es/ner/ner-ab.dat

#Spelling Corrector config file
CorrectorFile=$FREELINGSHARE/es/corrector/corrector.dat

## Phonetic encoding of words.
Phonetics=no
PhoneticsFile=$FREELINGSHARE/es/phonetics.dat

## NEC options
NEClassification=no
NECFile=$FREELINGSHARE/es/nec/nec-svm.dat

## Sense annotation options (none,all,mfs,ukb)
SenseAnnotation=none
SenseConfigFile=$FREELINGSHARE/es/senses.dat
UKBConfigFile=$FREELINGSHARE/es/ukb.dat

#### Tagger options
Tagger=hmm
TaggerHMMFile=$FREELINGSHARE/es/tagger.dat
TaggerRelaxFile=$FREELINGSHARE/es/constr_gram.dat
TaggerRelaxMaxIter=500
TaggerRelaxScaleFactor=670.0
TaggerRelaxEpsilon=0.001
TaggerRetokenize=yes
TaggerForceSelect=tagger

#### Parser options
GrammarFile=$FREELINGSHARE/es/grammar-dep.dat

#### Dependence Parser options
DepTxalaFile=$FREELINGSHARE/es/dep/dependences.dat

#### Coreference Solver options
CoreferenceResolution=no
CorefFile=$FREELINGSHARE/es/coref/coref.dat
\end{verbatim}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\chapter{Extending the library with analyzers for new languages}
\label{c-adding-lang}
 
  It is possible to extend the library with capability to deal with a
  new language. In some cases, this may be done without reprogramming,
  but for accurate results, some modules would require entering into
  the code.

  Since the text input language is an configuration option of the
  system, a new configuration file must be created for the language to
  be added (e.g. copying and modifying an existing one, such as the example
  presented in section~\ref{ss-config}). 
 
 %..................................................
 \section{Tokenizer}
  The first module in the processing chain is the tokenizer. As
  described in section~\ref{ss-options}, the behaviour of the
  tokenizer is controlled via the TokenizerFile option in
  configuration file. 

  To create a tokenizer for a new language, just create a new
  tokenization rules file (e.g. copying an existing one and adapting 
  its regexps to particularities of your language), and set 
  it as the value for the TokenizerFile option in your new 
  configuration file.

%..................................................
  \section{Morphological analyzer}
   The morphological analyzer module consists of several sub-modules
   that may require language customization. See
   sections~\ref{sec-maco} to \ref{file-prob} for details on data file formats for each
   module:

   \subsection{Multiword detection} 
    The LocutionsFile option in
    configuration file must be set to the name of a file that contains
    the multiwords you want to detect in your language. 

   \subsection{Nummerical expression detection} 
    If no specialized module is defined to detect nummerical
    expressions, the default behaviour is to recognize only numbers
    and codes written in digits (or mixing digits and non-digit
    characters).
  
    If you want to recognize language dependent expressions (such as
    numbers expressed in words --e.g. ``one hundred thirthy-six''),
    you have to program a {\em numbers\_mylanguage} class derived from
    abstract class {\em numbers\_module}.  Those classes are finite
    automata that recognize word sequences. An abstract class {\em
    automat} controls the sequence advance, so your derived class has
    little work to do apart from defining states and transitions for
    the automaton.

    A good idea to start with this issue is having a look at the 
    {\em numbers\_es}, {\em numbers\_en}, and {\em numbers\_ca} classes.
    State/transition diagrams of those automata can be found in the
    directory {\tt doc/diagrams}.
 
   \subsection{Date/time expression detection} 
    If no specialized module is defined to detect date/time
    expressions, the default behaviour is to recognize only simple
    date expressions (such as DD/MM/YYYY).
  
    If you want to recognize language dependent expressions (such as
    complex time expressions --e.g. ``wednesday, July 12th at half
    past nine''), you have to program a {\em date\_mylanguage} class
    derived from abstract class {\em dates\_module}.  Those classes
    are finite automata that recognize word sequences. An abstract
    class {\em automat} controls the sequence advance, so your derived
    class has little work to do apart from defining states and
    transitions for the automaton.

    A good idea to start with this issue is having a look at the 
    {\em dates\_es}, {\em dates\_en}, and {\em dates\_ca} classes.
    State/transition diagrams of those automata can be found in the
    directory {\tt doc/diagrams}.

   \subsection{Currency/ratio expression detection} If no specialized
    module is defined to detect date/time expressions, the default
    behaviour is to recognize only simple percentage expressions (such
    as ``23\%'').
  
    If you want to recognize language dependent expressions (such as
    complex ratio expressions --e.g. ``three out of four''-- or
    currency expression --e.g. ``2,000 australian dollar''), you have
    to program a {\em quantities\_mylanguage} class derived from
    abstract class {\em quantities\_module}.  Those classes are finite
    automata that recognize word sequences. An abstract class {\em
    automat} controls the sequence advance, so your derived class has
    little work to do apart from defining states and transitions for
    the automaton.

    A good idea to start with this issue is having a look at the {\em
    quantities\_es} and {\em quantities\_ca}
    classes.  

    In the case your language is a roman language (or at least, has a
    similar structure for currency expressions) you can easily develop
    your currency expression detector by copying the {\em quantities\_es}
    class, and modifying the CurrencyFile option to provide a file in
    which lexical items are adapted to your language.
    For instance: Catalan currency recognizer uses a copy of the 
    {\em quantities\_es} class, but a different CurrencyFile, since
    the syntactical structure for currency expression is the same in
    both languages, but lexical forms are different.

    If your language has a very different structure for those
    expressions, you may require a different format for the 
    CurrencyFile contents. Since that file will be used only 
    for your language, feel free to readjust its format.

   \subsection{Dictionary search} 
    The lexical forms for each language are sought in a database.
    You only have to specify in which file it is found
    with the DictionaryFile option.

    The dictionary file can be build with the {\tt indexdict} program
    you'll find in the binaries directory of FreeLing. This program
    reads data from stdin and indexes them into a DB file with the
    name given as a parameter.
   
    The input data is expected to contain one word form per line, each line
    with the format: \\
    {\tt form lemma1 tag1 lemma2 tag2 ...}\\
    E.g.\\
    {\tt abalanzar\'a abalanzar VMIC1S0 abalanzar VMIC3S0\\
    bajo bajar VMIP1S0 bajo AQ0MS0 bajo NCMS000 bajo SPS00\\
    efusivas efusivo AQ0FP0}

   \subsection{Affixed forms search} 
    Forms not found in dictionary may be submitted to an affix
    analysis to devise whether they are derived forms. The valid
    affixes and their application contexts are defined in the 
    affix rule file referred by AffixFile configuration option. 
    See section~\ref{file-suf} for details on affixation rules
    format.
   
     If your language has ortographic accentuation (such as Spanish,
    Catalan, and many other roman languages), the suffixation rules
    may have to deal with accent restoration when rebuilding the
    original roots. To do this, you have to to program a {\em
    accents\_mylanguage} class derived from abstract class {\em
    accents\_module}, which provides the service of restoring
    (according to the accentuation rules in your languages)
    accentuation in a root obtained after removing a given suffix.

    A good idea to start with this issue is having a look at the 
    {\em accents\_es} class.

   \subsection{Probability assignment} 

     The module in charge of assigning lexical probabilities to each
    word analysis only requires a data file, referenced by the 
    ProbabilityFile configuration option. 

     This file may be created using the script
     {\tt src/utilities/train-tagger/bin/TRAIN.sh} included in
     FreeLing source package, and a tagged corpus.

    See section~\ref{file-prob} for format details.

%..................................................
  \section{HMM PoS Tagger}

   The HMM PoS tagger only requires an appropriate HMM parameters file, 
   given by the TaggerHMMFile option. See section~\ref{file-hmm}
   for format details.
 
   To build a HMM tagger for a new language, you will need corpus 
   (preferably tagged), and you will have to write some probability 
   estimation scripts (e.g. you may use MLE with a simple add-one 
   smoothing).

   Nevertheless, the easiest way (if you have a tagged corpus) is
   using the estimation and smoothing script {\tt
   src/utilities/train-tagger/bin/TRAIN.sh} provided in FreeLing source package.

%..................................................
  \section{Relaxation Labelling PoS Tagger}

   The Relaxation Labelling PoS tagger only requires an appropriate
   pseudo- constraint grammar file,  given by the RelaxTaggerFile
   option. See section~\ref{file-relax} for format details.

   To build a Relax tagger for a new language, you will need corpus (preferably
   tagged), and you will have to write some compatibility estimation
   scripts. You can also write from scratch a knowledge-based constraint grammar.

   Nevertheless, the easiest way (if you have an annotated corpus) is
   using the estimation and smoothing script {\tt
   src/utilities/train-tagger/bin/TRAIN.sh} provided in FreeLing source package.  

   The produced constraint grammar files contain only simple bigram
   constraints, but the model may be improved by hand coding more
   complex context constraint, as can be seen in the Spanish data file
   in {\tt share/FreeLing/es/constr\_grammar.dat}

%..................................................
  \section{Named Entity Recognizer and Classifier}
 
  Named Entity recognition and classification modules can be trained
  for a new language, provided a hand-annotated large enough corpus is
  available.

  A README file and training scripts can be found in 
  {\tt src/utilities/nerc} in FreeLing tarball.

%..................................................
  \section{Chart Parser}

   The parser only requires a grammar which is consistent with the
   tagset used in the morphological and tagging steps.
   The grammar file must be specified in the GrammarFile option
   (or passed to the parser constructor). See section~\ref{file-cfg}
   for format details.

%..................................................
  \section{Dependency Parser}

   The depencency parser only requires a set of rules which is consistent with the
   PoS tagset and the non-terminal categories generated by the Chart
   Parser grammar.
   The grammar file must be specified in the DepRulesFile option
   (or passed to the parser constructor). See section~\ref{file-dep}
   for format details.

\bibliographystyle{alpha}
\bibliography{biblio} 

\end{document}

