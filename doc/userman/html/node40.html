<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with LaTeX2HTML 2008 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>BIO NER module (bioner)</TITLE>
<META NAME="description" CONTENT="BIO NER module (bioner)">
<META NAME="keywords" CONTENT="userman">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="Generator" CONTENT="LaTeX2HTML v2008">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="userman.css">

<LINK REL="previous" HREF="node39.html">
<LINK REL="up" HREF="node38.html">
<LINK REL="next" HREF="node41.html">
</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<A NAME="tex2html755"
  HREF="node41.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html751"
  HREF="node38.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html747"
  HREF="node39.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html753"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html756"
  HREF="node41.html">Quantity Recognition Module</A>
<B> Up:</B> <A NAME="tex2html752"
  HREF="node38.html">Named Entity Recognition Module</A>
<B> Previous:</B> <A NAME="tex2html748"
  HREF="node39.html">Basic NER module (np)</A>
 &nbsp; <B>  <A NAME="tex2html754"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR></DIV>
<!--End of Navigation Panel-->

<H2><A NAME="SECTION004112000000000000000">
<EM>BIO</EM> NER module (<TT>bioner</TT>)</A>
</H2>

<P>
The machine-learning based NER module uses a classification algorithm 
 to decide whether each word is at a NE begin (<TT>B</TT>), inside (<TT>I</TT>)
 or outside (<TT>O</TT>). Then, a simple viterbi algorithm is applied to
 guarantee sequence coherence.

<P>
It can be instantiated via the <TT>ner</TT> wrapper described above, or
 directly via its own API:
<PRE>
class bioner: public ner_module {
  public:
    /// Constructor, receives the name of the configuration file.
    bioner ( const std::string &amp; );

    /// analyze given sentence.
    void analyze(sentence &amp;) const;
    /// analyze given sentences.
    void analyze(std::list&lt;sentence&gt; &amp;) const;
    /// return analyzed copy of given sentence
    sentence analyze(const sentence &amp;) const;
    /// return analyzed copy of given sentences
    std::list&lt;sentence&gt; analyze(const std::list&lt;sentence&gt; &amp;) const;
};
</PRE>

<P>
The configuration file sets the required model and lexicon files, which
  may be generated from a training corpus using the scripts provided 
  with FreeLing (in folder <TT>src/utilities/nerc</TT>).
  Check the README and comments in the scripts to find out what to do.

<P>
The most important file in the set is the <TT>.rgf</TT> file, which contains
  a definition of the context features that must be extracted for each
  named entity.  
  The feature rule language is described in section <A HREF="node75.html#sec-rgf">4.4</A>.

<P>
The sections of the configuration file for <I CLASS="slanted">bioner</I> module are:

<UL>
<LI>Section <code>&lt;RGF&gt;</code> contains one line with the path to the
    RGF file of the model. This file is the definition of the features
    that will be taken into account for NER.  These features are
    processed by <TT>libfries</TT>.
<PRE>
&lt;RGF&gt;
ner.rgf
&lt;/RGF&gt;
</PRE>

<P>
</LI>
<LI>Section <code>&lt;Classifier&gt;</code> contains one line with the kind of
    classifier to use. Valid values are <code>AdaBoost</code> and
    <code>SVM</code>.
<PRE>
&lt;Classifier&gt;
Adaboost
&lt;/Classifier&gt;
</PRE>

<P>
</LI>
<LI>Section <code>&lt;ModelFile&gt;</code> contains one line with the path to
    the model file to be used. The model file must match the
    classifier type given in section <code>&lt;Classifier&gt;</code>.
<PRE>
&lt;ModelFile&gt;
ner.abm
&lt;/ModelFile&gt;
</PRE>
  The <TT>.abm</TT> files contain AdaBoost models based on shallow
  Decision Trees (see [<A
 HREF="node110.html#carreras03">CMP03</A>] for details). You don't need
  to understand this, unless you want to enter into the code of the
  AdaBoost classifier.

<P>
The <TT>.svm</TT> files contain Support Vector Machine models generated
  by <TT>libsvm</TT> [<A
 HREF="node110.html#chang11">CL11</A>]. You don't need to understand this, unless you want 
  to enter into the code of <TT>libsvm</TT>.

<P>
</LI>
<LI>Section <code>&lt;Lexicon&gt;</code> contains one line with the path to the
  lexicon file of the learnt model. The lexicon is used to translate
  string-encoded features generated by <TT>libfries</TT> to
  integer-encoded features needed by <TT>libomlet</TT>. The lexicon file
  is generated by <TT>libfries</TT> at training time.
<PRE>
&lt;Lexicon&gt;
ner.lex
&lt;/Lexicon&gt;
</PRE>
  The <TT>.lex</TT> file is a dictionary that assigns a number to each
  symbolic feature used in the AdaBoost or SVM model. You don't need to
  understand this either unless you are a Machine Learning student or
  the like.

<P>
</LI>
<LI>Section <code>&lt;UseSoftMax&gt;</code> contains only one line with <EM>  yes</EM> or <EM>no</EM>, indicating whether the classifier output must be
  converted to probabilities with the SoftMax function. Currently,
  AdaBoost models need that conversion, and SVM models do not.
<PRE>
&lt;UseSoftMax&gt;
yes
&lt;/UseSoftMax&gt;
</PRE>

<P>
</LI>
<LI>Section <code>&lt;Classes&gt;</code> contains only one line with the classes
  of the model and its translation to B, I, O tag.
<PRE>
&lt;Classes&gt;
0 B 1 I 2 O
&lt;/Classes&gt;
</PRE>

<P>
</LI>
<LI>Section <code>&lt;NE_Tag&gt;</code> contains only one line with the PoS tag that
  will be assigned to the recognized entities. If the NE classifier is
  going to be used later, it will have to be informed of this tag at
  creation time.
<PRE>
&lt;NE_Tag&gt;
NP00000
&lt;/NE_Tag&gt;
</PRE>

<P>
</LI>
<LI>Section <code>&lt;InitialProb&gt;</code> Contains the probabilities of
  seeing each class at the begining of a sentence. These probabilities
  are necessary for the Viterbi algorithm used to annotate NEs in a
  sentence.
<PRE>
&lt;InitialProb&gt;
B 0.200072
I 0.0
O 0.799928
&lt;/InitialProb&gt;
</PRE>

<P>
</LI>
<LI>Section <code>&lt;TransitionProb&gt;</code> Contains the transition
  probabilities for each class to each other class, used by the
  Viterbi algorithm.
<PRE>
&lt;TransitionProb&gt;
B B 0.00829346
B I 0.395481
B O 0.596225
I B 0.0053865
I I 0.479818
I O 0.514795
O B 0.0758838
O I 0.0
O O 0.924116
&lt;/TransitionProb&gt;
</PRE>

<P>
</LI>
<LI>Section <code>&lt;TitleLimit&gt;</code> contains only one line with an integer
  value stating the length beyond which a sentence written <I CLASS="slanted">  entirely</I> in uppercase will be considered a title and not a proper
  noun. Example:
<PRE>
&lt;TitleLimit&gt;
3
&lt;/TitleLimit&gt;
</PRE>

<P>
If <code>TitleLimit=0</code> (the default) title detection is
  deactivated (i.e, all-uppercase sentences are always marked as
  named entities).

<P>
The idea of this heuristic is that newspaper titles are usually
  written in uppercase, and tend to have at least two or three
  words, while named entities written in this way tend to be acronyms
  (e.g. IBM, DARPA, ...) and usually have at most one or two words.

<P>
For instance, if <code>TitleLimit=3</code> the sentence 
  <TT>FREELING ENTERS NASDAC UNDER CLOSE OBSERVATION OF MARKET ANALYSTS</TT>
  will not be recognized as a named entity, and will have its words analyzed
  independently. On the other hand, the sentence <TT>IBM INC.</TT>, having less than
  3 words, will be considered a proper noun.

<P>
Obviously this heuristic is not 100% accurate, but in some cases
  (e.g. if you are analyzing newspapers) it may be preferrable to the
  default behaviour (which is not 100% accurate, either).

<P>
</LI>
<LI>Section <code>&lt;SplitMultiwords&gt;</code> contains only one line with
    either <code>yes</code> or <code>no</code>. If <code>SplitMultiwords</code> is
    activated Named Entities still will be recognized but they will
    not be treated as a unit with only one Part-of-Speech tag for the
    whole compound. Each word gets its own Part-of-Speech tag
    instead.
<BR>
Capitalized words get the Part-of-Speech tag as
    specified in <code>NE_Tag</code>, The Part-of-Speech tags of
    non-capitalized words inside a Named Entity (typically,
    prepositions and articles) will be left untouched.
<PRE>
&lt;SplitMultiwords&gt;
no
&lt;/SplitMultiwords&gt;
</PRE>
</LI>
</UL>

<P>

<DIV CLASS="navigation"><HR>
<!--Navigation Panel-->
<A NAME="tex2html755"
  HREF="node41.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html751"
  HREF="node38.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html747"
  HREF="node39.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html753"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html756"
  HREF="node41.html">Quantity Recognition Module</A>
<B> Up:</B> <A NAME="tex2html752"
  HREF="node38.html">Named Entity Recognition Module</A>
<B> Previous:</B> <A NAME="tex2html748"
  HREF="node39.html">Basic NER module (np)</A>
 &nbsp; <B>  <A NAME="tex2html754"
  HREF="node1.html">Contents</A></B> </DIV>
<!--End of Navigation Panel-->
<ADDRESS>
Lluís Padró
2013-09-09
</ADDRESS>
</BODY>
</HTML>
